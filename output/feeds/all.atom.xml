<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Slender Means</title><link href="http://slendermeans.org/" rel="alternate"></link><link href="http://slendermeans.org/feeds/all.atom.xml" rel="self"></link><id>http://slendermeans.org/</id><updated>2014-01-17T14:30:00-05:00</updated><entry><title>A Geneva Convention for the Language Wars</title><link href="http://slendermeans.org/language-wars.html" rel="alternate"></link><updated>2014-01-17T14:30:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2014-01-17:language-wars.html</id><summary type="html">&lt;p&gt;I don&amp;#8217;t tend to get too sniffy about the quality of discourse on the
Internet. I have some appreciation for even the most pointless, uninformed
flamewars. (And maybe my take on Web site comments is for another post.) But
there&amp;#8217;s an increasingly popular topic of articles and blog posts which is starting to annoy
me a little. You&amp;#8217;ve likely read them&amp;#8212;they have titles like: &amp;#8220;Python is Eating R&amp;#8217;s Lunch,&amp;#8221; &amp;#8220;Why
Python is Going to Take Over Data Science,&amp;#8221; &amp;#8220;Why Python is a Pain in the Ass and
Will Never Beat R,&amp;#8221; &amp;#8220;Why Everyone Will Live on the Moon and Code in Julia in 5
Years,&amp;#8221; etc.&lt;/p&gt;
&lt;p&gt;This style of article obviously isn&amp;#8217;t unique to data analysis
languages. It&amp;#8217;s a classic nerd flamewar, in the proud tradition of text editor wars
and browser wars. Perhaps an added inflammatory agent here is the Data Science hype
machine.&lt;/p&gt;
&lt;p&gt;And that&amp;#8217;s all okay. Go on the Internet and bitch about languages you don&amp;#8217;t like, or tell
everyone why your preferred one is awesome. That&amp;#8217;s what
&lt;a href="http://news.ycombinator.com" title="Hacker News"&gt;the Internet&amp;#8217;s here for&lt;/a&gt;. And Lord knows I&amp;#8217;ve done it myself.&lt;/p&gt;
&lt;p&gt;My only problem is that it distracts from more important, more
interesting conversations about what&amp;#8217;s happening with data analysis
languages. Instead of pissing matches and popularity contests, the real
interesting phenomena is how developers are comparing notes, sharing cool
innovations, and increasing interoperability. A great example is the IPython
notebook. The notebook doesn&amp;#8217;t make Python better than other languages&amp;#8212;it makes
&lt;a href="http://nbviewer.ipython.org/url/beowulf.csail.mit.edu/18.337/fractals.ipynb" title="IJulia notebook"&gt;all&lt;/a&gt; &lt;a href="http://gibiansky.github.io/IHaskell/demo.html" title="IHaskell Notebook"&gt;languages&lt;/a&gt; &lt;a href="http://nbviewer.ipython.org/github/minad/iruby/blob/master/IRuby-Example.ipynb" title="IRuby notebook"&gt;better&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I think it&amp;#8217;s a really fascinating time for folks who use and think about
computer languages. The last 5 years or so has seen not only the introduction of
really cool new languages, but also extraordinary developments in existing
ones. I&amp;#8217;m psyched about all these languages and I want them all to succeed and
get better. Some days I want to code in R, some days in Python. Others in Julia,
or Clojure, or F#, or even C+&lt;span style="margin-left:-4px"&gt;+&lt;/span&gt;. I don&amp;#8217;t want any of them to stagnate or
disappear, or be &amp;#8220;beaten&amp;#8221; by any of the others. And I don&amp;#8217;t think that&amp;#8217;s
happening anyway.&lt;/p&gt;
&lt;p&gt;So what&amp;#8217;s below is a somewhat tongue-in-cheek list of suggestions for
facilitating productive and interesting discussions comparing languages. Many of them are not specific
to our little R/Python/Matlab/Julia skirmishes, but apply to lots of different
language wars (C+&lt;span style="margin-left:-4px"&gt;+&lt;/span&gt; vs. Java, Python vs. Perl, Ruby vs. Python, Clojure vs. Scala, Haskell vs.,
I dunno, everybody?) The last section is comprised of a couple of very general
notes about civility. I&amp;#8217;m strongly in favor everyone&amp;#8217;s right to be a smug prick
on the Internet. But, you know, you should probably try not to be a smug prick
on the Internet.&lt;/p&gt;
&lt;p&gt;And, please, feel free to add additions or suggestions in the comments, or in this &lt;a href="https://gist.github.com/carljv/8554723" title="Gist"&gt;Gist&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Section 1: Being Aware of Context&lt;/h2&gt;
&lt;h3&gt;§ 1, Article 1&lt;/h3&gt;
&lt;p&gt;Recognize that languages have goals and communities. It helps to evaluate them in
that context. Features that are high priority to you may not be high priority to
the majority of users in that language, and vice-versa.&lt;/p&gt;
&lt;h3&gt;§ 1, Article 2&lt;/h3&gt;
&lt;p&gt;Recognize that many smart, capable people are very productive in the language
you&amp;#8217;re slagging. The cool things science and industry are making in the language
speaks far louder than your casual dismissals of it on a message board.&lt;/p&gt;
&lt;p&gt;The same logic goes for language developers. For example,
&lt;a href="http://had.co.nz" title="Hadley Wickham"&gt;Hadley Wickham&lt;/a&gt; is a smart guy and a great programmer; he&amp;#8217;s probably not one to
waste his time improving a language that&amp;#8217;s some irreparably broken
dead-end. Same with &lt;a href="http://continuum.io" title="Continuum"&gt;these guys&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;§ 1, Article 3&lt;/h3&gt;
&lt;p&gt;Recognize that language design is the art of the tradeoff. Don&amp;#8217;t complain about
a design choice until you understand the logic behind it. In many cases, your
preferred design or feature was already considered, and would have led to
undesirable outcomes elsewhere. It is helpful and
interesting to disagree about how a tradeoff was managed, but do recognize that there
was one.&lt;/p&gt;
&lt;h3&gt;§ 1, Article 4&lt;/h3&gt;
&lt;p&gt;Distinguish between a feature request and a language critique. If you come to a
new language and miss some features of
your old language, that&amp;#8217;s fine. But that&amp;#8217;s not necessarily a failing of the new
language.&lt;/p&gt;
&lt;p&gt;A living, breathing language is a combination of both its features and its idioms. A
feature may not exist because its programmers tend to write code in a way that
obviates its need. Sometimes such idioms are crutches to compensate for truly
useful features that are missing; other times they are interesting and elegant expressions of a
problem that you&amp;#8217;re just not accustomed to. Try to spot the difference.&lt;/p&gt;
&lt;h3&gt;§ 1, Article 5&lt;/h3&gt;
&lt;p&gt;Pay your dues before dismissing a language. If you gave up on something in a language after finding it too difficult,
consider that the problem may be yours. It may not be, but at least
consider it.&lt;/p&gt;
&lt;h3&gt;§ 1, Article 6&lt;/h3&gt;
&lt;p&gt;Don&amp;#8217;t over-sell immature, alpha-version features, no matter how
promising they are. Promises don&amp;#8217;t cook rice. Sending unsuspecting users to
buggy, incomplete libraries just harms your cause in the long run.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Julia has a fast-growing library of packages!&amp;#8221; Sure, but less than a handful
    are close to production quality.&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;And now ggplot has been ported to Python!&amp;#8221; &lt;a href="http://blog.yhathq.com/posts/ggplot-for-python.html" title="ggplot for Python"&gt;Not quite yet.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Honest advertising of works-in-progress is encouraged, though. There&amp;#8217;s nothing
inherently wrong with immature libraries, many of which are fantastic.&lt;/p&gt;
&lt;h3&gt;§ 1, Article 7&lt;/h3&gt;
&lt;p&gt;Microbenchmarks are useful for understanding differences between languages and their
execution, but are
of limited use in pissing contests. No one knows exactly what percentage of the world&amp;#8217;s
working software is comprised of Fibonacci number calculations, but our best
guess is not much. &lt;/p&gt;
&lt;h2&gt;Section 2: Being Interesting&lt;/h2&gt;
&lt;h3&gt;§ 2, Article 1&lt;/h3&gt;
&lt;p&gt;Whether one language is going to take over another is not that
interesting, nor that meaningful. (When does a language get &amp;#8220;taken over?&amp;#8221; For
Christ&amp;#8217;s sakes, there&amp;#8217;s still a non-trivial amount of &lt;span class="caps"&gt;COBOL&lt;/span&gt; running out there in the wild.)&lt;/p&gt;
&lt;p&gt;Competition is pointless, but comparison is not. Languages are  increasingly adopting ideas from
each other, building interops with each other, and sharing
tooling. Having conversations about this process is far more interesting than
running popularity contests.&lt;/p&gt;
&lt;h3&gt;§ 2, Article 2&lt;/h3&gt;
&lt;p&gt;Avoid clichéd arguments. They are not necessarily incaccurate, but
they are boring.&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R is a &amp;#8220;&lt;span class="caps"&gt;DSL&lt;/span&gt;&amp;#8221; or &amp;#8220;not a real language&amp;#8221; (see Article 2 below); R is &amp;#8220;designed by statisticians,
  not computer scientists.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Semantic whitespace in Python sucks.&amp;#8221; (Generally, arguments over
  syntax are boring.)&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Julia doesn&amp;#8217;t have as many libraries as ${pretty much anything}.&amp;#8221;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to arguments, also avoid clichéd phrases. (See, &lt;em&gt;e.g.&lt;/em&gt;, &amp;#8220;not ready
for prime-time.&amp;#8221;)&lt;/p&gt;
&lt;h3&gt;§ 2, Article 3&lt;/h3&gt;
&lt;p&gt;Supplement abstract terms or subjective impressions with concrete definitions
and examples.&lt;/p&gt;
&lt;p&gt;Examples of statements that could use concrete support:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Code in language X is &lt;em&gt;more expressive&lt;/em&gt; than language Y.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;R is a &lt;em&gt;&lt;span class="caps"&gt;DSL&lt;/span&gt;&lt;/em&gt;, while Python is a &lt;em&gt;general purpose language.&lt;/em&gt;&amp;#8221;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Section 3: Being Civil&lt;/h2&gt;
&lt;h3&gt;§ 4, Article 1&lt;/h3&gt;
&lt;p&gt;Be sure that you can accurately summarize someone&amp;#8217;s argument before you start
composing your rebuttal.&lt;/p&gt;
&lt;h3&gt;§ 4, Article 2&lt;/h3&gt;
&lt;p&gt;You are not so smart that you are entitled to be smug. Some tips:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Nix hyperbolic vocabulary. No one and nothing associated with any language
      is &amp;#8220;stupid,&amp;#8221; &amp;#8220;dumb,&amp;#8221; &amp;#8220;crazy&amp;#8221;, &amp;#8220;broken,&amp;#8221; etc.&lt;/li&gt;
&lt;li&gt;Use of the word &amp;#8220;fail&amp;#8221; is strongly discouraged. Use of it as a noun is
      strictly prohibited.&lt;/li&gt;
&lt;li&gt;It is no victory&amp;#8212;not even a moral one&amp;#8212;to find someone
      &lt;a href="http://xkcd.com/386/" title="XKCD"&gt;wrong on the internet&lt;/a&gt;. Don&amp;#8217;t treat it a such. Offer a polite
      factual correction and allow for the possibility that you&amp;#8217;ve misunderstood.&lt;/li&gt;
&lt;/ol&gt;</summary><category term="programming"></category></entry><entry><title>Tricked out iterators in Julia</title><link href="http://slendermeans.org/julia-iterators.html" rel="alternate"></link><updated>2014-01-13T15:15:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2014-01-13:julia-iterators.html</id><summary type="html">&lt;script src="http://d3js.org/d3.v3.min.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;script src="./scripts/gadfly.js"&gt;&lt;/script&gt;

&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I want to spend some time messing about with iterators in Julia. I think they
not only provide a familiar and useful entry point into Julia&amp;#8217;s type system and dispatch
model, they&amp;#8217;re also interesting in their own right.&lt;a name="fnm-multdisp" href="#fn-multdisp" class="footnote-mark"&gt;1&lt;/a&gt; Clever application of iterators can
help to simplify complicated loops, better express their intent, and improve
memory usage.&lt;/p&gt;
&lt;p&gt;A word of warning about the code here. Much of the it isn&amp;#8217;t idiomatic Julia and I wouldn&amp;#8217;t
necessarily recommend using this style in a serious project. I also can&amp;#8217;t speak
to its performance vis-a-vis more obvious Julian alternatives. In some cases,
the style of the code examples below may help reduce memory usage, but
performance is not my main concern. (This may be the first blogpost about Julia
unconcerned with speed). Instead, I&amp;#8217;m just interested in different ways of
expressing iteration problems.&lt;/p&gt;
&lt;p&gt;For anyone who&amp;#8217;d like to play along at home, there&amp;#8217;s an IJulia notebook of
this material on &lt;a href="https://github.com/carljv/Julia-Iterators/blob/master/iterator_tricks.ipynb"&gt;Github&lt;/a&gt;, which can be viewed on nbviewer &lt;a href="http://nbviewer.ipython.org/urls/raw2.github.com/carljv/Julia-Iterators/master/iterator_tricks.ipynb?create=1"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;The Iterator Protocol&lt;/h2&gt;
&lt;p&gt;What do I mean by iterators?&lt;a name="fnm-iterable" href="#fn-iterable"
class="footnote-mark"&gt;2&lt;/a&gt; I mean any &lt;code&gt;I&lt;/code&gt; in Julia that works on
the right hand side of the statement &lt;code&gt;for i = I ...&lt;/code&gt;. That is, anything you can for-loop
over. This includes not only data collections like Arrays, Dicts, and Sets, but
also more abstract types like Ranges, as well as what I&amp;#8217;ll call &amp;#8220;higher order&amp;#8221;
iterators such as those that result from &lt;code&gt;zip&lt;/code&gt; or &lt;code&gt;enumerate&lt;/code&gt; functions.&lt;/p&gt;
&lt;p&gt;As an equivalent definition, an iterator in Julia is any type that implements
the &lt;strong&gt;iterator protocol&lt;/strong&gt;. The iterator protocol is comprised of three methods:
&lt;code&gt;start&lt;/code&gt;, &lt;code&gt;next&lt;/code&gt;, and &lt;code&gt;done&lt;/code&gt;. So any type in Julia for which these three methods
are defined is an iterator. It might be a dumb iterator or a broken iterator,
but it&amp;#8217;s an iterator. &lt;/p&gt;
&lt;p&gt;Since the &lt;code&gt;for&lt;/code&gt; statement works on iterators, and iterators are just a
collection of methods, we can define any for loop using calls to those methods. &lt;/p&gt;
&lt;p&gt;For example, this simple for loop&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;100
 64
 36
 16
  4
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;is equivalent to this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;100
 64
 36
 16
  4
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this example, the &lt;code&gt;start&lt;/code&gt; method provides the initial state of the iterator;
the &lt;code&gt;next&lt;/code&gt; method returns the value of the array at a given state, as well as what the
next state is. Finally, the &lt;code&gt;done&lt;/code&gt; method returns &lt;code&gt;true&lt;/code&gt; when we&amp;#8217;ve gone past
the end of the iterator, informing the loop that it should stop.&lt;/p&gt;
&lt;p&gt;If you know Python, the idea of the iterator protocol is probably familiar. In
Python, any object can be an iterator if it has the methods &lt;code&gt;__iter__&lt;/code&gt; and
&lt;code&gt;__next__&lt;/code&gt;. But notice the lack of side effects in the Julia implementation
&amp;#8212;calling &lt;code&gt;start&lt;/code&gt; or &lt;code&gt;next&lt;/code&gt; on the array has no affect on the array itself. &lt;code&gt;start&lt;/code&gt; is
basically a constant, always returning the value of the initial state whenever
you pass it the same type of iterator. And &lt;code&gt;next&lt;/code&gt; doesn&amp;#8217;t really increment
anything; it&amp;#8217;s just a mapping from &lt;em&gt;current state &amp;rightarrow; (value, next
state)&lt;/em&gt;. In general, the iterator itself has no internal state being incremented or
changed as you pass through a loop.&lt;/p&gt;
&lt;h2&gt;An iterator&amp;#8217;s state&lt;/h2&gt;
&lt;p&gt;More concretely, what&amp;#8217;s the &lt;em&gt;state&lt;/em&gt; of an iterator? How the state is
defined, and an iterator&amp;#8217;s sequence of states depends on the type of
the iterator itself. It&amp;#8217;s best to look at some examples.&lt;/p&gt;
&lt;h3&gt;Arrays.&lt;/h3&gt;
&lt;p&gt;Arrays are very intuitive iterators. They have states that are just integer
values from 1 to the length of the array. So &lt;code&gt;start&lt;/code&gt; returns 1.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;arr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;one&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;two&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;three&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;four&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;five&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;six&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;next&lt;/code&gt; mapping is &lt;em&gt;i&lt;/em&gt; &amp;rightarrow; &lt;em&gt;i+1&lt;/em&gt;, and the value of the iterator at any state
&lt;code&gt;i&lt;/code&gt; is just &lt;code&gt;a[i]&lt;/code&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;next(arr, &lt;/span&gt;&lt;span class="si"&gt;$&lt;/span&gt;&lt;span class="s"&gt;i) = &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;next(arr, 1) = (&amp;quot;one&amp;quot;,2)
next(arr, 2) = (&amp;quot;two&amp;quot;,3)
next(arr, 3) = (&amp;quot;three&amp;quot;,4)
next(arr, 4) = (&amp;quot;four&amp;quot;,5)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If this were a multidimensional array, say 3&amp;times;2 instead of 6&amp;times;1, we&amp;#8217;d
get the same result; iteration would just proceed along the rows of the matrix.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;done&lt;/code&gt; method returns true when the state is &lt;code&gt;i =
length(a) + 1&lt;/code&gt;. You might think it&amp;#8217;d be &lt;code&gt;length(a)&lt;/code&gt;, but recall the for-equivalent while loop
above. Having &lt;code&gt;done&lt;/code&gt; return true at the last index of the array would prevent
the loop from executing on the last element. So in our 6-element array, &lt;code&gt;done&lt;/code&gt;
is true when the state hits 7.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c"&gt;# not yet&lt;/span&gt;
&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;false
(&amp;quot;six&amp;quot;,7)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;true
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Ranges&lt;/h3&gt;
&lt;p&gt;Ranges have states that looks similar to arrays, except they start at zero.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;rng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;  &lt;span class="c"&gt;# length 10 range&lt;/span&gt;
&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# 0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;But the relationship between the current and next state is the same: &lt;em&gt;i&lt;/em&gt; &amp;rightarrow; &lt;em&gt;i+1&lt;/em&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;next(rng, &lt;/span&gt;&lt;span class="si"&gt;$&lt;/span&gt;&lt;span class="s"&gt;i) = &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;next(rng, 0) = (11,1)
next(rng, 1) = (12,2)
next(rng, 9) = (20,10)
next(rng, 10) = (21,11)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since we start at zero, the done state is one less than the equivalent array.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;true
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Unordered collections: Dicts, Sets, etc.&lt;/h3&gt;
&lt;p&gt;Arrays and ranges have a natural order, so the evolution of state is
straightforward. But what about collections such as dictionaries and sets that have no inherent
order? Like in many languages, such things can be iterated over, but the order
of iteration is not easily predictable.&lt;/p&gt;
&lt;p&gt;For example, here&amp;#8217;s a dictionary:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;dictit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{:&lt;/span&gt;&lt;span class="n"&gt;one&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;three&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;five&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;five&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;five!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;{:one=&amp;gt;1,:three=&amp;gt;3,:five=&amp;gt;&amp;quot;five!&amp;quot;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The starting state isn&amp;#8217;t 0 or 1, as would be natural for an ordered collection.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;s0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And while &lt;code&gt;next&lt;/code&gt; maps state &lt;em&gt;i&lt;/em&gt; to state &lt;em&gt;j&lt;/em&gt;, the relationship between &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt;
is not obvious. Here, while the first state is 3, the second is 11, and the rest
are similarly weird.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;((:one,1),11)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;((:three,3),13)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;((:five,&amp;quot;five!&amp;quot;),17)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictit&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;true
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The states, you probably and correctly suspect, are tied to the internal
implementation of the dictionary, e.g. how the keys are hashed. So the state
doesn&amp;#8217;t follow a predictable 1, 2, 3, &amp;#8230; order, and what order of elements we
get when iterating is essentially unpredictable.&lt;/p&gt;
&lt;p&gt;But because for loops handle the iterator&amp;#8217;s states for us, we rarely if ever have to worry about
the representation of an iterator&amp;#8217;s state. The for loop implicitly calls the &lt;code&gt;start&lt;/code&gt;,
&lt;code&gt;done&lt;/code&gt;, and &lt;code&gt;next&lt;/code&gt; methods, which do all this bookkeeping for us.&lt;/p&gt;
&lt;h2&gt;Iterators and Delayed Evaluation&lt;/h2&gt;
&lt;p&gt;While many iterators are collections of data in memory, like Arrays, Dicts, or
Sets, iterators can also represent abstract collections that aren&amp;#8217;t held in memory.&lt;/p&gt;
&lt;p&gt;Range is a good example. When we iterate over the range &lt;code&gt;1:10&lt;/code&gt;, we get the
sequence 1, 2, 3, &amp;#8230;, 10. But in memory, this range is comprised of only two
integers, 1 and 10. The values in between are only evaluated when we&amp;#8217;re looping over it.&lt;/p&gt;
&lt;p&gt;From &lt;a href=""&gt;https://github.com/JuliaLang/julia/blob/master/base/range.jl&lt;/a&gt;, here&amp;#8217;s how a
Range&amp;#8217;s iterator protocol is defined:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Ranges&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;}(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Range&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oftype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;}(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Range1&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;oftype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Ranges&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that the &lt;code&gt;next&lt;/code&gt; method calculates the value of the iterator in state
&lt;code&gt;i&lt;/code&gt;. This is different from an Array iterator, which just reads the element
&lt;code&gt;a[i]&lt;/code&gt; from memory.&lt;/p&gt;
&lt;p&gt;Iterators that exploit delayed evaluation like this can have important performance
benefits. If we want to iterate over the integers 1 to 10,000, iterating over an
Array means we have to allocate about &lt;span class="caps"&gt;80MB&lt;/span&gt; to hold it. A Range only requires
16 bytes; the same size as the range 1 to 100,000 or 1 to 100,000,000.&lt;/p&gt;
&lt;h3&gt;Application: Iterating over Fibonacci numbers&lt;/h3&gt;
&lt;p&gt;Here&amp;#8217;s another example of an iterator which computes values on demand, using the
&lt;code&gt;next&lt;/code&gt; method to do the work. &lt;code&gt;fibit(n)&lt;/code&gt; is an iterator over the first &lt;code&gt;n&lt;/code&gt;
Fibonacci numbers. When the iterator is constructed, it doesn&amp;#8217;t calculate all of
these numbers. Instead it waits for its &lt;code&gt;next&lt;/code&gt; method to be called, providing
the next Fibonacci number depending on the current one.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Iterator produces the first n Fibonacci numbers&lt;/span&gt;
&lt;span class="n"&gt;immutable&lt;/span&gt; &lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;:&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;last2&lt;/span&gt;&lt;span class="p"&gt;::(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# Specify types, e.g. BigInt to prevent overflow.&lt;/span&gt;
&lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;}((&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
        &lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;fi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;1 1 2 3 5 8 13 21 34 55
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Tasks/Co-routines&lt;/h3&gt;
&lt;p&gt;This talk of iterators with delayed evaluation may remind Pythonistas of
generators. And Julia has a type that is basically equivalent to Python&amp;#8217;s
generators, called Tasks. A Task is constructed by calling the &lt;code&gt;Task()&lt;/code&gt; 
constructor (or
&lt;code&gt;@task&lt;/code&gt; macro) on a function with a &lt;code&gt;produce&lt;/code&gt; statement, which issimilar to Python&amp;#8217;s
&lt;code&gt;yield&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Instead of using the &lt;code&gt;Fibit&lt;/code&gt; type above, we could get an equivalent iterator by
defining a Task that produces sequential Fibonacci numbers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; fibtask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zero&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;produce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; _it&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;
            &lt;span class="n"&gt;produce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_it&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;fibtask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fibtask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once we&amp;#8217;ve made the task, we get iteration for free.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;fibtask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;1 1 2 3 5 8 13 21 34 55
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Whether you create an iterator using a type with the iterator protocol, or by
constructing a Task, is up to you. There are pros and cons to each approach. By
defining your iterator as a specific type, you can dispatch lots of other
functions on it. Here, on the other hand, &lt;code&gt;fibtask&lt;/code&gt; is just a &lt;code&gt;Task&lt;/code&gt; type, so
defining methods for it means defining methods for all Tasks, which may be
undesirable or infeasible. On the other hand Tasks give you iterators with less
code. Below I&amp;#8217;ll show an example of an iterator that&amp;#8217;s hard to define with the
iterator protocol methods, but easy to define as a Task. And of course, Tasks
are coroutines, and can be used in those contexts.&lt;/p&gt;
&lt;h2&gt;Realizing Iterators without loops&lt;/h2&gt;
&lt;p&gt;So far, we&amp;#8217;ve talked about iterators in the context of for loops. We saw that
&lt;code&gt;for i = I&lt;/code&gt; was a construct for calling &lt;code&gt;I&lt;/code&gt;&amp;#8216;s &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;done&lt;/code&gt; and &lt;code&gt;next&lt;/code&gt;
methods, letting us realize and operate on the values in the iterator.&lt;/p&gt;
&lt;p&gt;But there are functions which can take iterators as inputs and implicitly iterate over them
to some desired result. This obviates the need for explicit for loops, and can
make for cleaner more functional code. Some examples follow.&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;collect&lt;/code&gt; and &lt;code&gt;reduce&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;collect&lt;/code&gt; function takes an iterator input, realizes all its values, and
&lt;em&gt;collects&lt;/em&gt; them into an array.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;10-element Array{Any,1}:
  1
  1
  2
  3
  5
  8
 13
 21
 34
 55
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;reduce&lt;/code&gt; function similarly realizes the values of an iterator, but then successively
applies an operator to them to give a scalar result.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;143
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That reduce operation is equivalent to the &lt;code&gt;sum&lt;/code&gt; function called with an
iterator argument.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;143
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this next line of code, I compute the sum of the reciprocals of the first
10,000 Fibonacci numbers (which should be close to &lt;a href="http://en.wikipedia.org/wiki/Reciprocal_Fibonacci_constant"&gt;this&lt;/a&gt;), using &lt;code&gt;collect&lt;/code&gt; to first put them into an array.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;./&lt;/span&gt; &lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="n"&gt;_000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;3.359885666243177553172011302918927179688905133731968486495553815325130318996609
e+00 with 256 bits of precision
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Comprehensions&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;collect&lt;/code&gt; function may remind you of an array comprehension, and it is
similar, but here we see array comprehension don&amp;#8217;t work on our iterator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;no method length(FibIt{Int64})
while loading In[26], in expression starting on line 1
 in anonymous at no file
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What&amp;#8217;s going on is that the array comprehension wants to allocate an array,
then fill it in with the values of the iterator. Since it doesn&amp;#8217;t know the
iterator&amp;#8217;s length (how many values it will produce), it doesn&amp;#8217;t know how large
an array to allocate.&lt;a name="fnm-arrcomp" href="#fn-arrcomp"
class="footnote-mark"&gt;3&lt;/a&gt; We can fix this for our Fibonacci iterator by
giving it a &lt;code&gt;length&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;FibIt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;10-element Array{Int64,1}:
  1
  1
  2
  3
  5
  8
 13
 21
 34
 55
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can redefine our sum-of-reciprocals using a comprehension instead of &lt;code&gt;collect&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="n"&gt;_000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;3.359885666243177553172011302918927179688905133731968486495553815325130318996712
e+00 with 256 bits of precision
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What if we tried this with our Fibonacci task?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fibtask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;no method length(Task)
while loading In[27], in expression starting on line 1
 in anonymous at no file
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We get the same issue; Tasks don&amp;#8217;t have a length method. The advantage of using
the &lt;code&gt;FibIt&lt;/code&gt; type is that we can easily define a length method for it. We can
only give our Fibonacci task a method if we give all Tasks a length method,
which wouldn&amp;#8217;t make sense.&lt;/p&gt;
&lt;h2&gt;The Iterator Package&lt;/h2&gt;
&lt;p&gt;When we calculated the sum of the reciprocals of Fibonacci number above, we had
to realize the values of the Fibonacci iterator before taking the
reciprocal, and then sum a collection of all those values. Alternatively, we could have called sum on an
iterator that produced &lt;em&gt;1/x&lt;/em&gt; for each Fibonacci number &lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;One way to do this would be to create a new iterator type, called
&lt;code&gt;ReciprocalFibIt&lt;/code&gt;, and given it its own &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;next&lt;/code&gt;, and &lt;code&gt;done&lt;/code&gt; methods. But that
feels a little excessive. Wouldn&amp;#8217;t it be nicer to be able to construct that iterator from
the Fibonacci iterator we already have? Essentially saying, &amp;#8220;hey, I want another
iterator that gives one over the values of that other iterator.&amp;#8221;&lt;/p&gt;
&lt;p&gt;That would be an example of what I&amp;#8217;ll call a &lt;em&gt;higher-order iterator&lt;/em&gt;, which is
an iterator constructed from one or more other iterators. &lt;code&gt;zip&lt;/code&gt; and &lt;code&gt;enumerable&lt;/code&gt;
are common examples.&lt;/p&gt;
&lt;p&gt;It turns out Julia has a neat little package of useful higher-order iterators;
called, obviously, Iterators. In the rest of (this already very long) post, I&amp;#8217;ll
explore some of things in the package. Pythonistas will notice similarities with
the itertools module in the Standard Library.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;using&lt;/span&gt; &lt;span class="n"&gt;Iterators&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Imap&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Imap&lt;/code&gt; iterator provides us with our wish above: a new iterator that is the
result of applying a function to the values of an existing iterator. In the case of our
reciprocal Fibonacci numbers, that function is &lt;code&gt;x -&amp;gt; 1/x&lt;/code&gt;. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;recipricalfib&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fibit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="n"&gt;_000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BigInt&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c"&gt;# A new iterator, composed&lt;/span&gt;
                                                      &lt;span class="c"&gt;# from a FibIt&lt;/span&gt;
&lt;span class="n"&gt;psi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recipricalfib&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# No collect needed&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;3.359885666243177553172011302918927179688905133731968486495553815325130318996609
e+00 with 256 bits of precision
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So &lt;code&gt;reciprocalfib&lt;/code&gt; is itself an iterator, whose values are only realized when
it&amp;#8217;s passed to the &lt;code&gt;sum&lt;/code&gt; function. We didn&amp;#8217;t have to allocate any arrays before
calling sum as with the &lt;code&gt;collect&lt;/code&gt; and comprehension examples above.&lt;/p&gt;
&lt;h2&gt;An IFilter iterator&lt;/h2&gt;
&lt;p&gt;Since we have a map-like iterator, why not a filter?&lt;a name="fnm-filter" href="#fn-filter" class="footnote-mark"&gt;4&lt;/a&gt; How would it work? Given an
iterator that produces values &lt;em&gt;v1&lt;/em&gt;, &lt;em&gt;v2&lt;/em&gt;, &lt;em&gt;v3&lt;/em&gt;, &amp;#8230;, the filter iterator would
only produce the values that met some predicate, skipping any that didn&amp;#8217;t.&lt;/p&gt;
&lt;p&gt;This isn&amp;#8217;t implemented in the Iterators package (because &lt;code&gt;Base.filter&lt;/code&gt; will
already do this, see &lt;a href="#fn-filter"&gt;footnote 4&lt;/a&gt;). It&amp;#8217;s a neat idea, but it turns
out to be tricky to define in terms of the iterator protocol. It&amp;#8217;s easy with a
Task, though.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; ifilter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;itr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; _it&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;itr&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;produce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;end&lt;/span&gt;
        &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;Task&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_it&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Application: A list of primes whose digits sum to a prime&lt;/h3&gt;
&lt;p&gt;Here&amp;#8217;s an example of it in action. We&amp;#8217;ll begin with a Range iterator from 1 to
1,000. I want to list all of numbers in that range that are (1) prime and
(2) have digits that sum to a prime. &lt;/p&gt;
&lt;p&gt;So &lt;code&gt;ifilter&lt;/code&gt; takes the predicate test and the original iterator, then produces only
those values from the original iterator that pass the test. Turns out there are
89 such primes between 1 and 1,000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; funnyprimetest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Integer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sumdigits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;parseint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
    &lt;span class="n"&gt;isprime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;isprime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sumdigits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ifilter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;funnyprimetest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;89-element Array{Any,1}:
   2
   3
   5
   7
  11
  23
  29
  41
  43
  47
  61
  67
  83
   ⋮
 829
 863
 881
 883
 887
 911
 919
 937
 953
 971
 977
 991
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Repeat and RepeatForever&lt;/h2&gt;
&lt;p&gt;Another surprisingly useful iterator is &lt;code&gt;Repeat&lt;/code&gt;, which simply produces an object
some number of times. Here the iterator is just the string &amp;#8220;echo!&amp;#8221; five times.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;echo!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;echo!
echo!
echo!
echo!
echo!
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we didn&amp;#8217;t provide the second argument, the result would be an iterator that
goes on infinitely, so its for loop would never terminate. Why would you want
that? I&amp;#8217;ll show some examples of its use below.&lt;/p&gt;
&lt;h3&gt;Extension: Repeating impure functions&lt;/h3&gt;
&lt;p&gt;One thing about the &lt;code&gt;Repeat&lt;/code&gt; iterator though, is that the object or value it
repeats is fixed at its construction. If you pass it a called function, it will
call that function once in the constructor, and then repeatedly return the
result of that first call. For pure functions, that&amp;#8217;s fine. The first call of
&lt;code&gt;sqrt(100)&lt;/code&gt; is the same as the second, third, or ten-thousandth call of
&lt;code&gt;sqrt(100)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the function is impure, though, we&amp;#8217;ll get undesired results.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
0.30142748588653046
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, the &lt;code&gt;rand&lt;/code&gt; function was called once in the constructor, and that result was repeated again
and again. I&amp;#8217;d prefer if I could get 10 separate calls to &lt;code&gt;rand&lt;/code&gt;. Here&amp;#8217;s one way
to get this to work.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Iterators&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Repeat&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;Function&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Iterators&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RepeatForever&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;Function&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;nothing&lt;/span&gt;

&lt;span class="c"&gt;# Note the function isn&amp;#39;t called in the constructor;&lt;/span&gt;
&lt;span class="c"&gt;# the `next` function does this.&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;end&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;0.6621100826024566
0.755346320113107
0.021395943367805037
0.7304018818932669
0.22941680891855865
0.966762896262876
0.13729437119070198
0.028788242666101915
0.5584434146272579
0.09166900954689794
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What I&amp;#8217;ve done is create new &lt;code&gt;next&lt;/code&gt; methods for the &lt;code&gt;Repeat&lt;/code&gt; and &lt;code&gt;RepeatForever&lt;/code&gt;
iterators. When the object of the iterators is a function, the &lt;code&gt;next&lt;/code&gt; methods
call the function. By passing the iterator an uncalled function object, I avoid the call
in the constructor, and defer it to the &lt;code&gt;next&lt;/code&gt; method.&lt;/p&gt;
&lt;h2&gt;Take and Drop&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Take&lt;/code&gt; iterator only iterates over some specified first values of its input
iterator. It works well in combination with infinite iterators, like &lt;code&gt;RepeatForever&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;randsforever&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;randsforever&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;10-element Array{Any,1}:
 0.719153
 0.660597
 0.280763
 0.54125 
 0.427029
 0.919311
 0.165029
 0.796911
 0.354417
 0.678271
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;randsforever&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;20-element Array{Any,1}:
 0.568741 
 0.614644 
 0.49445  
 0.0942616
 0.518134 
 0.126585 
 0.961748 
 0.698277 
 0.0805089
 0.32351
 0.797422 
 0.513762 
 0.601515 
 0.616174 
 0.460832 
 0.813204 
 0.172391 
 0.444915 
 0.732941 
 0.0550762
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;Drop&lt;/code&gt; iterator, on the other hand, &lt;em&gt;ignores&lt;/em&gt; some specified first values of its
input iterator. So, how many values should be printed in this for loop?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;randsforever&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="n"&gt;_000&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;9998&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Answer: just the last two, since we take 10,000 random numbers, but drop the first 9,998.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;0.26830900957427684
0.5141969888172926
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Extension: TakeWhile and TakeUntil&lt;/h3&gt;
&lt;p&gt;In some cases you may not want to take a fixed number of values from an
iterator, but instead you want to take values until some condition is met.&lt;/p&gt;
&lt;p&gt;To accomplish this, I&amp;#8217;ll create a &lt;code&gt;TakeWhile&lt;/code&gt; iterator, which takes values from
its input iterator so long as they pass some test.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;immutable&lt;/span&gt; &lt;span class="n"&gt;TakeWhile&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;
    &lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Function&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;takewhile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TakeWhile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;TakeWhile&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;TakeWhile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;TakeWhile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;


&lt;span class="n"&gt;tw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;takewhile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;4-element Array{Int64,1}:
 1
 2
 3
 4
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s also create a &lt;code&gt;TakeUntil&lt;/code&gt; iterator, which takes values until it finds one that
passes the test. So the last value produced by this iterator will pass the test
and all values before that will have failed.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;immutable&lt;/span&gt; &lt;span class="n"&gt;TakeUntil&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;
    &lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Function&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;takeuntil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TakeUntil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;TakeUntil&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;false&lt;/span&gt;

&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;TakeUntil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;


&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; Base&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;TakeUntil&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iscond&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;
    &lt;span class="n"&gt;iscond&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;done&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;takeuntil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c"&gt;# x &amp;lt;= sqrt(25) -&amp;gt; 1:5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;5-element Array{Any,1}:
 1
 2
 3
 4
 5
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Application: How long does it take a Poisson process to produce a prime number?&lt;/h3&gt;
&lt;p&gt;As an application of the &lt;code&gt;TakeUntil&lt;/code&gt; iterator, an experiment. How many draws do
we have to make from a Poisson process until we draw a prime number? For this
example, I&amp;#8217;ll use a Poisson with mean 5,000.&lt;/p&gt;
&lt;p&gt;In the code, we make a &lt;code&gt;Repeat&lt;/code&gt; iterator that repeatedly draws from the
Poisson. We pass this into &lt;code&gt;takeuntil&lt;/code&gt; and this creates an iterator that draws
from the Poisson until we find a prime number. While this is happening, we keep track of the
number of steps we took through this iterator.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Draw random integers from a distibrution d until you get a prime number.&lt;/span&gt;
&lt;span class="c"&gt;# Return the number of draws.&lt;/span&gt;
&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; primetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dparams&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;randgen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dparams&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;tu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;takeuntil&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;randgen&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;isprime&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tu&lt;/span&gt;
        &lt;span class="n"&gt;time&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;
    &lt;span class="n"&gt;time&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;primetime_poiss5k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;primetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Poisson&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;What&amp;#8217;s the average wait for a prime? Repeating the experiment 10,000 times, we
find the average number of draws is between 7 and 8.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;primetime_poiss5k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="n"&gt;_000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;7.6783
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To see the distribution of waiting times, I&amp;#8217;ll collect each repetition of the
experiment in an array that we can plot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;times&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;Int&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;repeated&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;primetime_poiss5k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="n"&gt;_000&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="primetimechart"&gt;&lt;/div&gt;

&lt;script src="scripts/primetime.js"&gt;&lt;/script&gt;

&lt;script&gt;
draw("#primetimechart");
&lt;/script&gt;

&lt;h2&gt;Partition&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Partition&lt;/code&gt; iterator split its input iterator into pieces, producing an
iterator over iterators. For example we could use it to partition the Range
iterator &lt;code&gt;1:100&lt;/code&gt; into two iterators, &lt;code&gt;1:50&lt;/code&gt; and &lt;code&gt;51:100&lt;/code&gt;. We can also make
overlapping partitions, for example, &lt;code&gt;1:50&lt;/code&gt;, &lt;code&gt;2:51&lt;/code&gt;, &lt;code&gt;3:52&lt;/code&gt;, etc. &lt;/p&gt;
&lt;h3&gt;Application: Moving average&lt;/h3&gt;
&lt;p&gt;One useful application of overlapping partitions is computing moving
averages. The following code imports Google&amp;#8217;s historical stock price from Yahoo
Finance and computes its 60-day moving average. &lt;/p&gt;
&lt;p&gt;First, we download the data, creating a 2-D array containing dates, volumes, and prices.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="n"&gt;googdata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;http://ichart.finance.yahoo.com/table.csv?s=&lt;span class="caps"&gt;GOOG&lt;/span&gt;&amp;amp;d=0&amp;amp;e=7&amp;amp;f=2014&amp;amp;g=d&amp;amp;a=0&amp;amp;b=7&amp;amp;c=2013&amp;amp;ignore=.csv&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="n"&gt;download&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="n"&gt;open&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="n"&gt;readall&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt; 
        &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;201&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
        &lt;span class="n"&gt;reverse&lt;/span&gt;  &lt;span class="c"&gt;# Dates start at most recent, so reverse for chron order.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We then create iterators over the dates and closing prices in the Array. These
iteratively extract and parse values from the relevant columns.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;googdata&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;close&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;parsefloat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;googdata&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can make 60-day sub-period partitions and compute the average of
each. Since I&amp;#8217;m using &lt;code&gt;imap&lt;/code&gt; nothing has been calculated yet. These are all just
iterators promising to do work when called.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ma60&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c"&gt;# &lt;span class="caps"&gt;NB&lt;/span&gt;: The Julian way to do this would be&lt;/span&gt;
&lt;span class="c"&gt;#     [mean(price[i-59:i]) for i = 60:length(price)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With all these useful iterators defined, I can just collect them into arrays for
plotting.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Geom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
     &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ma60&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Geom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
     &lt;span class="n"&gt;Guide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
     &lt;span class="n"&gt;Guide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;span class="caps"&gt;GOOG&lt;/span&gt; Daily Stock Price 60-Day Moving Avg.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="ma60chart"&gt;&lt;/div&gt;

&lt;script src="scripts/ma60.js"&gt;&lt;/script&gt;

&lt;script&gt;
draw("#ma60chart");
&lt;/script&gt;

&lt;h2&gt;Groupby&lt;/h2&gt;
&lt;p&gt;While the &lt;code&gt;Partition&lt;/code&gt; iterator makes partitions of specified lengths, the
&lt;code&gt;Groupby&lt;/code&gt; iterator splits an iterator based on some condition. One caveat is
that the input iterator has to be sorted in some way on the groupby condition,
so that values with the same condition are adjacent in the iterator. &lt;/p&gt;
&lt;h3&gt;Application: Do Labor Force figures follow Benford&amp;#8217;s Law?&lt;/h3&gt;
&lt;p&gt;In this example, I&amp;#8217;m going to look at &lt;a href="http://en.wikipedia.org/wiki/Benford%27s_law"&gt;Benford&amp;#8217;s Law&lt;/a&gt; using the &lt;code&gt;Groupby&lt;/code&gt;
iterator. Benford&amp;#8217;s Law posits that the leading digits of numbers in many
data sources follows a regular distribution. I&amp;#8217;ll use the &lt;code&gt;Groupby&lt;/code&gt; iterator to
group the data by first digit and check this.&lt;/p&gt;
&lt;p&gt;The data I&amp;#8217;ll examine is the size
of the labor force population in each &lt;span class="caps"&gt;U.S.&lt;/span&gt; county in 2012.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="n"&gt;lfdata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;http://www.bls.gov/lau/laucnty12.txt&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
                   &lt;span class="n"&gt;download&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
                   &lt;span class="n"&gt;open&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
                   &lt;span class="n"&gt;readall&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
                   &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\r\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
                   &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;125&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;  &lt;span class="c"&gt;# Rows with data&lt;/span&gt;
                   &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;92&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;      &lt;span class="c"&gt;# Column w/ &lt;span class="caps"&gt;LF&lt;/span&gt; data&lt;/span&gt;
                   &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;  &lt;span class="c"&gt;# 1,000 -&amp;gt; 1000&lt;/span&gt;
                   &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;                          &lt;span class="c"&gt;# Remove header&lt;/span&gt;
                   &lt;span class="n"&gt;sort&lt;/span&gt;                                    
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The analysis is simple with a &lt;code&gt;Groupby&lt;/code&gt; iterator. It splits up the data by
leading digit, and then I just calculate the frequency of each leading digit in
the data by taking the length of each leading-digit group as a share of
the total length of the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;dgroups&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lfdata&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c"&gt;# Groups by first digit&lt;/span&gt;
&lt;span class="c"&gt;# Extract the digit from the group members&lt;/span&gt;
&lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;parseint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt; &lt;span class="n"&gt;dgroups&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# Compute the frequency&lt;/span&gt;
&lt;span class="n"&gt;frequency&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;imap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lfdata&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dgroups&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Benford&amp;#8217;s Law posits that the frequency of digit &lt;em&gt;d&lt;/em&gt; in data should be
log&lt;sub&gt;10&lt;/sub&gt;(&lt;em&gt;d+1&lt;/em&gt;) - log&lt;sub&gt;10&lt;/sub&gt;(&lt;em&gt;d&lt;/em&gt;). This function prints out a
table of the observed frequencies next to the expected frequencies per Benford&amp;#8217;s Law.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;benfordcheck&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obs_freqs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pred_freqs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Digit Frequency Compared to Benford&amp;#39;s Law&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;=========================================&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Digit  Observed  Expected  Difference&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;obs_freqs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred_freqs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;%5d %9.2f %9.2f %11.2f&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can see the labor force data follows Benford&amp;#8217;s Law quite closely.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;benfordcheck&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;Digit Frequency Compared to Benford&amp;#39;s Law
=========================================

Digit  Observed  Expected  Difference
    1     30.09     30.10       -0.01
    2     16.46     17.61       -1.15
    3     12.02     12.49       -0.48
    4      9.72      9.69        0.03
    5      8.29      7.92        0.37
    6      6.30      6.69       -0.39
    7      6.02      5.80        0.23
    8      5.84      5.12        0.72
    9      5.25      4.58        0.67
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To plot the comparison, I can collect the values from our iterators into a
DataFrame.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;benford_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="c"&gt;# Extract the digit&lt;/span&gt;
                       &lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                       &lt;span class="n"&gt;observed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;frequency&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                       &lt;span class="n"&gt;expected&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="barchart"&gt;&lt;/div&gt;

&lt;script src="./scripts/bl.js"&gt;&lt;/script&gt;

&lt;script&gt;
draw("#barchart");
&lt;/script&gt;

&lt;h2&gt;Iterate&lt;/h2&gt;
&lt;p&gt;Though its name might be a little confusing, the &lt;code&gt;Iterate&lt;/code&gt; iterator is one of my
favorites. It recursively applies a function to a starting value, that is
&lt;code&gt;f(...f(f(f(x)))...)&lt;/code&gt;. I come across applications for it all over the place.&lt;/p&gt;
&lt;h3&gt;Application: Autoregressive time series processes&lt;/h3&gt;
&lt;p&gt;One application is producing autoregressive time series processes. An &lt;span class="caps"&gt;AR&lt;/span&gt;(1)
process has the form &lt;em&gt;x&lt;sub&gt;t+1&lt;/sub&gt; = px&lt;sub&gt;t&lt;/sub&gt; + e&lt;sub&gt;t+1&lt;/sub&gt;&lt;/em&gt;, where
&lt;em&gt;e&lt;/em&gt; is some random noise. If
We define the function &lt;em&gt;f(x) = px + e&lt;/em&gt;, then &lt;em&gt;x&lt;sub&gt;t+2&lt;/sub&gt;&lt;/em&gt; as a function
of &lt;em&gt;x&lt;sub&gt;t&lt;sub&gt;&lt;/em&gt; is &lt;em&gt;f(f(x&lt;sub&gt;t&lt;/sub&gt;))&lt;/em&gt;. Subsequent values can be similarly
produced by iteratively applying the function.&lt;/p&gt;
&lt;p&gt;First the code for the &lt;span class="caps"&gt;AR&lt;/span&gt;(1) function itself, along with a helper function for
plotting a realization of the process.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; ar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;plotar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arseq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arseq&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arseq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Geom&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                            &lt;span class="n"&gt;Guide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Time&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Guide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                            &lt;span class="n"&gt;Guide&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Defining a coefficient and a standard deviation for the random variable, I pass
them through a process that creates an iterator that recursively applies the
function, starting with a randomly-drawn initial value. Then I collect 250 values of
that iterator and plot them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="n"&gt;ar1coef&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="n"&gt;ar1sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.15&lt;/span&gt;                                           

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ar1coef&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ar1sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;iterate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ar1sigma&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;plotar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;span class="caps"&gt;AR&lt;/span&gt;(1) Time Series&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="ar1chart"&gt;&lt;/div&gt;

&lt;script src="scripts/ar1.js"&gt;&lt;/script&gt;

&lt;script&gt;
draw("#ar1chart");
&lt;/script&gt;

&lt;p&gt;This idea can be extended an &lt;span class="caps"&gt;AR&lt;/span&gt;(p) process, where the current value of &lt;em&gt;x&lt;/em&gt;
depends on several past values. Whereas the coefficient was a scalar in the
&lt;span class="caps"&gt;AR&lt;/span&gt;(1) model, it&amp;#8217;s a matrix now, but the formula is otherwise the same.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="nf"&gt; ar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coeffs&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="n"&gt;AbstractVector&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="kt"&gt;Float64&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coeffs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;coeffs&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="k"&gt;end&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),:]]&lt;/span&gt;
    &lt;span class="n"&gt;Sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Phi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Sigma&lt;/span&gt; &lt;span class="o"&gt;.*&lt;/span&gt; &lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For an example, here&amp;#8217;s 250-periods simulated from an &lt;span class="caps"&gt;AR&lt;/span&gt;(3) model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="n"&gt;ar3coeffs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="kd"&gt;const&lt;/span&gt; &lt;span class="n"&gt;ar3sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.15&lt;/span&gt;

&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ar3coeffs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ar3sigma&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ar&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;iterate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ar3sigma&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;take&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;|&amp;gt;&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;plotar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;span class="caps"&gt;AR&lt;/span&gt;(3) Time Series&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="ar3chart"&gt;&lt;/div&gt;

&lt;script src="scripts/ar3.js"&gt;&lt;/script&gt;

&lt;script&gt;
draw("#ar3chart");
&lt;/script&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Most iteration you&amp;#8217;ll see in the wild uses simple collections or ranges as the
iterator, performing extensive work inside the loop. Sometimes our problem can be
better expressed using more complicated iterators whose structure represents the
logic of our iteration. One thing to notice in all the examples was that once
the iterators were defined, there was very little to do after iterating over
them. Typically I was just collecting the iteration values into an array, or
reducing them with an operation to a scalar result. We were also able to build
the problems in such a way that calculation of values in the iterators was
delayed until absolutely necessary.&lt;/p&gt;
&lt;p&gt;There are tradeoffs to this sort of style, and much of the stuff in this
post was more cute than practical. But it was a fun exploration of how to create types
that interact with protocols in Julia. Julia&amp;#8217;s type system and dispatch design
are very powerful and interesting, and gives programmers a lot of flexibility in
expressing their problems.&lt;/p&gt;
&lt;hr /&gt;
&lt;ol class="footnotes"&gt;

&lt;li&gt;&lt;a name="fn-multdisp"&gt;&lt;/a&gt;
While
we&amp;#8217;ll see lots of examples of extending Julia&amp;#8217;s base functions dispatched on
newly-defined types, we won&amp;#8217;t see much &lt;i&gt;multiple dispatch&lt;/i&gt;, which is an
important design feature of Julia. In fact, pretty much everything here could be
implemented in a single-dispatch &lt;span class="caps"&gt;OO&lt;/span&gt; language.
&lt;a href="#fnm-multdisp"&gt;&lt;i class="fa fa-level-up"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a name="fn-iterable"&gt;&lt;/a&gt;
Pythonistas
may be thinking about the distinction between &lt;i&gt;iterators&lt;/i&gt; and &lt;i&gt;iterables&lt;/i&gt;. (See,
e.g. &lt;a href=" http://stackoverflow.com/questions/9884132/understanding-pythons-iterator-iterable-and-iteration-protocols-what-exact"&gt;this Stack Overflow thread&lt;/a&gt;.) That distinction doesn&amp;#8217;t really apply to
Julia. So I won&amp;#8217;t use the term iterable here, and I&amp;#8217;ll define an iterator in the two
ways discussed above: (1) it is valid in a &lt;code&gt;for i = I&lt;/code&gt; statement, and (2) it
implements the iterator protocol.
&lt;a href="#fnm-iterable"&gt;&lt;i class="fa fa-level-up"&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a name="fn-arrcomp"&gt;&lt;/a&gt;
This limitation seems
to come from the idea that only iterators with known lengths can be counted on
to produce multidimensional arrays. This may be changed in future versions of
Julia. See, e.g. &lt;a href="https://github.com/JuliaLang/julia/issues/550"&gt;Issue #550&lt;/a&gt;.  The &lt;code&gt;collect&lt;/code&gt; function uses the
&lt;code&gt;push!&lt;/code&gt; function to dynamically allocate the array, but &lt;code&gt;collect&lt;/code&gt; can only give
a 1-D Array output, whereas comprehensions can be multidimensional.
&lt;a href="#fnm-arrcomp"&gt;&lt;i class="fa fa-level-up"&gt;
&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a name="fn-filter"&gt;&lt;/a&gt;
Actually, Julia&amp;#8217;s &lt;code&gt;filter&lt;/code&gt; function already does this. If you pass
that function a predicate or condition function and an iterator, it produce a
&lt;code&gt;Filter&lt;/code&gt; object that you can then iterate over. This is different from
&lt;code&gt;map&lt;/code&gt; which can take an input iterator, but returns the result of
mapping the function immediately.
&lt;a href="#fnm-filter"&gt;&lt;i class="fa fa-level-up"&gt;
&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;

&lt;/ol&gt;</summary><category term="julia"></category></entry><entry><title>Pardon the dust</title><link href="http://slendermeans.org/pardon-the-dust.html" rel="alternate"></link><updated>2013-09-30T00:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2013-09-30:pardon-the-dust.html</id><summary type="html">&lt;p&gt;&lt;strong&gt; Update 9/10/2013 &lt;/strong&gt; New posts are going up on the blog, but I&amp;#8217;m going to keep this post at the top for a while. Consider the site in beta for the moment, and please use the comment section of this post to report any issues. If you&amp;#8217;re using &lt;span class="caps"&gt;IE&lt;/span&gt; to try and view the site, I&amp;#8217;m sorry. But I&amp;#8217;m not that sorry.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Update 9/3/2013 &lt;/strong&gt; Things should be working reasonably well. A few kinks to work out, and I have to migrate the former site&amp;#8217;s comments, but the current site is pretty much ready to go.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This is the new home for my blog, &lt;em&gt;Slender Means&lt;/em&gt;. It&amp;#8217;s currently in-progress, and I&amp;#8217;m still finishing up the design, and fixing weird links and typos from the Wordpress to Pelican migration.&lt;/p&gt;
&lt;p&gt;In the meantime, a more usable version sits at the old home: &lt;a href="http://slendrmeans.wordpress.com"&gt;http://slendrmeans.wordpress.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for visiting!&lt;/p&gt;
&lt;p&gt;-c.&lt;/p&gt;</summary><category term=""></category></entry><entry><title>Machine Learning for Hackers Chapter 8: Principal Components Analysis</title><link href="http://slendermeans.org/ml4h-ch8.html" rel="alternate"></link><updated>2013-09-06T17:30:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2013-09-06:ml4h-ch8.html</id><summary type="html">&lt;p&gt;The &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch8/ch8.ipynb"&gt;code for Chapter 8&lt;/a&gt; has been sitting around for a long time now. Let&amp;#8217;s blow the dust off and check it out. One thing before we start: explaining &lt;span class="caps"&gt;PCA&lt;/span&gt; well is kinda hard. If any experts reading feel like I&amp;#8217;ve described something imprecisely (and have a better description), I&amp;#8217;m very open to suggestions.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Chapter 8 is about &lt;em&gt;Principal Components Analysis&lt;/em&gt; (&lt;span class="caps"&gt;PCA&lt;/span&gt;), which the authors perform on data with time series of prices for 24 stocks. In very broad terms, &lt;span class="caps"&gt;PCA&lt;/span&gt; is about projecting many real-life, observed variables onto a smaller number of &amp;#8220;abstract&amp;#8221; variables, the principal components. Principal components are selected in order to best preserve the variation and correlation of the original variables. For example, if we have 100 variables in our data, which are all highly correlated, we can project them down to just a few principal components&amp;#8212;-i.e., the high correlation between them can be imagined as coming from an underlying factor that drives all of them, with some other less important factors driving their differences. When variables aren&amp;#8217;t highly correlated, more principal components are needed to describe them well.&lt;/p&gt;
&lt;p&gt;As you might imagine, &lt;span class="caps"&gt;PCA&lt;/span&gt; can be a very effective way of dealing with multi-collinearity that crops up in datasets with lots of variables. The downside is that &lt;span class="caps"&gt;PCA&lt;/span&gt; is just a mechanical process that is independent of the phenomenon we&amp;#8217;re studying; the &amp;#8220;principal components&amp;#8221; we find don&amp;#8217;t have to have any real-world meaning&amp;#8212;-they&amp;#8217;re just mathematical constructs. Sometimes we can give meaningful interpretations to the principal components by analogizing them to real underlying factors that theoretically drive our data. But this can be tricky, from both a technical and epistemological standpoint.&lt;/p&gt;
&lt;p&gt;For the stocks the authors analyze, they ultimately try and reduce their description to a single principal component, which they interpret as a kind of &amp;#8220;market-wide&amp;#8221; factor, and compare with a broad market index (here the &lt;span class="caps"&gt;DJIA&lt;/span&gt;). This is a not uncommon application of &lt;span class="caps"&gt;PCA&lt;/span&gt; in stock analysis. But they&amp;#8217;ve got a technical problem here.&lt;/p&gt;
&lt;p&gt;To perform &lt;span class="caps"&gt;PCA&lt;/span&gt;, your data have to have a meaningful covariance matrix (or correlation matrix, but the conditions are equivalent). They analyze stock &lt;em&gt;prices&lt;/em&gt;, which are non-stationary time series variables. This means their covariance matrices change with time, so you can&amp;#8217;t really estimate a meaningful covariance matrix from a sample of data. Your estimator implicitly assumes the data are stationary, so your estimated covariance matrix is meaningless. If we calculate the stock &lt;em&gt;returns&lt;/em&gt; in the data, though, we can do &lt;span class="caps"&gt;PCA&lt;/span&gt; properly, and we&amp;#8217;ll see the relationship of the resulting principal component with the broad market index is much cleaner.&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re comfortable with &lt;span class="caps"&gt;PCA&lt;/span&gt; already, you don&amp;#8217;t really have to worry about the conceptual content of this chapter. If you&amp;#8217;re not, my advice it to take this chapter as a decent toy example of where and why one uses &lt;span class="caps"&gt;PCA&lt;/span&gt;, but don&amp;#8217;t apply what&amp;#8217;s done here elsewhere without learning more first. I&amp;#8217;m not going to explain &lt;span class="caps"&gt;PCA&lt;/span&gt; in any detail; I just want to show where &lt;span class="caps"&gt;PCA&lt;/span&gt; functions live in the Python ecosystem and how they work. But, like most machine learning techniques, it shouldn&amp;#8217;t be used at home without adult supervision.&lt;/p&gt;
&lt;p&gt;As usual the IPython notebook lives at the Github repo &lt;a href="https://github.com/carljv/Will_it_Python/blob/master/MLFH/ch8/ch8.ipynb"&gt;here&lt;/a&gt;, and can be viewed via nbviewer &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch8/ch8.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Stock data munging&lt;/h2&gt;
&lt;p&gt;The raw data are in a long format, with (no. of stocks) &amp;times; (no. of days) rows, and three columns (a date, a stock ticker and a price for that ticker on that day). This sort of dataset lends itself to a pandas DataFrame with a hierarchical index&amp;#8212;and since there&amp;#8217;s only one variable in the data (the price), we&amp;#8217;ll &lt;code&gt;squeeze&lt;/code&gt; the DataFrame to get a Series. The Dow Jones data, containing just one ticker, is more straightforward.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;prices_long&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/stock_prices.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                       &lt;span class="n"&gt;parse_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;squeeze&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dji_all&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/dji.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;squeeze&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With the stock data indexed this way, it&amp;#8217;s easy to create a &lt;code&gt;date&lt;/code&gt; &amp;times; &lt;code&gt;ticker&lt;/code&gt; DataFrame with prices as entries&amp;#8212;-we just use &lt;code&gt;unstack&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;prices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prices_long&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Since we&amp;#8217;ll ultimately want to perform this analysis with price returns, I&amp;#8217;m going to create a similar dataset, just with returns instead of prices (note this will have one less day of data, since we don&amp;#8217;t know the return for the first day in the data).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;calc_returns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;calc_returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note I&amp;#8217;m using log returns here. Pandas DataFrames have a &lt;code&gt;pct_change&lt;/code&gt; method that would provide another way of computing returns.&lt;/p&gt;
&lt;p&gt;The authors&amp;#8217; &lt;span class="caps"&gt;PCA&lt;/span&gt; strategy here is to extract a &amp;#8220;stock index&amp;#8221; factor from the stock data by using the first principal component&amp;#8212;-this is the single principal component that captures the most variation in the underlying data.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;make_pca_index&lt;/code&gt; is going to extract this first principal component, using the &lt;code&gt;PCA&lt;/code&gt; function in scikit-learn&amp;#8217;s &lt;code&gt;sklearn.decomposition&lt;/code&gt; module. This is not the only way to get a &lt;span class="caps"&gt;PCA&lt;/span&gt; in Python&amp;#8212;-indeed &lt;span class="caps"&gt;PCA&lt;/span&gt; is mechanically just an eigen-decomposition of the data&amp;#8217;s correlation or covariance, so you could do this all from scratch in Numpy. The scikit-learn implementation, though, gives us a convenient &lt;code&gt;PCA&lt;/code&gt; object to work with. And as usual with scikit-learn, the &lt;a href="http://scikit-learn.org/stable/modules/decomposition.html#pca"&gt;documentation&lt;/a&gt; is very good.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;PCA&lt;/code&gt; function works with a covariance or correlation matrix. We&amp;#8217;re going to use a correlation matrix here; and our function will just take either stock price or return data, compute its correlation, then find the first principal component of the data. Notice the sign change I do at the end there&amp;#8212;-the component ended up being negatively related to the data (i.e. when this factor goes down, the data go up, etc.). &lt;span class="caps"&gt;PCA&lt;/span&gt; results are typically sign- and scale-invariant; hence the problems with interpretation. We&amp;#8217;ll make our resulting index a &amp;#8220;postive&amp;#8221; one by just reversing the sign.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_pca_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Compute the correlation matrix of a set of stock data, and return&lt;/span&gt;
&lt;span class="sd"&gt;    the first principal component.&lt;/span&gt;

&lt;span class="sd"&gt;    By default, the data are scaled to have mean zero and variance one&lt;/span&gt;
&lt;span class="sd"&gt;    before performing the &lt;span class="caps"&gt;PCA&lt;/span&gt;.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;scale_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;data_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;data_std&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
    &lt;span class="n"&gt;corrs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_std&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cov&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;pca&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;PCA&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;corrs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c"&gt;# We end up getting a negative value for the index, so we&amp;#39;ll reverse&lt;/span&gt;
    &lt;span class="c"&gt;# the sign to have it be more intuitive.&lt;/span&gt;
    &lt;span class="n"&gt;mkt_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_std&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mkt_index&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;A &lt;span class="caps"&gt;PCA&lt;/span&gt; index with price data&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s copy the authors and make an index from raw price data. Since prices don&amp;#8217;t have meaningully-estimated correlations, this isn&amp;#8217;t really correct, but it&amp;#8217;s useful to compare with what&amp;#8217;s in the book.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;price_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_pca_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To see what&amp;#8217;s going on, let&amp;#8217;s make two plots: a scatter plot of our &lt;span class="caps"&gt;PCA&lt;/span&gt; index with the &lt;span class="caps"&gt;DJIA&lt;/span&gt;, and a time series plot with the two indices. These correspond to figures 8-4 and 8-5 in the book.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;121&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;price_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dji&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;k.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;span class="caps"&gt;PCA&lt;/span&gt; index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Dow Jones Index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ols_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;OLSreg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dji&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;price_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;price_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ols_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fittedvalues&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
         &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;R2 = &lt;/span&gt;&lt;span class="si"&gt;%4.3f&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ols_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rsquared&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;upper left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;122&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;span class="caps"&gt;PCA&lt;/span&gt; Price Index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dji&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;span class="caps"&gt;DJ&lt;/span&gt; Index&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;upper left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a class = "image" href="../images/pca_price.png"&gt;
  &lt;img src="../images/pca_price.png" width=400 /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This actually seems to look okay, and wouldn&amp;#8217;t really alert us to any problems if we didn&amp;#8217;t know any better. Let&amp;#8217;s repeat the exercise with returns, though.&lt;/p&gt;
&lt;h3&gt;A &lt;span class="caps"&gt;PCA&lt;/span&gt; index with returns data&lt;/h3&gt;
&lt;p&gt;Since returns are stationary, we can estimate a meaningful correlation matrix, and our &lt;span class="caps"&gt;PCA&lt;/span&gt; will make more sense.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;returns_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_pca_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;returns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And the plots:&lt;/p&gt;
&lt;p&gt;&lt;a class = "image" href="../images/pca_returns.png"&gt;
  &lt;img src="../images/pca_returns.png" width=400 /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Looking at these, we see a much more straightforward linear relationship between the returns to the &lt;span class="caps"&gt;DJIA&lt;/span&gt; and the &lt;span class="caps"&gt;PCA&lt;/span&gt; index derived from stock returns.&lt;/p&gt;
&lt;h3&gt;Explained variance&lt;/h3&gt;
&lt;p&gt;Since the principal components are just eigenvalues, there will be as many of them as their are columns in our data (here 24). As we add components we explain more and more of the original correlation matrix. Once we add all 24 the amount of variation/correlation explained is 100%&amp;#8212;-all we&amp;#8217;ve done is define a rotation of the matrix, so there&amp;#8217;s no information lost. But a plot of the cumulative explained variance as we add principal components can help us to see how far and how reliably the data can be reduced.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pca_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;No. of principal components&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Cumulative variance explained&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a class="image" href="../images/pca_variance.png"&gt;
  &lt;img src="../images/pca_variance.png" width=400 /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Factor loadings&lt;/h3&gt;
&lt;p&gt;We can also check out the loadings of the principal component across the stocks. What this shows us is how a change in the relates to the stocks in our data. For example a a component going up might cause half the stock returns in the data to go up and half to go down (it would positively load on some and negatively load on others.) We would expect, intuitively, a factor representing &amp;#8220;the market,&amp;#8221; as we think our first component does, to load on our stocks in the same direction, and roughly the same magnitude. And this is basically what we see.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bar&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;pca_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;components_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a class="image" href="../images/pca_loadings.png"&gt;
  &lt;img src="../images/pca_loadings.png" width=400 /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Since &lt;span class="caps"&gt;PCA&lt;/span&gt; is such a widely-used and fundamental technique, it&amp;#8217;s important to know how to do it in Python, and the scikit-learn implementation is a good one. Check out the documentation &lt;a href="http://scikit-learn.org/stable/modules/decomposition.html#pca"&gt;here&lt;/a&gt;. Of course, like any statistical technique, &lt;span class="caps"&gt;PCA&lt;/span&gt; can definitely be misused, or at least easily misintepreted, so handle with care.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>I’ve seen the best minds of my generation destroyed by Matlab …</title><link href="http://slendermeans.org/julia-loops.html" rel="alternate"></link><updated>2013-05-11T16:52:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2013-05-11:julia-loops.html</id><summary type="html">&lt;p&gt;(Note: this is very quick and not well thought out. Mostly a
conversation starter as opposed to any real thesis on the subject.)&lt;/p&gt;
&lt;p&gt;This post is a continuation of a Twitter conversation &lt;a href="https://twitter.com/johnmyleswhite/status/332920041626554369"&gt;here&lt;/a&gt;, started
when John Myles White poked the hornets&amp;#8217; nest. (Python&amp;#8217;s nest? Where do
Pythons live?)&lt;/p&gt;
&lt;p&gt;&lt;img src="../images/jmw_tweet.jpg" width=450px /&gt;&lt;/p&gt;
&lt;p&gt;The gist with John&amp;#8217;s code is &lt;a href="https://gist.github.com/johnmyleswhite/5556201"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This isn&amp;#8217;t a very thoughtful post. But the conversation was becoming
sort of a shootout and my thoughts (half-formed as they are) were a bit
longer than a tweet. Essentially, I think the Python performance
shootouts&amp;#8212;PyPy, Numba, Cython&amp;#8212;are missing the point.&lt;/p&gt;
&lt;p&gt;The point is, I think, that loops are a crutch. A 3-nested for loop in
Julia that increments a counter takes 8 lines of code (1 initialize
counter, 3 for statements, 1 increment statement, 3 end statements).
Only one of those lines tells me what the code does.&lt;/p&gt;
&lt;p&gt;But most scientific programmers learned to code in imperative languages
and that style of thinking and coding has become natural. I&amp;#8217;ve often
seen comments like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="forloop_tweet" src="../images/forloop_tweet.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;Which I think simply equates readability with familiarity. That isn&amp;#8217;t
wrong, but it isn&amp;#8217;t the whole story.&lt;/p&gt;
&lt;p&gt;Anyway, a lot of the responses to John&amp;#8217;s code were showing that, hey,
you can get fast loops in Python, with either JITing (PyPy, Numba) or
Cython. So here are my thoughts:&lt;/p&gt;
&lt;p&gt;​1. Cython is great. I&amp;#8217;ve used it with great success myself. But Julia
   gives me fast loops while keeping the dynamic typing; i.e., I&amp;#8217;m still
   writing in Julia. Cython is a manifestation of what the Julia developers
   call the &amp;#8220;two-language problem.&amp;#8221;  My programmer-productivity happens in
   the slow, dynamic language, and I swap to a more painful language for
   critical bottlenecks and glue the two together. Cython is a more
   pleasant manifestation of the problem, especially since it lets you
   evolve in an exploratory, piece-meal way from your first language to
   your second language. But you still end up with code that is nice
   dynamic-typing and abstractions on the outside; gross static typing and
   low-level imperative stuff on the inside. (And Cython examples are often
   clean and simple, but the code can get hairy very quickly.)&lt;/p&gt;
&lt;p&gt;​2. One of the nice things about the slow for loops in Python and R is
   that they force you to think about other ways to express your problem. R
   and Python programmers start thinking about how they can exploit arrays
   and other ADTs, and higher-order functions to express they&amp;#8217;re problem.
   Avoiding the loop performance hit is the first reason, but then many of
   them start to realize they like their code better this way. The
   adjustment is hard at first, but once you get their, it&amp;#8217;s hard to go
   back.&lt;/p&gt;
&lt;p&gt;Forget about the Numpy, PyPy, Cython solutions to John&amp;#8217;s problem. I
think it&amp;#8217;s safe to say his original pure Python code would be considered
pretty un-Pythonic, to the extent that&amp;#8217;s a thing. Python programmers are
discouraged from that style of writing-C-in-Python, for both performance
reasons, and conceptual reasons. Python programmers just think the
alternatives (e.g. list comprehensions) are more expressive and
maintainable. They&amp;#8217;re not avoiding for loops because they&amp;#8217;re slow: they
don&amp;#8217;t &lt;strong&gt;want&lt;/strong&gt; to write for loops.&lt;/p&gt;
&lt;p&gt;Maybe Julia is the answer to this
problem. Since list comprehensions, higher-order-functions (applies,
maps, etc.) wrap imperative loops, and Julia loops are fast, then these
things can be written in Julia and be fast.&lt;/p&gt;
&lt;p&gt;But that requires some thought about how
Julia devs want Julia programmers to program. Julia is great and
really promising, and it&amp;#8217;s got an opportunity to let scientific
programmers really raise their game. But I&amp;#8217;d hate the big pitch for
Julia to be: hey, you can write fast loops! And it would basically
become a refuge for people who never learned to properly code R and are
are fed up with slow loops, or for Matlab guys who&amp;#8217;s licenses ran out.&lt;/p&gt;</summary><category term="julia"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 7: Numerical optimization with deterministic and stochastic methods</title><link href="http://slendermeans.org/ml4h-ch7.html" rel="alternate"></link><updated>2013-02-12T18:51:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2013-02-12:ml4h-ch7.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Chapter 7 of &lt;em&gt;Machine Learning for Hackers&lt;/em&gt; is about numerical
optimization. The authors organize the chapter around two examples of
optimization. The first is a straightforward least-squares problem like
that we&amp;#8217;ve encountered already doing linear regressions, and is amenable
to standard iterative algorithms (e.g. gradient descent). The second is
a problem with a discrete search space, not clearly differentiable, and
so lends itself to a stochastic/heuristic optimization technique (though
we&amp;#8217;ll see the optimization problem is basically artificial). The first
problem gives us a chance to play around with Scipy&amp;#8217;s optimization
routines. The second problem has us hand-coding a Metropolis algorithm;
this doesn&amp;#8217;t show off much new Python, but it&amp;#8217;s fun nonetheless.&lt;/p&gt;
&lt;p&gt;The notebook for this chapter is at the github report &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/ch7"&gt;here&lt;/a&gt;, or you
can view it online via nbviewer &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch7/ch7.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Ridge regression by least-squares&lt;/h2&gt;
&lt;p&gt;In &lt;a href="../ml4h-ch6.html"&gt;chapter 6&lt;/a&gt; we estimated &lt;span class="caps"&gt;LASSO&lt;/span&gt; regressions, which added an L1
penalty on the parameters to the &lt;span class="caps"&gt;OLS&lt;/span&gt; loss-function. The ridge regression
works the same way, but applies an L2 penalty to the parameters. The
ridge regression is a somewhat more straightforward optimization
problem, since the L2 norm we use gives us a differentiable loss
function.&lt;/p&gt;
&lt;p&gt;In this example, we&amp;#8217;ll regress weight on height, similar to &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch5/ch5.ipynb"&gt;chapter
5&lt;/a&gt;. We can specify the loss (sum of squared errors) function for the
ridge regression with the following function in Python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;Xmat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Height&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;prepend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ridge_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Compute &lt;span class="caps"&gt;SSE&lt;/span&gt; of the ridge regression.&lt;/span&gt;
&lt;span class="sd"&gt;    This is the normal regression &lt;span class="caps"&gt;SSE&lt;/span&gt;, plus the&lt;/span&gt;
&lt;span class="sd"&gt;    L2 cost of the parameters.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;predicted&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;sse&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sse&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The authors use R&amp;#8217;s &lt;code&gt;optim&lt;/code&gt; function, which defaults to the Nelder-Mead
simplex algorithm. This algorithm doesn&amp;#8217;t use any gradient or Hessian
information to optimize the function. We&amp;#8217;ll want to try out some
gradient methods, though. Even though the functions for these methods
will compute numerical gradients and Hessians for us, for the ridge
problem these are easy enough to specify explicitly.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ridge_grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    The gradiant of the ridge regression &lt;span class="caps"&gt;SSE&lt;/span&gt;.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;grad&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;
    &lt;span class="n"&gt;grad&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;grad&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ridge_hess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;The hessian of the ridge regression &lt;span class="caps"&gt;SSE&lt;/span&gt;.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;lam&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Like the &lt;span class="caps"&gt;LASSO&lt;/span&gt; regressions we worked with in &lt;a title&gt;chapter 6&lt;/a&gt;, the
ridge requires a penalty parameter to weight the L2 cost of the
coefficient parameters (called &lt;code&gt;lam&lt;/code&gt; in the functions above; &lt;code&gt;lambda&lt;/code&gt; is
a keyword in Python). The authors assume we&amp;#8217;ve already found an
appropriate value via cross-validation, and that value is 1.0.&lt;/p&gt;
&lt;p&gt;We can now try to minimize the loss function with a couple of different
algorithms. First the Nelder-Mead simplex, which should correspond to
the authors&amp;#8217; use of &lt;code&gt;optim&lt;/code&gt; in R.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Starting values for the a, b (intercept, slope) parameters&lt;/span&gt;
&lt;span class="n"&gt;params0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c"&gt;# Nelder-Mead simplex&lt;/span&gt;
&lt;span class="n"&gt;ridge_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Solution: a = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt;, b = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1612442.197636&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;117&lt;/span&gt;
&lt;span class="n"&gt;Function&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;221&lt;/span&gt;
&lt;span class="n"&gt;Solution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;340.565&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;7.565&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the Newton conjugate-gradient method. We need to give this function
a gradient; the Hessian is optional. First without the Hessian:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fmin_ncg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fprime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ridge_grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Solution: a = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt;, b = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1612442.197636&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Function&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;Gradient&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;
&lt;span class="n"&gt;Hessian&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;Solution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;340.565&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;7.565&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now supplying the Hessian:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fmin_ncg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fprime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;ridge_grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;fhess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ridge_hess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Solution: a = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt;, b = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1612442.197636&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Function&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="n"&gt;Gradient&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Hessian&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Solution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;340.565&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;7.565&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Fortunately, we get the same results for all three methods. Supplying
the Hessian to the Newton method shaves some time off, but in this
simple application, it&amp;#8217;s not really worth coding up a Hessian function
(except for fun).&lt;/p&gt;
&lt;p&gt;Lastly, the &lt;span class="caps"&gt;BFGS&lt;/span&gt; method, supplied with the gradient:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fmin_ncg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_error&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fprime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ridge_grad&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;fhess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ridge_hess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Solution: a = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt;, b = &lt;/span&gt;&lt;span class="si"&gt;%8.3f&lt;/span&gt;&lt;span class="s"&gt; &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ridge_fit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1612442.197636&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Function&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;
&lt;span class="n"&gt;Gradient&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Hessian&lt;/span&gt; &lt;span class="n"&gt;evaluations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Solution&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;340.565&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;7.565&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For this simple problem, all of these methods work well. For more
complicated problems, there are considerations which would lead you to
prefer one over another, or perhaps to use them in combination. There
are also several more methods available, some which allow you to solve
constrained optimization problems. Check out the very good
&lt;a href="http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"&gt;documentation&lt;/a&gt;. Also note that if you&amp;#8217;re not into hand-coding
gradients, scipy has a function &lt;code&gt;derivative&lt;/code&gt; in its &lt;code&gt;misc&lt;/code&gt; module that
will compute numerical derivatives. In many cases, the functions will do
this automatically if you fail to provide a function to their gradient
arguments.&lt;/p&gt;
&lt;h2&gt;Optimizing on sentences with the Metropolis algorithm&lt;/h2&gt;
&lt;p&gt;The second example in this chapter is a &amp;#8220;code-breaking&amp;#8221; exercise. We
start with a message &amp;#8220;here is some sample text&amp;#8221;, which we encrypt using
a Ceasar cipher that shifts each letter in the message to the next
letter in the alphabet (with Z going to A). We can represent the cipher
(or any cipher) in Python with a dict that maps each letter to its
encrypted counterpart.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;letters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;d&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;e&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;f&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;j&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;p&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;q&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;s&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;v&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;ceasar_cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;letters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;letters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;letters&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])}&lt;/span&gt;
&lt;span class="n"&gt;inverse_ceasar_cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;ceasar_cipher&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ceasar_cipher&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;inverse_ceasar_cipher&lt;/code&gt; dict reverses the cipher, so we can get an
original message back from one that&amp;#8217;s been encrypted by the Ceasar
cipher. Based on these structures, let&amp;#8217;s make functions that will
encrypt and decrypt text.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cipher_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ceasar_cipher&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;# Split the string into a list of characters to apply&lt;/span&gt;
    &lt;span class="c"&gt;# the decoder over.&lt;/span&gt;
    &lt;span class="n"&gt;strlist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ciphered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;strlist&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ciphered&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decipher_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ceasar_cipher&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c"&gt;# Split the string into a list of characters to apply&lt;/span&gt;
    &lt;span class="c"&gt;# the decoder over.&lt;/span&gt;
    &lt;span class="n"&gt;strlist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# Invert the cipher dictionary (k, v) -&amp;gt; (v, k)&lt;/span&gt;
    &lt;span class="n"&gt;decipher_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;deciphered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;decipher_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;strlist&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;deciphered&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To decrypt our message, we&amp;#8217;ll design a Metropolis algorithm that
randomly proposes ciphers, decrypts the message according to the
proposed cipher, and see&amp;#8217;s how probable that message is based on a
lexical database of word frequency in Wikipedia.&lt;/p&gt;
&lt;p&gt;The following functions are used to generate proposal ciphers for the
Metropolis algorithm. The idea is to randomly generate ciphers and see
what text they result in. If the text resulting from a proposed cipher
is more likely (according to the lexical database) than the current
cipher, we accept the proposal. If it&amp;#8217;s not, we accept it wil a
probability that is lower the less likely the resulting text is.&lt;/p&gt;
&lt;p&gt;The method of generating new proposals is important. The authors use a
method that chooses a key (letter) at random from the current cipher,
and swaps its with some other letter. For example, if we start with the
Ceasar Cipher, our proposal might randomly choose to re-map A to N
(instead of B). The proposal would then be the same a the Ceasar Cipher,
but with A → N and M → B (since A originally mapped to B and M
originally mapped to N). This proposal-generating mechanism is
encapsulated in &lt;code&gt;propose_modified_cipher_from_cipher&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is inefficient in a few ways. First, the letter chosen to modify in
the cipher may not even appear in the text, so the proposed cipher won&amp;#8217;t
modify the text at all and you end up wasting cycles generating a lot of
useless proposals. Second, we may end up picking a letter that occurs in
a highly likely word, which will increase the probability of generating
an inferior proposal.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ll suggest another mechanism that, instead of selecting a letter from
the current cipher to re-map, will choose a letter amongst the non-words
in the current deciphered text. For example, if our current deciphered
text is &amp;#8220;hello wqrld&amp;#8221;, we will only select amongst {w, q, r, l, d} to
modify at random. The minimizes the chances that a modified cipher will
turn real words into gibberish and produce less likely text. The
function propose_modified_cipher_from_text performs this proposal
mechanism.&lt;/p&gt;
&lt;p&gt;One way to think of this is that it&amp;#8217;s analogous to tuning the variance
of the proposal distribution in the typical Metropolis algorithm. If the
variance is too low, our algorithm won&amp;#8217;t efficiently explore the target
distribution. If it&amp;#8217;s too high, we&amp;#8217;ll end up generating lots of lousy
proposals. Our cipher proposal rules can suffer from similar problems.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_random_cipher&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Randomly generate a cipher dictionary (a one-to-one letter -&amp;gt; letter&lt;/span&gt;
&lt;span class="sd"&gt;    map).&lt;/span&gt;
&lt;span class="sd"&gt;    Used to generate the starting cipher of the algorithm.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;letters&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;letters&lt;/span&gt;&lt;span class="p"&gt;[:]&lt;/span&gt;
    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;cipher_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;modify_cipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_output&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Swap a single key in a cipher dictionary.&lt;/span&gt;

&lt;span class="sd"&gt;    Old: a -&amp;gt; b, ..., m -&amp;gt; n, ...&lt;/span&gt;
&lt;span class="sd"&gt;    New: a -&amp;gt; n, ..., m -&amp;gt; b, ...&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;decipher_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;old_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;new_cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;new_cipher&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_output&lt;/span&gt;
    &lt;span class="n"&gt;new_cipher&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;decipher_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_output&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;old_output&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new_cipher&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;propose_modified_cipher_from_cipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                        &lt;span class="n"&gt;lexical_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lexical_database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Generates a new cipher by choosing and swapping a key in the&lt;/span&gt;
&lt;span class="sd"&gt;    current cipher.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="c"&gt;# Unused&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;new_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;letters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;modify_cipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;propose_modified_cipher_from_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;lexical_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lexical_database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Generates a new cipher by choosing a swapping a key in the current&lt;/span&gt;
&lt;span class="sd"&gt;    cipher, but only chooses keys that are letters that appear in the&lt;/span&gt;
&lt;span class="sd"&gt;    gibberish words in the current text.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;deciphered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decipher_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;letters_to_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;deciphered&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;letters_to_sample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;letters_to_sample&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;deciphered&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;letters_to_sample&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;new_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;letters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;modify_cipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we need to be able to compute a message&amp;#8217;s likelihood (from the
lexical database). The log-likelihood of a message is just the sum of
the log-likelihoods of each word (one-gram) in the message. If the word
is gibberish (i.e., doesn&amp;#8217;t occur in the database) it gets a tiny
probability set to the smallest floating-point precision.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;one_gram_prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_gram&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lexical_database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_gram&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;finfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eps&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;text_logp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lexical_database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;deciphered&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decipher_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;logp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;one_gram_prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
    &lt;span class="n"&gt;deciphered&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;logp&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can now use these functions in our Metropolis algorithm. Each step in
the metropolis algorithm proposes a cipher, deciphers the text according
the proposal, and computes the log-likelihood of the deciphered message.
If the likelihood of the deciphered message is better under the proposal
cipher than the current cipher, we definitely accept that proposal for
our next step. If not, we only accept the proposal with a probability
based on the relative likelihood of the proposal to the current cipher.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll define this function to take an arbitrary proposal function via the
&lt;code&gt;proposal_rule&lt;/code&gt; argument. So far, this can be one of the two
&lt;code&gt;propose_modified_cipher_from_*&lt;/code&gt; functions defined above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;metropolis_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proposal_rule&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;lexical_database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;proposed_cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;proposal_rule&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lp1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_logp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lp2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_logp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proposed_cipher&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;lp2&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;lp1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;proposed_cipher&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lp2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;lp1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;proposed_cipher&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cipher_dict&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To run the algorithm, just wrap the step function inside a loop. There&amp;#8217;s
no stopping rule for the algorithm, so we have to choose a number of
iterations, and hope it&amp;#8217;s enough to get us to the optimum. Let&amp;#8217;s use
250,000.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;here is some sample text&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ciphered_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cipher_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ceasar_cipher&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;niter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250000&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;metropolis_decipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ciphered_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proposal_rule&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;niter&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_random_cipher&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;deciphered_text_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;logp_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;niter&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;logp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_logp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ciphered_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;current_deciphered_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decipher_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ciphered_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;deciphered_text_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_deciphered_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;logp_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;cipher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metropolis_step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ciphered_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cipher&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proposal_rule&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;deciphered_text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;deciphered_text_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;logp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;logp_list&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;niter&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First let&amp;#8217;s look at the authors&amp;#8217; proposal rule. While they managed to get a reasonable decrypted message
in about 50,000 iterations, we&amp;#8217;re still reading gibberish after 250,000.
As they say in the book, their results are an artefact of a lucky seed
value.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;results0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metropolis_decipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ciphered_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;propose_modified_cipher_from_cipher&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;niter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;results0&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

               &lt;span class="n"&gt;deciphered_text&lt;/span&gt;       &lt;span class="n"&gt;logp&lt;/span&gt;
 &lt;span class="mi"&gt;10000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fyrvbu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
 &lt;span class="mi"&gt;20000&lt;/span&gt; &lt;span class="n"&gt;wudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fbrkxu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;87.124919&lt;/span&gt;
 &lt;span class="mi"&gt;30000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fnrbau&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
 &lt;span class="mi"&gt;40000&lt;/span&gt; &lt;span class="n"&gt;wudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fmrjiu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;87.124919&lt;/span&gt;
 &lt;span class="mi"&gt;50000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fyrnbu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
 &lt;span class="mi"&gt;60000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fxrnvu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
 &lt;span class="mi"&gt;70000&lt;/span&gt; &lt;span class="n"&gt;pudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fvrnlu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;87.561022&lt;/span&gt;
 &lt;span class="mi"&gt;80000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fvrxgu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
 &lt;span class="mi"&gt;90000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fbrvtu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;100000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fjrnlu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;110000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fprbju&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;120000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fnrjcu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;130000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;flrvpu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;140000&lt;/span&gt; &lt;span class="n"&gt;puku&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;flrvxu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;88.028362&lt;/span&gt;
&lt;span class="mi"&gt;150000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fxrviu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;160000&lt;/span&gt; &lt;span class="n"&gt;pulu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;ftrdzu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;88.323162&lt;/span&gt;
&lt;span class="mi"&gt;170000&lt;/span&gt; &lt;span class="n"&gt;wuzu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;flrxdu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;89.575925&lt;/span&gt;
&lt;span class="mi"&gt;180000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;firamu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;190000&lt;/span&gt; &lt;span class="n"&gt;wudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fyrzqu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;87.124919&lt;/span&gt;
&lt;span class="mi"&gt;200000&lt;/span&gt; &lt;span class="n"&gt;wudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fnraxu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;87.124919&lt;/span&gt;
&lt;span class="mi"&gt;210000&lt;/span&gt; &lt;span class="n"&gt;puku&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fjrnyu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;88.028362&lt;/span&gt;
&lt;span class="mi"&gt;220000&lt;/span&gt; &lt;span class="n"&gt;puku&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;firyau&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;88.028362&lt;/span&gt;
&lt;span class="mi"&gt;230000&lt;/span&gt; &lt;span class="n"&gt;pudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fkrcvu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;87.561022&lt;/span&gt;
&lt;span class="mi"&gt;240000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;ftrwzu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;span class="mi"&gt;250000&lt;/span&gt; &lt;span class="n"&gt;kudu&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;feru&lt;/span&gt; &lt;span class="n"&gt;fprxzu&lt;/span&gt; &lt;span class="n"&gt;hush&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;86.585205&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, let&amp;#8217;s try the alternative proposal rule, which only chooses letters
from gibberish words when it modifies the current cipher to propose a
new one. The algorithm doesn&amp;#8217;t find the actual message, but it actually
finds a more likely message (according the the lexical database) within
20,000 iterations.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;results1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metropolis_decipher&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ciphered_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;propose_modified_cipher_from_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;niter&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;results1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="n"&gt;deciphered_text&lt;/span&gt;       &lt;span class="n"&gt;logp&lt;/span&gt;
 &lt;span class="mi"&gt;10000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;mi&lt;/span&gt; &lt;span class="n"&gt;isle&lt;/span&gt; &lt;span class="n"&gt;izlkde&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;68.946850&lt;/span&gt;
 &lt;span class="mi"&gt;20000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
 &lt;span class="mi"&gt;30000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
 &lt;span class="mi"&gt;40000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
 &lt;span class="mi"&gt;50000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
 &lt;span class="mi"&gt;60000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
 &lt;span class="mi"&gt;70000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;us&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;38.176725&lt;/span&gt;
 &lt;span class="mi"&gt;80000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
 &lt;span class="mi"&gt;90000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;100000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;110000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;120000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;130000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;140000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;150000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;us&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;38.176725&lt;/span&gt;
&lt;span class="mi"&gt;160000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;170000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;37.012894&lt;/span&gt;
&lt;span class="mi"&gt;180000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;190000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;200000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;210000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;220000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;230000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;240000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;35.784429&lt;/span&gt;
&lt;span class="mi"&gt;250000&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;some&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;37.012894&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The graph below plots the likelihood paths of the algorithm for the two
proposal rules. The blue line is the log-likelihood of the original
message we&amp;#8217;re trying to recover.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/metropolis_likpaths.png"&gt;
  &lt;img src="../images/metropolis_likpaths.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Direct calculation of the most likely message&lt;/h2&gt;
&lt;p&gt;The Metropolis algorithm is kind of pointless for this application. It&amp;#8217;s
really just jumping around looking for the most likely phrase. But since
the likelihood of a message is just the sum of the log probabilities of
the log probabilities of its component words, we just need to look for
the most likely words of the lengths of the words of the ciphered
message.&lt;/p&gt;
&lt;p&gt;If the message at some point is &amp;#8220;fgk tp hpdt&amp;#8221;, then, if run long enough,
the algorithm should just find the most likely three-letter word, the
most likely two-letter word, and the most likely four-letter word. But
we can look these up directly.&lt;/p&gt;
&lt;p&gt;For example, the message we encrypted is &amp;#8216;here is some sample text&amp;#8217;,
which has word lengths 4, 2, 4, 6, 4. What&amp;#8217;s the most likely message
with these word lengths?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maxprob_message&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_lens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;lexical_database&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;db_word_series&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lexical_db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;db_word_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;db_word_series&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;max_prob_wordlist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;logp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;word_lens&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;db_words_i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db_word_series&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;db_word_len&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;db_max_prob_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lexical_db&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;db_words_i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;idxmax&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;logp&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lexical_db&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;db_words_i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="n"&gt;max_prob_wordlist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;db_max_prob_word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_prob_wordlist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logp&lt;/span&gt;

&lt;span class="n"&gt;maxprob_message&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;with&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;with&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;united&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;with&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;25.642396806584493&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, technically, we should have decoded our message to be &amp;#8220;with of
united with&amp;#8221; instead of &amp;#8220;here is some sample text&amp;#8221;. This is not a
shining endorsement of this methodology for decrypting messages.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While it was a fun exercise to code up the Metropolis decrypter in this
chapter, it didn&amp;#8217;t show off any new Python functionality. The ridge
problem, while less interesting, showed off some of the optimization
algorithms in Scipy. There&amp;#8217;s a lot of good stuff in Scipy&amp;#8217;s &lt;code&gt;optimize&lt;/code&gt;
module, and its &lt;a href="http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html"&gt;documentation&lt;/a&gt; is worth checking out.&lt;/p&gt;</summary></entry><entry><title>Machine Learning for Hackers Chapter 6: Regression models with regularization</title><link href="http://slendermeans.org/ml4h-ch6.html" rel="alternate"></link><updated>2013-02-08T20:07:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2013-02-08:ml4h-ch6.html</id><summary type="html">&lt;p&gt;In my opinion, Chapter 6 is the most important chapter in &lt;em&gt;Machine
Learning for Hackers&lt;/em&gt;. It introduces the fundamental problem of machine
learning: overfitting and the bias-variance tradeoff. And it
demonstrates the two key tools for dealing with it: regularization and
cross-validation.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s also a fun chapter to write in Python, because it lets me play with
the fantastic &lt;a href="http://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt; library. scikit-learn is loaded with
hi-tech machine learning models, along with convenient &amp;#8220;pipeline&amp;#8221;-type
functions that facilitate the process of cross-validating and selecting
hyperparameters for models. Best of all, it&amp;#8217;s &lt;a href="http://scikit-learn.org/stable/"&gt;very well
documented&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Fitting a sine wave with polynomial regression&lt;/h2&gt;
&lt;p&gt;The chapter starts out with a useful toy example&amp;#8212;trying to fit a curve
to data generated by a sine function over the interval [0, 1] with added
Gaussian noise. The natural way to fit nonlinear data like this is using
a polynomial function, so that the output, &lt;em&gt;y&lt;/em&gt; is a function of powers
of the input &lt;em&gt;x&lt;/em&gt;. But there are two problems with this.&lt;/p&gt;
&lt;p&gt;First, we can generate highly correlated regressors by taking powers of
&lt;em&gt;x&lt;/em&gt;, leading to noisy parameter estimates. The input &lt;em&gt;x&lt;/em&gt; are evenly
space numbers on the interval [0, 1]. So &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;x&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; are going to
have a correlation over 95%. Similar with &lt;em&gt;x&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; and &lt;em&gt;x&lt;sup&gt;3&lt;/sup&gt;&lt;/em&gt;. The
solution to this is to use &lt;em&gt;orthogonalized&lt;/em&gt; polynomial functions:
tranformations of x that, when summed, result in polynomial functions,
but are orthogonal (therefore uncorrelated) with each other.&lt;/p&gt;
&lt;p&gt;Luckily, we can easily calculate these transformations using patsy. The
&lt;code&gt;C(x, Poly)&lt;/code&gt; transform computes orthonormal polynomial functions of &lt;em&gt;x&lt;/em&gt;,
then we&amp;#8217;ll extract out various orders of the polynomial. So
&lt;code&gt;Xpoly[:, :2]&lt;/code&gt; selects out the 0th and 1st order functions, then when
summed will give us a first order polynomial (i.e. linear). Similarly
&lt;code&gt;Xpoly[: :4]&lt;/code&gt; gives us the 0th through 3rd order functions, which sum up
to a cubic polynomial.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sin_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;101&lt;/span&gt;&lt;span class="p"&gt;)})&lt;/span&gt;
&lt;span class="n"&gt;sin_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;pi&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;sin_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;101&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sin_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Xpoly&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dmatrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C(x, Poly)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Xpoly1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xpoly&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Xpoly3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xpoly&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Xpoly5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xpoly&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Xpoly25&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xpoly&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The problem we encounter now is how to choose what order polynomial to
fit to the data. Any data can be fit well (i.e. have a high R^2^) if we
use a high enough order polynomial. But we will start to over-fit our
data; capturing noise specific to our sample, leading to poor
predictions on new data. The graph below shows the fits to the data of a
straight line, a 3rd-order polynomial, a 5th-order polynomial, and a
25th-order polynomial. Notice how the last fit gives us all kinds of
degrees of freedom to capture specific datapoints, and the excessive
&amp;#8220;wiggles&amp;#8221; look like we&amp;#8217;re fitting to noise.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/sine_wave_polyfits.png"&gt;
  &lt;img src="../images/sine_wave_polyfits.png" width = 450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In machine learning, this problem is solved with
&lt;em&gt;regularization&lt;/em&gt;&amp;#8212;penalizing large parameter estimates in a way that,
hopefully, shrinks down the coefficients on all but the most important
inputs. Here&amp;#8217;s where scikit-learn shines.&lt;/p&gt;
&lt;h2&gt;Preventing overfitting with regularization&lt;/h2&gt;
&lt;p&gt;The penalty parameter in a regularized regression is typically found via
cross-validation; for each candidate penalty one repeatedly fits the
model on subsets on the data, and the penalty value that gives the best
fit across the cross-validation &amp;#8220;folds&amp;#8221; is chosen. In the book, the
authors hand-code up a cross-validation scheme, looping over possible
penalties and subsets of the data and recording the MSEs.&lt;/p&gt;
&lt;p&gt;In scikit-learn you can usually automate the cross-validation procedure,
by one of a couple of ways. Many models have a &lt;code&gt;CV&lt;/code&gt;version, or, if not,
you can wrap your model in a function like &lt;a href="http://scikit-learn.org/stable/modules/grid_search.html"&gt;&lt;code&gt;GridSearchCV&lt;/code&gt;&lt;/a&gt; which is a
convenience function around all the looping and fit-recording entailed
in a cross-validation. Here I&amp;#8217;ll use the &lt;a href="http://scikit-learn.org/stable/modules/linear_model.html"&gt;&lt;code&gt;LassoCV&lt;/code&gt;&lt;/a&gt; function, which
performs cross-validation for a &lt;span class="caps"&gt;LASSO&lt;/span&gt;-penalized linear regression.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;lasso_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LassoCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy_X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;normalize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lasso_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lasso_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xpoly&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lasso_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lasso_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xpoly&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first line sets up the model by specifying some options. The only
interesting one here is &lt;code&gt;cv&lt;/code&gt;, which specifies how many cross-validation
folds to run on each penalty-parameter value. The second line fits the
model: here&amp;#8217;s I&amp;#8217;m going to run a 10th-order polynomial regression, and
let the &lt;span class="caps"&gt;LASSO&lt;/span&gt; penalty shrink away all but the most important orders.
Finally, &lt;code&gt;lasso_path&lt;/code&gt; provides the objective function that our penalty
parameter is suppose to optimize in the cross-validations (typically
&lt;span class="caps"&gt;RMSE&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;After running the &lt;code&gt;fit()&lt;/code&gt; method, &lt;code&gt;LassoCV&lt;/code&gt; will provide useful output
attributes, including the &amp;#8220;optimal&amp;#8221; penalty parameter, stored in
&lt;code&gt;.alpha_&lt;/code&gt;. Note that scikit-learn refers to the penalty parameter as
&lt;code&gt;alpha&lt;/code&gt;, while R&amp;#8217;s &lt;code&gt;glmnet&lt;/code&gt;, which the authors use to implement the
&lt;span class="caps"&gt;LASSO&lt;/span&gt; model, calls it &lt;code&gt;lambda&lt;/code&gt;. I&amp;#8217;m more accustomed to the penalty
parameter being denoted with lambda myself. Note also that &lt;code&gt;glmnet&lt;/code&gt; uses
&lt;code&gt;alpha&lt;/code&gt; elsewhere.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Plot the average &lt;span class="caps"&gt;MSE&lt;/span&gt; across folds&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lasso_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alphas_&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lasso_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mse_path_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;span class="caps"&gt;RMSE&lt;/span&gt; (avg. across folds)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;r&amp;#39;\$-&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;log(&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;lambda)\$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# Indicate the lasso parameter that minimizes the average &lt;span class="caps"&gt;MSE&lt;/span&gt; across&lt;/span&gt;
&lt;span class="n"&gt;folds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axvline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lasso_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha_&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/lasso_cv_fits_poly.png"&gt;
  &lt;img src="../images/lasso_cv_fits_poly.png" width = 450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The value of the penalty parameter itself isn&amp;#8217;t all that meaningful. So
let&amp;#8217;s take a look at what the resulting coefficient estimates are when
we apply the penalty.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Deg. Coefficient&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;r_&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lasso_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;intercept_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lasso_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;coef_&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;Deg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Coefficient&lt;/span&gt;
  &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.003584&lt;/span&gt;
  &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;5.359452&lt;/span&gt;
  &lt;span class="mi"&gt;2&lt;/span&gt;     &lt;span class="mf"&gt;0.000000&lt;/span&gt;
  &lt;span class="mi"&gt;3&lt;/span&gt;     &lt;span class="mf"&gt;4.689958&lt;/span&gt;
  &lt;span class="mi"&gt;4&lt;/span&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.000000&lt;/span&gt;
  &lt;span class="mi"&gt;5&lt;/span&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.547131&lt;/span&gt;
  &lt;span class="mi"&gt;6&lt;/span&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.047675&lt;/span&gt;
  &lt;span class="mi"&gt;7&lt;/span&gt;     &lt;span class="mf"&gt;0.124998&lt;/span&gt;
  &lt;span class="mi"&gt;8&lt;/span&gt;     &lt;span class="mf"&gt;0.133224&lt;/span&gt;
  &lt;span class="mi"&gt;9&lt;/span&gt;    &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.171974&lt;/span&gt;
 &lt;span class="mi"&gt;10&lt;/span&gt;     &lt;span class="mf"&gt;0.090685&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So the &lt;span class="caps"&gt;LASSO&lt;/span&gt;, after selecting a penalty parameter via cross-validation,
results in essentially a 3rd-order polynomial model: &lt;em&gt;y = -5.4x +
4.7x^3^&lt;/em&gt;. This makes sense since, as we saw above, we&amp;#8217;d captured the
important features of the data by the time we&amp;#8217;d fit a 3rd order
polynomial.&lt;/p&gt;
&lt;h2&gt;Predicting O&amp;#8217;Reilly book sales using back-cover descriptions&lt;/h2&gt;
&lt;p&gt;Next I&amp;#8217;ll use the same model to tackle some real data. We have the sales
ranks of the top-100 selling O&amp;#8217;Reilly books. We&amp;#8217;d like to see if we use
the text on the back-cover description of the book to predict its rank.
So the output variable is the rank of the book (reversed so that 100 is
the top-selling book, and 1 is the 100th best-selling book), while the
input variables are all the terms that appear in these 100 books&amp;#8217; back
covers. For each book the value of an input variable is the number of
times the term appears on its back cover. Many of the input values will
be zero (for example, the term &amp;#8220;javascript&amp;#8221; will occur many times in a
book about javascript, but zero times in every other book).&lt;/p&gt;
&lt;p&gt;So the matrix of input variables is just our old friend, the
term-document matrix. Creating this (using any of the methods described
in the posts for [chapter 3][] or [chapter 4][]), we can just apply
&lt;code&gt;LassoCV&lt;/code&gt; again.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;lasso_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LassoCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lasso_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lasso_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc_tdm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ranks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because of the size and nature of the input data, this runs pretty
slowly (about 3-5 minutes for me). And, because there seems to be no
good prediction model to be had here, the model doesn&amp;#8217;t alway converge.
If we do get a convergent run, we find the &lt;span class="caps"&gt;CV&lt;/span&gt; procedure wants us to
shrink all the coefficients to zero: no input is worth keeping per the
&lt;span class="caps"&gt;LASSO&lt;/span&gt;. (Note that since the x-axis in the graph is -log(penalty), moving
left on the axis, towards 0, means more regularization.) This is the
same result the authors find.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/lasso_cv_fits_text.png"&gt;
  &lt;img src="../images/lasso_cv_fits_text.png" width = 450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Logistic regression with cross-validation&lt;/h1&gt;
&lt;p&gt;With the previous model a bust, the authors regroup and try to fit a
more simple output variable: a binary indicator of whether the book is
in the top-50 sellers or not. Since they&amp;#8217;re modeling a 0/1 outcome, they
use a logistic regression. Like the linear models we used above, we can
also apply regularizers to logistic regression.&lt;/p&gt;
&lt;p&gt;In the book, the authors again code up an explicit cross-validation
procedure. The &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch6/ch6.ipynb"&gt;notebook&lt;/a&gt; for this chapter has some code that
replicates their procedure, but here I&amp;#8217;ll discuss a version that uses
scikit-learn&amp;#8217;s &lt;code&gt;GridCV&lt;/code&gt; function, which automates the cross-validation
procedure for us. (the term &amp;#8220;grid&amp;#8221; is a little confusing here, since
we&amp;#8217;re only optimizing over one variable, the penalty parameter; the term
&amp;#8220;grid&amp;#8221; is a little more intuitive in a 2-or-more-dimension search).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;penalty&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;l1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;c_grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;score_func&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_one_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_cv_folds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainX&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We initialize the &lt;code&gt;GridCV&lt;/code&gt; procedure by telling it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What model we&amp;#8217;re using: logistic, with a penalty parameter &lt;code&gt;C&lt;/code&gt;, initialized at 1.0, using the L1 (&lt;span class="caps"&gt;LASSO&lt;/span&gt;) penalty.&lt;/li&gt;
&lt;li&gt;A grid/array of parameter value candidates to search over: here values of &lt;code&gt;C&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A score function to optimize: before we were using the &lt;span class="caps"&gt;RMSE&lt;/span&gt; of the regression, here we&amp;#8217;ll use a correct classification rate, given by &lt;code&gt;zero_one_score&lt;/code&gt;, in scikit-learn&amp;#8217;s &lt;code&gt;metrics&lt;/code&gt; module.&lt;/li&gt;
&lt;li&gt;The number of cross-validation folds to performs; this defined elsewhere in the variable &lt;code&gt;n_cv_folds&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then I fit the model on training data (a random subset of 80). After
running this, We can check what value it chose for the penalty
parameter, &lt;code&gt;C&lt;/code&gt;, and what the in-sample error-rate for this value was.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_score_&lt;/span&gt;
&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.29377144516536563&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="mf"&gt;0.375&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And again, let&amp;#8217;s plot the error rates against values of &lt;code&gt;C&lt;/code&gt; to vizualize
how regularization affects the model accuracy.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;rates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid_scores_&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;stds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_cv_folds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt;
&lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid_scores_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill_between&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rates&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;stds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rates&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;stds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;o-k&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Avg. error rate across folds&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;C (regularization parameter)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Avg. error rate (and +/- 1 s.e.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;best&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/logistic_cv_errors.png"&gt;
  &lt;img src="../images/logistic_cv_errors.png" width = 450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After fitting to the training set, we can predict on the test set and
and see how accurate the model is on new data using the
&lt;code&gt;classification_report&lt;/code&gt; function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classification_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testX&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

             &lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="n"&gt;f1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt; &lt;span class="n"&gt;support&lt;/span&gt;
          &lt;span class="mi"&gt;0&lt;/span&gt;       &lt;span class="mf"&gt;0.78&lt;/span&gt;   &lt;span class="mf"&gt;0.44&lt;/span&gt;     &lt;span class="mf"&gt;0.56&lt;/span&gt;      &lt;span class="mi"&gt;16&lt;/span&gt;
          &lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="mf"&gt;0.18&lt;/span&gt;   &lt;span class="mf"&gt;0.50&lt;/span&gt;     &lt;span class="mf"&gt;0.27&lt;/span&gt;       &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="n"&gt;avg&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total&lt;/span&gt;       &lt;span class="mf"&gt;0.66&lt;/span&gt;   &lt;span class="mf"&gt;0.45&lt;/span&gt;     &lt;span class="mf"&gt;0.50&lt;/span&gt;      &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And the confusion matrix shows we got 9 instances classified correctly
(the diagonal), and 11 incorrectly (the off-diagonal).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39; Predicted&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39; Class&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testX&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="n"&gt;Predicted&lt;/span&gt;
  &lt;span class="n"&gt;Class&lt;/span&gt;
  &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Cross-validation often requires a lot of bookkeeping code. Writing this
over and over again for different applications is inefficient and
error-prone. So it&amp;#8217;s great that scikit-learn has functions that
encapsulate the cross-validation process in convenient
abstractions/interfaces that do the bookkeeping for you. It also has a
wide array of useful, cutting-edge models, and the
&lt;a href="http://scikit-learn.org/stable/"&gt;documentation&lt;/a&gt; is not just clear and organized, but also
educational: there are lots of examples and exposition that explains how
the underlying models work, not just what the &lt;span class="caps"&gt;API&lt;/span&gt; is.&lt;/p&gt;
&lt;p&gt;So even though we didn&amp;#8217;t build any kick-ass, high-accuracy predictive
models here, we did get to explore some fundamental methods in building
&lt;span class="caps"&gt;ML&lt;/span&gt; models, and get acquainted with the powerful tools in scikit-learn.&lt;/p&gt;</summary></entry><entry><title>Machine Learning for Hackers Chapter 5: Linear regression (with categorical regressors)</title><link href="http://slendermeans.org/ml4h-ch5.html" rel="alternate"></link><updated>2012-12-28T01:32:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-12-28:ml4h-ch5.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Chapter 5 of &lt;em&gt;Machine Learning for Hackers&lt;/em&gt; is a relatively simple
exercise in running linear regressions. Therefore, this post will be
short, and I&amp;#8217;ll only discuss the more interesting regression example,
which nicely shows how patsy formulas handle categorical variables.&lt;/p&gt;
&lt;h2&gt;Linear regression with categorical independent variables&lt;/h2&gt;
&lt;p&gt;In chapter 5, the authors construct several linear regressions, the last
of which is a multi-variate regression descriping the number of page
views of top-viewed web sites. The regression is pretty straightforward,
but includes two categorical variables: &lt;code&gt;HasAdvertising&lt;/code&gt;, which takes
values &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt;; and &lt;code&gt;InEnglish&lt;/code&gt;, which takes values &lt;code&gt;Yes&lt;/code&gt;,
&lt;code&gt;No&lt;/code&gt; and &lt;code&gt;NA&lt;/code&gt; (missing).&lt;/p&gt;
&lt;p&gt;If we include these variables in the formula, then patsy/statmodels will
automatically generate the necessary dummy variables. For
&lt;code&gt;HasAdvertising&lt;/code&gt;, we get a dummy variable equal to one when the the
value is &lt;code&gt;True&lt;/code&gt;. For &lt;code&gt;InEnglish&lt;/code&gt;, which takes three values, we get two
separate dummy variables, one for &lt;code&gt;Yes&lt;/code&gt;, one for &lt;code&gt;No&lt;/code&gt;, with the missing
value serving as the baseline.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;np.log(PageViews) ~ np.log(UniqueVisitors) + HasAdvertising +&lt;/span&gt;
&lt;span class="n"&gt;InEnglish&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;pageview_fit_multi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;pageview_fit_multi&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Results in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;OLS&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Regression&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PageViews&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.480&lt;/span&gt;
&lt;span class="nl"&gt;Model:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;OLS&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Adj&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squared&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.478&lt;/span&gt;
&lt;span class="nl"&gt;Method:&lt;/span&gt; &lt;span class="n"&gt;Least&lt;/span&gt; &lt;span class="n"&gt;Squares&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;229.4&lt;/span&gt;
&lt;span class="nl"&gt;Date:&lt;/span&gt; &lt;span class="n"&gt;Sat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt; &lt;span class="n"&gt;Nov&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Prob&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;statistic&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.52e-139&lt;/span&gt;
&lt;span class="nl"&gt;Time:&lt;/span&gt; &lt;span class="mi"&gt;09&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1481.1&lt;/span&gt;
&lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AIC&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2972.&lt;/span&gt;
&lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;995&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;BIC&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2997.&lt;/span&gt;
&lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="o"&gt;==========================================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;.]&lt;/span&gt;

&lt;span class="o"&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.9450&lt;/span&gt; &lt;span class="mf"&gt;1.148&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.695&lt;/span&gt; &lt;span class="mf"&gt;0.090&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;4.197&lt;/span&gt; &lt;span class="mf"&gt;0.307&lt;/span&gt;
&lt;span class="n"&gt;HasAdvertising&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="mf"&gt;0.3060&lt;/span&gt; &lt;span class="mf"&gt;0.092&lt;/span&gt; &lt;span class="mf"&gt;3.336&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt; &lt;span class="mf"&gt;0.126&lt;/span&gt; &lt;span class="mf"&gt;0.486&lt;/span&gt;
&lt;span class="n"&gt;InEnglish&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="mf"&gt;0.8347&lt;/span&gt; &lt;span class="mf"&gt;0.209&lt;/span&gt; &lt;span class="mf"&gt;4.001&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.425&lt;/span&gt; &lt;span class="mf"&gt;1.244&lt;/span&gt;
&lt;span class="n"&gt;InEnglish&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Yes&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1691&lt;/span&gt; &lt;span class="mf"&gt;0.204&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.828&lt;/span&gt; &lt;span class="mf"&gt;0.408&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.570&lt;/span&gt; &lt;span class="mf"&gt;0.232&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;UniqueVisitors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mf"&gt;1.2651&lt;/span&gt; &lt;span class="mf"&gt;0.071&lt;/span&gt; &lt;span class="mf"&gt;17.936&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;1.127&lt;/span&gt; &lt;span class="mf"&gt;1.403&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="nl"&gt;Omnibus:&lt;/span&gt; &lt;span class="mf"&gt;73.424&lt;/span&gt; &lt;span class="n"&gt;Durbin&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Watson&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2.068&lt;/span&gt;
&lt;span class="n"&gt;Prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Omnibus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="n"&gt;Jarque&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Bera&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;JB&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;92.632&lt;/span&gt;
&lt;span class="nl"&gt;Skew:&lt;/span&gt; &lt;span class="mf"&gt;0.646&lt;/span&gt; &lt;span class="n"&gt;Prob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;JB&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;7.68e-21&lt;/span&gt;
&lt;span class="nl"&gt;Kurtosis:&lt;/span&gt; &lt;span class="mf"&gt;3.744&lt;/span&gt; &lt;span class="n"&gt;Cond&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="mf"&gt;570.&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we were going to do this without the formula &lt;span class="caps"&gt;API&lt;/span&gt;, we&amp;#8217;d have to
explicity make these dummies. For comparison, here&amp;#8217;s that.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;LogUniqueVisitors&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;UniqueVisitors&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;HasAdvertisingYes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;HasAdvertising&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Yes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;InEnglishYes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;InEnglish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Yes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;InEnglishNo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;InEnglish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;No&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;linreg_fit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;OLS&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;PageViews&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
&lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_1k_sites&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;HasAdvertisingYes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s"&gt;&amp;#39;LogUniqueVisitors&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s"&gt;&amp;#39;InEnglishNo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;InEnglishYes&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
&lt;span class="n"&gt;prepend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;linreg_fit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 4: Priority e-mail ranking</title><link href="http://slendermeans.org/ml4h-ch4.html" rel="alternate"></link><updated>2012-12-28T00:00:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-12-28:ml4h-ch4.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m not going to write much about this chapter. In my opinion the payoff-to-effort ratio for this project is pretty low. The algorithm for ranking e-mails is pretty straightforward, but in my opinion seriously flawed. Most of the code in the chapter (and there&amp;#8217;s a lot of it) revolves around parsing the text in the files. It&amp;#8217;s a good exercise in thinking through feature extraction, but it&amp;#8217;s not got a lot of new &lt;span class="caps"&gt;ML&lt;/span&gt; concepts. And from my perspective, there&amp;#8217;s not much opportunity to show off any Python goodness. But, I&amp;#8217;ll hit a couple of points that are new and interesting.&lt;/p&gt;
&lt;p&gt;The complete code is at the Github repo &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/ch4"&gt;here&lt;/a&gt;, and you can read the notebook via nbviewer &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch4/ch4.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Vectorized string methods in pandas.&lt;/strong&gt; Back in &lt;a href="../ml4h-ch1-p2.html"&gt;Chapter 1&lt;/a&gt;, I groused about lacking vectorized functions for operations on strings or dates in pandas. If it wasn&amp;#8217;t a numpy ufunc, you had to use the pandas &lt;code&gt;map()&lt;/code&gt; method. That&amp;#8217;s changed a lot over the summer, and since pandas 0.9.0, we can call &lt;a href="http://pandas.pydata.org/pandas-docs/stable/basics.html#vectorized-string-methods"&gt;vectorized string methods&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, here&amp;#8217;s the code in my chapter for program that identifies e-mails that are part of a thread, by looking for &amp;#8220;re:&amp;#8221;-like prefixes on the subjects.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;reply_pattern&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;(re:|re\[\d\]:)&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;fwd_pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;(fw:|fw[\d]:)&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;thread_flag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Returns True if string s matches the thread patterns.&lt;/span&gt;
&lt;span class="sd"&gt;    If s is a pandas Series, returns a Series of booleans.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;basestring&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reply_pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reply_pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;clean_subject&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Removes all the reply and forward labeling from a&lt;/span&gt;
&lt;span class="sd"&gt;    string (an e-mail subject) s.&lt;/span&gt;
&lt;span class="sd"&gt;    If s is a pandas Series, returns a Series of cleaned&lt;/span&gt;
&lt;span class="sd"&gt;    strings.&lt;/span&gt;
&lt;span class="sd"&gt;    This will help find the initial message in the thread&lt;/span&gt;
&lt;span class="sd"&gt;    (which won&amp;#39;t have any of the reply/forward labeling.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;basestring&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;s_clean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reply_pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;s_clean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fwd_pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s_clean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;s_clean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s_clean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s_clean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reply_pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;s_clean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s_clean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fwd_pattern&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;s_clean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s_clean&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;s_clean&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In &lt;code&gt;thread_flag&lt;/code&gt;, if the input is a pandas series of e-mail subject lines, then the function will use a vectorized string function, called with &lt;code&gt;.str.contains()&lt;/code&gt; to see if a pattern matching a reply-type prefix is in the subject. The function will therefore return a pandas series of booleans, that are &lt;code&gt;True&lt;/code&gt; for all the subjects that have a reply pattern, and &lt;code&gt;False&lt;/code&gt; for all the subjects that don&amp;#8217;t.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;clean_subjects&lt;/code&gt;, if given a pandas Series input, will use the vectorized string methods &lt;code&gt;.str.replace()&lt;/code&gt; and &lt;code&gt;.str.strip()&lt;/code&gt; to clean the re- and fwd-like patterns out of the subjects.&lt;/p&gt;
&lt;p&gt;Notice there are some differences between the naming of pandas string methods and the base string methods or &lt;code&gt;re&lt;/code&gt; module functions that perform similar operations on single strings. For example, there&amp;#8217;s no &lt;code&gt;contains&lt;/code&gt; function in &lt;code&gt;re&lt;/code&gt;; we use &lt;code&gt;re.search()&lt;/code&gt;. Similarly &lt;code&gt;.str.replace()&lt;/code&gt; does what we&amp;#8217;d use &lt;code&gt;re.sub()&lt;/code&gt; to do on a single string.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. More term-document matrices&lt;/strong&gt; In &lt;a href="../ml4h-ch3.html"&gt;Chapter 3&lt;/a&gt; we built a term-document matrix to extract term-frequency features from a set of e-mails. This chapter has a similar exercise, applied to both e-mail messages and their subjects. In the code for that chapter, I built a &lt;span class="caps"&gt;TDM&lt;/span&gt; function that wrapped the term-document matrix function in the &lt;code&gt;textmining&lt;/code&gt; package, adding some options that tried to mimic the &lt;code&gt;tdm&lt;/code&gt; function in R&amp;#8217;s &lt;code&gt;tm&lt;/code&gt; package. I use that same function, &lt;code&gt;tdm_df&lt;/code&gt;, here. In the post for that chapter, I lamented that I couldn&amp;#8217;t find a decent term-document matrix function for Python. The one in &lt;code&gt;textmining&lt;/code&gt; was too barebones and I was surprised there was nothing that fit the bill in &lt;span class="caps"&gt;NLTK&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In comments to that post, Vishal Goklani pointed me to the &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer"&gt;&lt;code&gt;CountVectorizer&lt;/code&gt;&lt;/a&gt; function in scikits-learn (in the &lt;code&gt;sklearn.feature_extraction.text&lt;/code&gt; module). Despite the rather generic name, this will give you a &lt;span class="caps"&gt;TDM&lt;/span&gt; from a set of documents, returned in the form of a sparse matrix. Here&amp;#8217;s quick-and-dirty wrapper function that returns a &lt;span class="caps"&gt;TDM&lt;/span&gt; in the form of a pandas DataFrame.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sklearn_tdm_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Create a term-document matrix (&lt;span class="caps"&gt;TDM&lt;/span&gt;) in the form of a pandas DataFrame&lt;/span&gt;
&lt;span class="sd"&gt;    Uses sklearn&amp;#39;s CountVectorizer function.&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters&lt;/span&gt;
&lt;span class="sd"&gt;    ----------&lt;/span&gt;
&lt;span class="sd"&gt;    docs: a sequence of documents (files, filenames, or the content) to be&lt;/span&gt;
&lt;span class="sd"&gt;        included in the &lt;span class="caps"&gt;TDM&lt;/span&gt;. See the `input` argument to CountVectorizer.&lt;/span&gt;
&lt;span class="sd"&gt;    **kwargs: keyword arguments for CountVectorizer options.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns&lt;/span&gt;
&lt;span class="sd"&gt;    -------&lt;/span&gt;
&lt;span class="sd"&gt;    tdm_df: A pandas DataFrame with the term-document matrix. Columns are terms,&lt;/span&gt;
&lt;span class="sd"&gt;        rows are documents.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="c"&gt;# Initialize the vectorizer and get term counts in each document.&lt;/span&gt;
    &lt;span class="n"&gt;vectorizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CountVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;word_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;docs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# .vocabulary_ is a Dict whose keys are the terms in the documents,&lt;/span&gt;
    &lt;span class="c"&gt;# and whose entries are the columns in the matrix returned by fit_transform()&lt;/span&gt;
    &lt;span class="n"&gt;vocab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vectorizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocabulary_&lt;/span&gt;

    &lt;span class="c"&gt;# Make a dictionary of Series for each term; convert to DataFrame&lt;/span&gt;
    &lt;span class="n"&gt;count_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getcol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;tdm_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;count_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tdm_df&lt;/span&gt;

&lt;span class="c"&gt;# Call the function on e-mail messages. The token_pattern is set so that terms are only&lt;/span&gt;
&lt;span class="c"&gt;# words with two or more letters (no numbers or punctuation)&lt;/span&gt;
&lt;span class="n"&gt;message_tdm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sklearn_tdm_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;message&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                             &lt;span class="n"&gt;stop_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;english&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;charset_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                             &lt;span class="n"&gt;token_pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;[a-zA-Z]{2,}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;3. Timezone issues and rank instability.&lt;/strong&gt; In the book, the authors compute stats measuring how active threads are. This depends on the time-stamps of the messages, which the authors parse out of the e-mail files. They ignore the time-zone information in the time-stamps, and this seems to create some bugs. For example, the following thread has two e-mails:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nl"&gt;Name:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sadev&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bug&lt;/span&gt; &lt;span class="mi"&gt;840&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;spam_level_char&lt;/span&gt; &lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;removal&lt;/span&gt;
    &lt;span class="mi"&gt;734&lt;/span&gt;    &lt;span class="mi"&gt;2002&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;09&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;06&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;56&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;07&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;
    &lt;span class="mi"&gt;763&lt;/span&gt;    &lt;span class="mi"&gt;2002&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;09&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;06&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;56&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;04&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you ignore the timezones, it looks like 763 comes three hours after 734. But looking at the timezones, you can see that 734 actually comes &lt;em&gt;four seconds after&lt;/em&gt; 763. So this is a far more active thread than the code in the book calculates.&lt;/p&gt;
&lt;p&gt;This sort of issue has a pretty big effect on the ranks of the messages. The rank is just the product of 5 feature weights (based on sender info., thread activity, and term features). Even though the authors scale the individual feature weights (typically with log-scales), by calculating the final rank as a product, you can get big rank difference based on what might seem to be practically similar features (even without any bugs)&amp;#8212;for example, in some cases it doesn&amp;#8217;t take a big difference to double a feature&amp;#8217;s weight, which then doubles the e-mail&amp;#8217;s rank.So it seems to me the ranking procedure in the book is not very stable. This is fine, since it&amp;#8217;s just meant to be illustrative, but of course you want to be aware of this issue for a more serious exercise.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I didn&amp;#8217;t go into much detail here. If you&amp;#8217;re interested in seeing a lot of Python and pandas text parsing in action, definitely check out the &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch4/ch4.ipynb"&gt;code&lt;/a&gt;.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>ARM Chapter 5: Logistic models of well-switching in Bangladesh</title><link href="http://slendermeans.org/arm-ch5.html" rel="alternate"></link><updated>2012-12-22T19:10:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-12-22:arm-ch5.html</id><summary type="html">&lt;p&gt;The logistic regression we ran for &lt;a href="../ml4h-ch2-p2.html"&gt;chapter 2 of &lt;em&gt;Machine Learning for
Hackers&lt;/em&gt;&lt;/a&gt; was pretty simple. So I wanted to find an example that would
dig a little deeper into statsmodels&amp;#8217;s capabilities and the power of the
patsy formula language.&lt;/p&gt;
&lt;p&gt;So, I&amp;#8217;m taking an intermission from &lt;em&gt;Machine Learning for Hackers&lt;/em&gt; and
am going to show an example from Gelman and Hill&amp;#8217;s &lt;a href="http://www.stat.columbia.edu/~gelman/arm/"&gt;&lt;em&gt;Data Analysis Using
Regression and Multilevel/Hierarchical Models&lt;/em&gt;&lt;/a&gt; &lt;em&gt;(&amp;#8220;&lt;span class="caps"&gt;ARM&lt;/span&gt;&amp;#8221;)&lt;/em&gt;. The chapter
has a great example of going through the process of building,
interpreting, and diagnosing a logistic regression model. We&amp;#8217;ll end up
with a model with lots of interactions and variable transforms, which is
a great showcase for patsy and the statmodels formula &lt;span class="caps"&gt;API&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Logistic model of well-switching in Bangladesh&lt;/h2&gt;
&lt;p&gt;Our data are information on about 3,000 respondent households in
Bangladesh with wells having an unsafe amount of arsenic. The data
record the amount of arsenic in the respondent&amp;#8217;s well, the distance to
the nearest safe well (in meters), whether that respondent &amp;#8220;switched&amp;#8221;
wells by using a neighbor&amp;#8217;s safe well instead of their own, as well as
the respondent&amp;#8217;s years of education and a dummy variable indicating
whether they belong to a community association.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;  &lt;span class="k"&gt;switch&lt;/span&gt; &lt;span class="n"&gt;arsenic&lt;/span&gt;      &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="n"&gt;assoc&lt;/span&gt; &lt;span class="n"&gt;educ&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mf"&gt;2.36&lt;/span&gt; &lt;span class="mf"&gt;16.826000&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mf"&gt;0.71&lt;/span&gt; &lt;span class="mf"&gt;47.321999&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;      &lt;span class="mi"&gt;0&lt;/span&gt;    &lt;span class="mf"&gt;2.07&lt;/span&gt; &lt;span class="mf"&gt;20.966999&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mf"&gt;1.15&lt;/span&gt; &lt;span class="mf"&gt;21.486000&lt;/span&gt;     &lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="mi"&gt;12&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;      &lt;span class="mi"&gt;1&lt;/span&gt;    &lt;span class="mf"&gt;1.10&lt;/span&gt; &lt;span class="mf"&gt;40.874001&lt;/span&gt;     &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="mi"&gt;14&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our goal is to model well-switching decision. Since it&amp;#8217;s a binary
variable (1 = switch, 0 = no switch), we&amp;#8217;ll use logistic regression.&lt;/p&gt;
&lt;p&gt;The IPython notebook is at the Github repo &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/ARM/ch5"&gt;here&lt;/a&gt;, and you can go
&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/ARM/ch5/arsenic_wells_switching.ipynb"&gt;here&lt;/a&gt; to view it on nbviewer. The analysis follows &lt;em&gt;&lt;span class="caps"&gt;ARM&lt;/span&gt;&lt;/em&gt; chapter
5.4.&lt;/p&gt;
&lt;h2&gt;Model 1: Distance to a safe well&lt;/h2&gt;
&lt;p&gt;For our first pass, we&amp;#8217;ll just use the distance to the nearest safe
well. Since the distance is recorded in meters, and the effect of one
meter is likely to be very small, we can get nicer model coefficients if
we scale it. Instead of creating a new scaled variable, we&amp;#8217;ll just do it
in the formula description using the &lt;code&gt;I()&lt;/code&gt; function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;switch ~ I(dist/100.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;model1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Optimization terminated successfully.
    Current function value: 2038.118913
    Iterations 4
    Logit Regression Results&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="k"&gt;switch&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3020&lt;/span&gt;
&lt;span class="nl"&gt;Model:&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3018&lt;/span&gt;
&lt;span class="nl"&gt;Method:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MLE&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="nl"&gt;Date:&lt;/span&gt; &lt;span class="n"&gt;Sat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Pseudo&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squ&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.01017&lt;/span&gt;
&lt;span class="nl"&gt;Time:&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2038.1&lt;/span&gt;
&lt;span class="nl"&gt;converged:&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Null&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2059.0&lt;/span&gt;
&lt;span class="n"&gt;&lt;span class="caps"&gt;LLR&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;9.798e-11&lt;/span&gt;

&lt;span class="o"&gt;==================================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;.]&lt;/span&gt;

&lt;span class="o"&gt;----------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="mf"&gt;0.6060&lt;/span&gt; &lt;span class="mf"&gt;0.060&lt;/span&gt; &lt;span class="mf"&gt;10.047&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.488&lt;/span&gt; &lt;span class="mf"&gt;0.724&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.6219&lt;/span&gt; &lt;span class="mf"&gt;0.097&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;6.383&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.813&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.431&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s plot this model. We&amp;#8217;ll want to jitter the &lt;code&gt;switch&lt;/code&gt; data, since
it&amp;#8217;s all 0/1 and will over-plot.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/switch_dist_jittter.png"&gt;
    &lt;img src="../images/switch_dist_jittter.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another way to look at this is to plot the densities of distance for
switchers and non-switchers. We expect the distribution of switchers to
have more mass over short distances and the distribution of
non-switchers to have more mass over long distances.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/switch_dist_kde.png"&gt;
    &lt;img src="../images/switch_dist_kde.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Model 2: Distance to a safe well and the arsenic level of own well&lt;/h2&gt;
&lt;p&gt;Next, let&amp;#8217;s add the arsenic level as a regressor. We&amp;#8217;d expect
respondents with higher arsenic levels to be more motivated to switch.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;switch ~ I(dist / 100.) + arsenic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;



&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1965.334134&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Regression&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;switch&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3020&lt;/span&gt;
&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3017&lt;/span&gt;
&lt;span class="n"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MLE&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Sat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Pseudo&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.04551&lt;/span&gt;
&lt;span class="n"&gt;Time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;29&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1965.3&lt;/span&gt;
&lt;span class="n"&gt;converged&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Null&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2059.0&lt;/span&gt;
&lt;span class="n"&gt;&lt;span class="caps"&gt;LLR&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.995e-41&lt;/span&gt;

&lt;span class="o"&gt;==================================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;----------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="mf"&gt;0.0027&lt;/span&gt; &lt;span class="mf"&gt;0.079&lt;/span&gt; &lt;span class="mf"&gt;0.035&lt;/span&gt; &lt;span class="mf"&gt;0.972&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.153&lt;/span&gt; &lt;span class="mf"&gt;0.158&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.8966&lt;/span&gt; &lt;span class="mf"&gt;0.104&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.593&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.101&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.692&lt;/span&gt;
&lt;span class="n"&gt;arsenic&lt;/span&gt; &lt;span class="mf"&gt;0.4608&lt;/span&gt; &lt;span class="mf"&gt;0.041&lt;/span&gt; &lt;span class="mf"&gt;11.134&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.380&lt;/span&gt; &lt;span class="mf"&gt;0.542&lt;/span&gt;

&lt;span class="o"&gt;==================================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which is what we see. The coefficients are what we&amp;#8217;d expect: the farther
to a safe well, the less likely a respondent is to switch, but the
higher the arsenic level in their own well, the more likely.&lt;/p&gt;
&lt;h3&gt;Marginal Effects&lt;/h3&gt;
&lt;p&gt;To see the effect of these on the probability of switching, let&amp;#8217;s
calculate the marginal effects at the mean of the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;margeff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.21806505&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.11206108&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, for the mean respondent, an increase of 100 meters to the nearest
safe well is associated with a 22% lower probability of switching. But
an increase of 1 in the arsenic level is associated with an 11% higher
probability of switching.&lt;/p&gt;
&lt;h3&gt;Class separability&lt;/h3&gt;
&lt;p&gt;To get a sense of how well this model might classify switchers and
non-switchers, we can plot each class of respondent in
(distance-arsenic)-space.
We don&amp;#8217;t see very clean separation, so we&amp;#8217;d expect the model to have a
fairly high error rate. But we do notice that the
short-distance/high-arsenic region of the graph is mostly comprised
switchers, and the long-distance/low-arsenic region is mostly comprised
of non-switchers.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/dist_arsenic_sep.png"&gt;
    &lt;img src="../images/dist_arsenic_sep.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Model 3: Adding an interaction&lt;/h2&gt;
&lt;p&gt;It&amp;#8217;s sensible that distance and arsenic would interact in the model. In
other words, the effect of an 100 meters on your decision to switch
would be affected by how much arsenic is in your well.&lt;/p&gt;
&lt;p&gt;Again, we don&amp;#8217;t have to pre-compute an explicit interaction variable. We
can just specify an interaction in the formula description using the &lt;code&gt;:&lt;/code&gt;
operator.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;switch ~ I(dist / 100.) + arsenic + I(dist / 100.):arsenic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;model3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1963.814202&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Regression&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;switch&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3020&lt;/span&gt;
&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3016&lt;/span&gt;
&lt;span class="n"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MLE&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Sat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Pseudo&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.04625&lt;/span&gt;
&lt;span class="n"&gt;Time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1963.8&lt;/span&gt;
&lt;span class="n"&gt;converged&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Null&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2059.0&lt;/span&gt;
&lt;span class="n"&gt;&lt;span class="caps"&gt;LLR&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.830e-41&lt;/span&gt;

&lt;span class="o"&gt;==========================================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1479&lt;/span&gt; &lt;span class="mf"&gt;0.118&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.258&lt;/span&gt; &lt;span class="mf"&gt;0.208&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.378&lt;/span&gt; &lt;span class="mf"&gt;0.083&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5772&lt;/span&gt; &lt;span class="mf"&gt;0.209&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.759&lt;/span&gt; &lt;span class="mf"&gt;0.006&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.987&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.167&lt;/span&gt;
&lt;span class="n"&gt;arsenic&lt;/span&gt; &lt;span class="mf"&gt;0.5560&lt;/span&gt; &lt;span class="mf"&gt;0.069&lt;/span&gt; &lt;span class="mf"&gt;8.021&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.420&lt;/span&gt; &lt;span class="mf"&gt;0.692&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1789&lt;/span&gt; &lt;span class="mf"&gt;0.102&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.748&lt;/span&gt; &lt;span class="mf"&gt;0.080&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.379&lt;/span&gt; &lt;span class="mf"&gt;0.022&lt;/span&gt;

&lt;span class="o"&gt;==========================================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The coefficient on the interaction is negative and significant. While we
can&amp;#8217;t directly intepret its quantitative effect on switching, the
qualitative interpretation gels with our intuition. Distance has a
negative effect on switching, but this negative effect is reduced when
arsenic levels are high. Alternatively, the arsenic level have a
positive effect on switching, but this positive effect is reduced as
distance to the nearest safe well increases.&lt;/p&gt;
&lt;h2&gt;Model 4: Adding education, more interactions, and centering variables&lt;/h2&gt;
&lt;p&gt;Respondents with more eduction might have a better understanding of the
harmful effects of arsenic and therefore may be more likely to switch.
Education is in years, so we&amp;#8217;ll scale it for more sensible coefficients.
We&amp;#8217;ll also include interactions amongst all the regressors.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;re also going to center the variables, to help with interpretation of
the coefficients. Once more, we can just do this in the formula, without
pre-computing centered variables.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model_form&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;switch ~ center(I(dist / 100.)) + center(arsenic) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(I(educ / 4.)) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(I(dist / 100.)) : center(arsenic) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(I(dist / 100.)) : center(I(educ / 4.)) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(arsenic) : center(I(educ / 4.))&amp;#39;&lt;/span&gt;
             &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_form&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;model4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1945.871775&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Regression&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;switch&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3020&lt;/span&gt;
&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3013&lt;/span&gt;
&lt;span class="n"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MLE&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Sat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Pseudo&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.05497&lt;/span&gt;
&lt;span class="n"&gt;Time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1945.9&lt;/span&gt;
&lt;span class="n"&gt;converged&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Null&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2059.0&lt;/span&gt;
&lt;span class="n"&gt;&lt;span class="caps"&gt;LLR&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.588e-46&lt;/span&gt;

&lt;span class="o"&gt;===============================================================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;---------------------------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="mf"&gt;0.3563&lt;/span&gt; &lt;span class="mf"&gt;0.040&lt;/span&gt; &lt;span class="mf"&gt;8.844&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.277&lt;/span&gt; &lt;span class="mf"&gt;0.435&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.9029&lt;/span&gt; &lt;span class="mf"&gt;0.107&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.414&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.113&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.693&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="mf"&gt;0.4950&lt;/span&gt; &lt;span class="mf"&gt;0.043&lt;/span&gt; &lt;span class="mf"&gt;11.497&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.411&lt;/span&gt; &lt;span class="mf"&gt;0.579&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;educ&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.1850&lt;/span&gt; &lt;span class="mf"&gt;0.039&lt;/span&gt; &lt;span class="mf"&gt;4.720&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.108&lt;/span&gt; &lt;span class="mf"&gt;0.262&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1177&lt;/span&gt; &lt;span class="mf"&gt;0.104&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.137&lt;/span&gt; &lt;span class="mf"&gt;0.256&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.321&lt;/span&gt; &lt;span class="mf"&gt;0.085&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;educ&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.3227&lt;/span&gt; &lt;span class="mf"&gt;0.107&lt;/span&gt; &lt;span class="mf"&gt;3.026&lt;/span&gt; &lt;span class="mf"&gt;0.002&lt;/span&gt;
&lt;span class="mf"&gt;0.114&lt;/span&gt; &lt;span class="mf"&gt;0.532&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;educ&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.0722&lt;/span&gt; &lt;span class="mf"&gt;0.044&lt;/span&gt; &lt;span class="mf"&gt;1.647&lt;/span&gt; &lt;span class="mf"&gt;0.100&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.014&lt;/span&gt;
&lt;span class="mf"&gt;0.158&lt;/span&gt;

&lt;span class="o"&gt;===============================================================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Model assessment: binned residual plots&lt;/h3&gt;
&lt;p&gt;Plotting residuals to regressors can alert us to issues like
nonlinearity or heteroskedasticity. Plotting raw residuals in a binary
model isn&amp;#8217;t usually informative, so we do some smoothing. Here, we&amp;#8217;ll
averaging the residuals within bins of the regressor. (A lowess or
moving average might also work.)&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m going to write a function to provide the binned residual data
dynamically (and another helper function to plot the data). To create
the bins I&amp;#8217;m going to use the handy &lt;code&gt;qcut&lt;/code&gt; function in pandas, which
bins a vector of data into quantiles. Then I&amp;#8217;ll use &lt;code&gt;groupby&lt;/code&gt; to
calculate the bin means and confidence intervals.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;bin_residuals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;resid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Compute average residuals within bins of a variable.&lt;/span&gt;

&lt;span class="sd"&gt;    Returns a dataframe indexed by the bins, with the bin midpoint,&lt;/span&gt;
&lt;span class="sd"&gt;    the residual average within the bin, and the confidence interval&lt;/span&gt;
&lt;span class="sd"&gt;    bounds.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;resid_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;var&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;resid&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;resid_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bins&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;qcut&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;bin_group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resid_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bins&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;bin_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bin_group&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;var&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bin_group&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lower_ci&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_group&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_group&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;upper_ci&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_group&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="n"&gt;bin_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;var&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_binned_residuals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Plotted binned residual averages and confidence intervals.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;var&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;var&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lower_ci&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;-r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;var&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;bin_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;upper_ci&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;-r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;arsenic_resids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bin_residuals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;arsenic&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;dist_resids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bin_residuals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model4&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dist&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;121&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Residual (bin avg.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Arsenic (bin avg.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_binned_residuals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic_resids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;122&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_binned_residuals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist_resids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Residual (bin avg.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Distance (bin avg.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/arsenic_dist_bin_resid.png"&gt;
    &lt;img src="../images/arsenic_dist_bin_resid.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Model 5: log-scaling arsenic&lt;/h2&gt;
&lt;p&gt;The binned residual plot indicates some nonlinearity in the arsenic
variable. Note how the model over-estimated for low arsenic and
underestimates for high arsenic. This suggests a log transformation or
something similar.&lt;/p&gt;
&lt;p&gt;We can again do this transformation right in the formula.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;model_form&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;switch ~ center(I(dist / 100.)) +&lt;/span&gt;
               &lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;#39; +&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(I(educ / 4.)) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(I(dist / 100.)) : center(np.log(arsenic)) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(I(dist / 100.)) : center(I(educ / 4.)) + &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
              &lt;span class="s"&gt;&amp;#39;center(np.log(arsenic)) : center(I(educ / 4.))&amp;#39;&lt;/span&gt;
             &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_form&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;model5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;




&lt;span class="n"&gt;Optimization&lt;/span&gt; &lt;span class="n"&gt;terminated&lt;/span&gt; &lt;span class="n"&gt;successfully&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1931.554102&lt;/span&gt;
&lt;span class="n"&gt;Iterations&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Regression&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;switch&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3020&lt;/span&gt;
&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3013&lt;/span&gt;
&lt;span class="n"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MLE&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Sat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Pseudo&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;squ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.06192&lt;/span&gt;
&lt;span class="n"&gt;Time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;57&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1931.6&lt;/span&gt;
&lt;span class="n"&gt;converged&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Null&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2059.0&lt;/span&gt;
&lt;span class="n"&gt;&lt;span class="caps"&gt;LLR&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;3.517e-52&lt;/span&gt;

&lt;span class="o"&gt;==================================================================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;------------------------------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="mf"&gt;0.3452&lt;/span&gt; &lt;span class="mf"&gt;0.040&lt;/span&gt; &lt;span class="mf"&gt;8.528&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.266&lt;/span&gt; &lt;span class="mf"&gt;0.425&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.9796&lt;/span&gt; &lt;span class="mf"&gt;0.111&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;8.809&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.197&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.762&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.9036&lt;/span&gt; &lt;span class="mf"&gt;0.070&lt;/span&gt; &lt;span class="mf"&gt;12.999&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.767&lt;/span&gt; &lt;span class="mf"&gt;1.040&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;educ&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.1785&lt;/span&gt; &lt;span class="mf"&gt;0.039&lt;/span&gt; &lt;span class="mf"&gt;4.577&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.102&lt;/span&gt; &lt;span class="mf"&gt;0.255&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1567&lt;/span&gt; &lt;span class="mf"&gt;0.185&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.846&lt;/span&gt;
&lt;span class="mf"&gt;0.397&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.520&lt;/span&gt; &lt;span class="mf"&gt;0.206&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;100.&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;educ&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.3384&lt;/span&gt; &lt;span class="mf"&gt;0.108&lt;/span&gt; &lt;span class="mf"&gt;3.141&lt;/span&gt; &lt;span class="mf"&gt;0.002&lt;/span&gt;
&lt;span class="mf"&gt;0.127&lt;/span&gt; &lt;span class="mf"&gt;0.550&lt;/span&gt;
&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;arsenic&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;educ&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="mf"&gt;0.0601&lt;/span&gt; &lt;span class="mf"&gt;0.070&lt;/span&gt; &lt;span class="mf"&gt;0.855&lt;/span&gt; &lt;span class="mf"&gt;0.393&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.078&lt;/span&gt; &lt;span class="mf"&gt;0.198&lt;/span&gt;

&lt;span class="o"&gt;==================================================================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And the binned residual plot for arsenic now looks better.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/logarsenic_dist_bin_resid.png"&gt;
    &lt;img src="../images/logarsenic_dist_bin_resid.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Model error rates&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;pred_table()&lt;/code&gt; gives us a confusion matrix for the model. We can use
this to compute the error rate of the model.&lt;/p&gt;
&lt;p&gt;We should compare this to the null error rates, which comes from a model
that just classifies everything as whatever the most prevalent response
is. Here 58% of the respondents were switchers, so the null model just
classifies everyone as a switcher, and therefore has an error rate of
42%.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;model5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_table&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Model Error rate: {0: 3.0%}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_table&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;model5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pred_table&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Null Error Rate: {0: 3.0%}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;switch&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mf"&gt;568.&lt;/span&gt; &lt;span class="mf"&gt;715.&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;387.&lt;/span&gt; &lt;span class="mf"&gt;1350.&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;Model&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Null&lt;/span&gt; &lt;span class="n"&gt;Error&lt;/span&gt; &lt;span class="n"&gt;Rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So this was a more in-depth example of running a logistic regression
with statsmodels and the formula &lt;span class="caps"&gt;API&lt;/span&gt;. Unlike last time, when we were
just specifying the variables in the model, here we used the formula
language to apply transforms and create interactions. I really love
this: it drastically reduces the number of steps between thinking up a
model and fitting it.&lt;/p&gt;</summary></entry><entry><title>Machine Learning for Hackers Chapter 2, Part 2: Logistic regression with statsmodels</title><link href="http://slendermeans.org/ml4h-ch2-p2.html" rel="alternate"></link><updated>2012-12-21T04:04:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-12-21:ml4h-ch2-p2.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I last left chapter 2 of &lt;em&gt;Maching Learning for Hackers&lt;/em&gt; (a long time
ago), running some kernel density estimators on height and weight data
(see &lt;a href="../ml4h-ch2-p1.html"&gt;here&lt;/a&gt;. The next part of the chapter plots a scatterplot of
weight vs. height and runs a lowess smoother through it. I&amp;#8217;m not going
to write any more about the lowess function in statsmodels. I&amp;#8217;ve
discussed some issues with it (i.e. it&amp;#8217;s slow) &lt;a href="../lowess-speed.html"&gt;here&lt;/a&gt;. And it&amp;#8217;s my
sense that the lowess &lt;span class="caps"&gt;API&lt;/span&gt;, as it is now in statsmodels, is not long for
this world. The code is all in the IPython notebooks in &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH2"&gt;the Github
repo&lt;/a&gt; and is pretty straightforward.&lt;/p&gt;
&lt;h2&gt;Patsy and statsmodels formulas&lt;/h2&gt;
&lt;p&gt;What I want to skip to here is the logistic regressions the authors run
to close out the chapter. Back in the spring, I coded up the chapter in
&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/CH2/ch2.ipynb"&gt;this notebook&lt;/a&gt;. At this point, there wasn&amp;#8217;t really much cohesion
between pandas and statsmodels. You&amp;#8217;d end up doing data exploration and
munging with pandas, then pulling what you needed out of dataframes into
numpy arrays, and passing those arrays to statsmodels. (After writing
seemingly needless boilerplate code like
&lt;code&gt;X = sm.add_constant(X, prepend = True)&lt;/code&gt;. Who&amp;#8217;s out there running all
these regressions without constant terms, such that it makes sense to
force the use to explicitly add a constant vector to the data matrix?)&lt;/p&gt;
&lt;p&gt;Over the summer, though, something quite cool happened. &lt;a href="https://patsy.readthedocs.org/en/latest/#"&gt;patsy&lt;/a&gt;
brought a formula interface to Python, and it got integrated into a
number components of statsmodels. Skipper Seabold&amp;#8217;s &lt;a href="http://jseabold.net/presentations/seabold_pydata2012.html#slide1"&gt;Pydata
presentation&lt;/a&gt; is a good overview and demo. In a nutshell, statsmodels
now talks to your pandas dataframes via an expressive &amp;#8220;formula&amp;#8221;
description of your model.&lt;/p&gt;
&lt;p&gt;For example, imagine we had a dataframe, &lt;code&gt;df&lt;/code&gt;, with variables &lt;code&gt;x1&lt;/code&gt;,
&lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt;. If we wanted to regress &lt;code&gt;y&lt;/code&gt; on &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; with the
standard statmodels &lt;span class="caps"&gt;API&lt;/span&gt;, we&amp;#8217;d code something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Xmat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;x2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prepend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;yvec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;ols_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;OLS&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;yvec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Xmat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which is tolerable with short variable names. Once you start using
longer names or need more &lt;span class="caps"&gt;RHS&lt;/span&gt; variables it becomes a mess. With patsy
and the formula &lt;span class="caps"&gt;API&lt;/span&gt;, you just have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ols_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y \~ x1 + x2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which is just as simple as using &lt;code&gt;lm&lt;/code&gt; in R. You can also specify
variable transformations and interactions in the formula, without
needing to pre-compute variable for them. It&amp;#8217;s pretty slick.&lt;/p&gt;
&lt;p&gt;All of this is still brand new, and largely undocumented, so proceed
with caution. But I&amp;#8217;ve gotten very excited incorporating it into my
code. Stuff I wrote just 5 or 6 months ago looks clunky and outdated.&lt;/p&gt;
&lt;p&gt;So I&amp;#8217;ve updated the IPython notebook for chapter 2, &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/CH2/ch2_with_formulas.ipynb"&gt;here&lt;/a&gt;, to
incorporate the formula &lt;span class="caps"&gt;API&lt;/span&gt;. That&amp;#8217;s what I&amp;#8217;ll discuss in the rest of the
post.&lt;/p&gt;
&lt;h2&gt;Logistic regression with formulas in statmodels&lt;/h2&gt;
&lt;p&gt;The authors run a logistic regression to see if they can use a person&amp;#8217;s
height and weight to determine their gender. I&amp;#8217;m not really sure why
you&amp;#8217;d run such a model (or how meaningful it is once you run it, given
how co-linear height and weight are), but it&amp;#8217;s easy enough for
illustrating how to mechanically run a logistic regression and use it to
linearly separate groups.&lt;/p&gt;
&lt;p&gt;The dataset contains variables &lt;code&gt;Height&lt;/code&gt;, &lt;code&gt;Weight&lt;/code&gt;, and &lt;code&gt;Gender&lt;/code&gt;. The
latter is a string encoded either &lt;code&gt;Male&lt;/code&gt; or &lt;code&gt;Female&lt;/code&gt;. To run a logistic
regression, we&amp;#8217;ll want to transform this to a numerical 0/1 variable. We
can do this a number of ways, but I&amp;#8217;ll use the &lt;code&gt;map&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;statstmodels.formula.api&lt;/code&gt; module has a number of functions,
including &lt;code&gt;ols&lt;/code&gt;, &lt;code&gt;logit&lt;/code&gt;, and &lt;code&gt;glm&lt;/code&gt;. If we import &lt;code&gt;logit&lt;/code&gt; from the
module we can run a logistic regression easily.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;male_logit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formula&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Male \~ Height + Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;male_logit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With these results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;Optimization&lt;/span&gt; &lt;span class="nx"&gt;terminated&lt;/span&gt; &lt;span class="nx"&gt;successfully&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="nx"&gt;Current&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2091.297971&lt;/span&gt;
&lt;span class="nx"&gt;Iterations&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="nx"&gt;Logit&lt;/span&gt; &lt;span class="nx"&gt;Regression&lt;/span&gt; &lt;span class="nx"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="nx"&gt;Dep&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;Variable&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Male&lt;/span&gt; &lt;span class="nx"&gt;No&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;Observations&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="nx"&gt;Model&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Logit&lt;/span&gt; &lt;span class="nx"&gt;Df&lt;/span&gt; &lt;span class="nx"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;9997&lt;/span&gt;
&lt;span class="nx"&gt;Method&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;&lt;span class="caps"&gt;MLE&lt;/span&gt;&lt;/span&gt; &lt;span class="nx"&gt;Df&lt;/span&gt; &lt;span class="nx"&gt;Model&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="nb"&gt;Date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;Thu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="nx"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="nx"&gt;Pseudo&lt;/span&gt; &lt;span class="nx"&gt;R&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;squ&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.6983&lt;/span&gt;
&lt;span class="nx"&gt;Time&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;41&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt; &lt;span class="nx"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2091.3&lt;/span&gt;
&lt;span class="nx"&gt;converged&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt; &lt;span class="nx"&gt;&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;Null&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;6931.5&lt;/span&gt;
&lt;span class="nx"&gt;&lt;span class="caps"&gt;LLR&lt;/span&gt;&lt;/span&gt; &lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="nx"&gt;coef&lt;/span&gt; &lt;span class="nx"&gt;std&lt;/span&gt; &lt;span class="nx"&gt;err&lt;/span&gt; &lt;span class="nx"&gt;z&lt;/span&gt; &lt;span class="nx"&gt;P&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="nx"&gt;z&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nx"&gt;Conf.&lt;/span&gt; &lt;span class="nx"&gt;Int.&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

&lt;span class="o"&gt;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="nx"&gt;Intercept&lt;/span&gt; &lt;span class="mf"&gt;0.6925&lt;/span&gt; &lt;span class="mf"&gt;1.328&lt;/span&gt; &lt;span class="mf"&gt;0.521&lt;/span&gt; &lt;span class="mf"&gt;0.602&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.911&lt;/span&gt; &lt;span class="mf"&gt;3.296&lt;/span&gt;
&lt;span class="nx"&gt;Height&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.4926&lt;/span&gt; &lt;span class="mf"&gt;0.029&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;17.013&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.549&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.436&lt;/span&gt;
&lt;span class="nx"&gt;Weight&lt;/span&gt; &lt;span class="mf"&gt;0.1983&lt;/span&gt; &lt;span class="mf"&gt;0.005&lt;/span&gt; &lt;span class="mf"&gt;38.663&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.188&lt;/span&gt; &lt;span class="mf"&gt;0.208&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Just for fun, we can also run the logistic regression via a &lt;span class="caps"&gt;GLM&lt;/span&gt; with a
binomial family and logit link. This is similar to how I&amp;#8217;d run it in R.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;male_glm_logit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;glm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Male \~ Height + Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;families&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Binomial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;families&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logit&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;male_glm_logit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And the results are the same:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Generalized&lt;/span&gt; &lt;span class="n"&gt;Linear&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt; &lt;span class="n"&gt;Regression&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;Dep&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Observations&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="nl"&gt;Model:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;GLM&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Residuals&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;9997&lt;/span&gt;
&lt;span class="n"&gt;Model&lt;/span&gt; &lt;span class="n"&gt;Family&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Binomial&lt;/span&gt; &lt;span class="n"&gt;Df&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;Link&lt;/span&gt; &lt;span class="n"&gt;Function&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;logit&lt;/span&gt; &lt;span class="n"&gt;Scale&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="nl"&gt;Method:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;IRLS&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;Log&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Likelihood&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2091.3&lt;/span&gt;
&lt;span class="nl"&gt;Date:&lt;/span&gt; &lt;span class="n"&gt;Thu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt; &lt;span class="n"&gt;Dec&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt; &lt;span class="n"&gt;Deviance&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4182.6&lt;/span&gt;
&lt;span class="nl"&gt;Time:&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;41&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;37&lt;/span&gt; &lt;span class="n"&gt;Pearson&lt;/span&gt; &lt;span class="n"&gt;chi2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;9.72e+03&lt;/span&gt;
&lt;span class="n"&gt;No&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Iterations&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;span class="n"&gt;coef&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="n"&gt;err&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="err"&gt;\&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;|&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;95.0&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;Conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Int&lt;/span&gt;&lt;span class="p"&gt;.]&lt;/span&gt;

&lt;span class="o"&gt;------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class="n"&gt;Intercept&lt;/span&gt; &lt;span class="mf"&gt;0.6925&lt;/span&gt; &lt;span class="mf"&gt;1.328&lt;/span&gt; &lt;span class="mf"&gt;0.521&lt;/span&gt; &lt;span class="mf"&gt;0.602&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.911&lt;/span&gt; &lt;span class="mf"&gt;3.296&lt;/span&gt;
&lt;span class="n"&gt;Height&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.4926&lt;/span&gt; &lt;span class="mf"&gt;0.029&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;17.013&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.549&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.436&lt;/span&gt;
&lt;span class="n"&gt;Weight&lt;/span&gt; &lt;span class="mf"&gt;0.1983&lt;/span&gt; &lt;span class="mf"&gt;0.005&lt;/span&gt; &lt;span class="mf"&gt;38.663&lt;/span&gt; &lt;span class="mf"&gt;0.000&lt;/span&gt; &lt;span class="mf"&gt;0.188&lt;/span&gt; &lt;span class="mf"&gt;0.208&lt;/span&gt;

&lt;span class="o"&gt;==============================================================================&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now we can use the coefficients to plot a separating line in height-weight space.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;logit_pars&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;male_logit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;
&lt;span class="n"&gt;intercept&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;logit_pars&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Intercept&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;logit_pars&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;logit_pars&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Height&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;logit_pars&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s plot the data, color-coded by sex, and the separating line.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c"&gt;# Women points (coral)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights_f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;mfc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;None&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mec&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;coral&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# Men points (blue)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights_m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;mfc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;None&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mec&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# The separating line&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;intercept&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;slope&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
&lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#&lt;span class="caps"&gt;461B7E&lt;/span&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Height (in.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Weight (lbs.)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;upper left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/logit_hw_sex_separate.png"&gt;
    &lt;img src="../images/logit_hw_sex_separate.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There are several more examples using Patsy formulas with statsmodels
functions in later chapters. If you&amp;#8217;re accustomed to R&amp;#8217;s formula
notation, the transition from running models in R to running models in
statsmodels is easy. One of the annoying things in Python versus R is
the need to pull arrays out of pandas dataframes, because the functions
you want to apply to the data (say estimating models, or plotting) don&amp;#8217;t
interface with the dataframe, but instead numpy arrays. It&amp;#8217;s not
terrible, but it adds a layer of friction in the analysis. So it&amp;#8217;s great
that statsmodels is starting to integrate well with pandas.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 3: Naive Bayes Text Classification</title><link href="http://slendermeans.org/ml4h-ch3.html" rel="alternate"></link><updated>2012-12-20T04:20:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-12-20:ml4h-ch3.html</id><summary type="html">&lt;p&gt;I realize I haven&amp;#8217;t blogged about the rest of chapter 2 yet. I&amp;#8217;ll get
back to that, but chapter 3 is on my mind today. If you haven&amp;#8217;t seen
them yet, IPython notebooks up to chapter 9 are all up in the &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH"&gt;Github
repo&lt;/a&gt;. To view them online, you can check the links on &lt;a href="../category/will-it-python.html"&gt;this page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Chapter 3 is about text classification. The authors build a classifier
that will identify whether an e-mail is spam or not (&amp;#8220;ham&amp;#8221;) based on the
content of the e-mail&amp;#8217;s message. I won&amp;#8217;t go into much detail on how the
Naive Bayes classifier they use works (beyond what&amp;#8217;s evident in the
code). The theory is described well in the book and many other places.
I&amp;#8217;m just going to discuss implementation, assuming you know how the
classifier works in theory. The Python code for this project relies
heavily on the &lt;span class="caps"&gt;NLTK&lt;/span&gt; (Natural Language Toolkit) package, which is a
comprehensive library that includes functions for doing &lt;span class="caps"&gt;NLP&lt;/span&gt; and text
analysis, as well as an array of benchmark text corpora to use them on.
If you want to go deep into this stuff, two good resources are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://shop.oreilly.com/product/9780596516499.do"&gt;&lt;em&gt;Natural Language Processing with Python&lt;/em&gt;&lt;/a&gt; by S. Bird, E. Klein,
    and E. Loper; and&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.packtpub.com/python-text-processing-nltk-20-cookbook/book"&gt;&lt;em&gt;Python Text Processing with &lt;span class="caps"&gt;NLTK&lt;/span&gt; 2.0 Cookbook&lt;/em&gt;&lt;/a&gt; by J. Perkins&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Two versions of the program&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ve coded up two different versions of this chapter. The first,
&lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/CH3/ch3.ipynb"&gt;here&lt;/a&gt;, tries to follow the book relatively closely. The general
procedure they use is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parse and tokenize the e-mails&lt;/li&gt;
&lt;li&gt;Create a term-document matrix of the e-mails&lt;/li&gt;
&lt;li&gt;Calculate features of the training e-mails using the term-document
    matrix&lt;/li&gt;
&lt;li&gt;Train the classifier on these features&lt;/li&gt;
&lt;li&gt;Test the classifier on other sets of spam and ham e-mails&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;#8217;m not going to discuss this version in much detail, but you should
take a look at the notebook if you&amp;#8217;re interested. Two big takeaways from
this are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Python lacks a good term-document matrix tool.** I was surprised to find that &lt;span class="caps"&gt;NLTK&lt;/span&gt;, which has so much functionality including helper functions like &lt;code&gt;FreqDist&lt;/code&gt;, doesn&amp;#8217;t have a function for making term-document matrices similar to the &lt;code&gt;tdm&lt;/code&gt; function in R&amp;#8217;s &lt;code&gt;tm&lt;/code&gt; package. There is a Python module called &lt;code&gt;textmining&lt;/code&gt; (which you can install with pip) that does have a term-document matrix function, but it&amp;#8217;s pretty rudimentary. What you&amp;#8217;ll see in this chapter is that I&amp;#8217;ve coded up a term-document matrix function that uses the one in &lt;code&gt;textmining&lt;/code&gt; but adds some bells and whistles, and returns the &lt;span class="caps"&gt;TDM&lt;/span&gt; as a
(typically sparse) pandas dataframe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The authors&amp;#8217; classifier suffers from numerical errors.** The Naive
Bayes classifier calcalates the probability that a message is spam by
calculating the probability that the message&amp;#8217;s terms occur in a spam
message. So if the message is just &amp;#8220;buy viagra&amp;#8221;, and &amp;#8220;buy&amp;#8221; occurs in 75%
of the training spam, and &amp;#8220;viagra&amp;#8221; occurs in 50% of the training spam,
then the classifier assigns this a &amp;#8216;spam&amp;#8217; probability of .75 * .50 =
37.5%. The problem with this calculation is that there are typically
many terms, and the probabilities are often small, so their product can
end up smaller than machine precision and underflow to zero. The way
around this is to take the sum of the log probabilities (so log(.75) +
log(.25)). The authors don&amp;#8217;t do this, though, and it&amp;#8217;s apparent that
they end up with underflow errors. See, for example, the code output on
page 89. This is also what leads to them having essentially the same
error rates for &amp;#8220;hard&amp;#8221; ham as they do for &amp;#8220;easy&amp;#8221; ham in the tables on
pages 89 and 92. Once you fix this problem, it turns out the classifier
is actually much better for spam and easy ham than it appears in the
book, but it&amp;#8217;s way worse for hard ham.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;#8217;m going to focus on the second version of the program, though, in the
notebook called &lt;code&gt;ch3_nltk.ipynb&lt;/code&gt;. You can view it online &lt;a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/CH3/ch3_nltk.ipynb"&gt;here&lt;/a&gt;.In
this version, I use &lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#8217;s built-in &lt;code&gt;NaiveBayesClassifier&lt;/code&gt; function, and
avoid creating the &lt;span class="caps"&gt;TDM&lt;/span&gt; (which isn&amp;#8217;t really used for much in the original
code anyway).&lt;/p&gt;
&lt;h2&gt;Building a Naive Bayes spam classifier with &lt;span class="caps"&gt;NLTK&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ll follow the same logic as the program from chapter 3, but I&amp;#8217;ll do so
with a workflow more suited to &lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#8217;s functions. So instead of creating
a term-document matrix, and building my own Naive Bayes classifier, Ill
build a &lt;code&gt;features → label&lt;/code&gt; association for each training e-mail, and
feed a list of these to &lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#8217;s &lt;code&gt;NaiveBayesClassifier&lt;/code&gt; function.&lt;/p&gt;
&lt;h3&gt;Extracting word features from the e-mail messages&lt;/h3&gt;
&lt;p&gt;The program begins with some simple code that loads the e-mail files
from the directories, extracts the &amp;#8220;message&amp;#8221; or body of the e-mail, and
loads all those messages into a list. This follows the book&amp;#8217;s code
pretty closely, and we end up with training and testing lists of spam,
easy ham, and hard ham. The training data will be the e-mails in the
training directories for spam and easy ham. (So, like in the book, we&amp;#8217;re
not training on any hard ham.)&lt;/p&gt;
&lt;p&gt;Each e-mail in our classifier&amp;#8217;s training data will have a label (&amp;#8220;spam&amp;#8221;
or &amp;#8220;ham&amp;#8221;) and a feature set. For this application, we&amp;#8217;re just going to
use a feature set that is just a set of the unique words in the e-mail.
Below, I&amp;#8217;ll turn this into a dictionary to feed into the
&lt;code&gt;NaiveBayesClassifier&lt;/code&gt;, but first, let&amp;#8217;s get the set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is a similar to a &amp;#8220;bag-of-words&amp;#8221; model, in that it
doesn&amp;#8217;t care about word order or other semantic information. But a
&amp;#8220;bag-of-words&amp;#8221; usually considers the frequency of the word within the
document (like a histogram of the words), whereas we&amp;#8217;re only concerned
with whether it&amp;#8217;s in an e-mail, not how often it occurs.*&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Parsing and tokenizing the e-mails&lt;/h3&gt;
&lt;p&gt;I&amp;#8217;m going to use &lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#8217;s &lt;code&gt;wordpunct_tokenize&lt;/code&gt; function to break the
message into tokens. This splits tokens at white space and (most)
punctuation marks, and returns the punctuation along with the tokens on
each side. So &lt;code&gt;"I don't know. Do you?"&lt;/code&gt; becomes
&lt;code&gt;["I", "don","'", "t", "know", ".", "Do", "you", "?"]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you look through some of the training e-mails in
&lt;code&gt;train_spam_messages&lt;/code&gt; and &lt;code&gt;train_ham_messages&lt;/code&gt;, you&amp;#8217;ll notice a few
features that make extracting words tricky.&lt;/p&gt;
&lt;p&gt;First, there are a couple of odd text artefacts. The string &amp;#8216;3D&amp;#8217; shows
up in strange places in &lt;span class="caps"&gt;HTML&lt;/span&gt; attributes and other places, and we&amp;#8217;ll
remove these. Furthermore there seem to be some mid-word line wraps
flagged with an &amp;#8216;=&amp;#8217; where the word is broken across lines. For example,
the word &amp;#8216;apple&amp;#8217; might be split across lines like &amp;#8216;app=\nle&amp;#8217;. We want
to strip these out so we can recover &amp;#8216;apple&amp;#8217;. We&amp;#8217;ll want to deal with
all these first, before we apply the tokenizer.&lt;/p&gt;
&lt;p&gt;Second, there&amp;#8217;s a lot of &lt;span class="caps"&gt;HTML&lt;/span&gt; in the messages. We&amp;#8217;ll have to decide
first whether we want to keep &lt;span class="caps"&gt;HTML&lt;/span&gt; info in our set of words. If we do,
and we apply &lt;code&gt;wordpunct_tokenize&lt;/code&gt; to some &lt;span class="caps"&gt;HTML&lt;/span&gt;, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;&lt;span class="caps"&gt;HEAD&lt;/span&gt;&amp;gt;&amp;lt;/&lt;span class="caps"&gt;HEAD&lt;/span&gt;&amp;gt;&amp;lt;&lt;span class="caps"&gt;BODY&lt;/span&gt;&amp;gt;&amp;lt;!-- Comment --&amp;gt;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;would tokenize to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;lt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;HEAD&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;gt;&amp;lt;/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;HEAD&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;gt;&amp;lt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;BODY&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;gt;&amp;lt;!--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Comment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So if we drop the punctuation tokens, and get the unique set of what
remains, we&amp;#8217;d have &lt;code&gt;{"HEAD", "BODY", "Comment"}&lt;/code&gt;, which seems like what
we&amp;#8217;d want. For example, it&amp;#8217;s nice that this method doesn&amp;#8217;t make,
&lt;code&gt;&amp;lt;HEAD&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;/HEAD&amp;gt;&lt;/code&gt; separate words in our set, but just captures the
existence of this tag with the term &lt;code&gt;"HEAD"&lt;/code&gt;. It might be a problem that
we won&amp;#8217;t distinguish between the &lt;span class="caps"&gt;HTML&lt;/span&gt; tag &lt;code&gt;&amp;lt;HEAD&amp;gt;&lt;/code&gt; and &amp;#8220;head&amp;#8221; used as an
English word in the message. But for the moment I&amp;#8217;m willing to bet that
sort of conflation won&amp;#8217;t have a big effect on the classifier.&lt;/p&gt;
&lt;p&gt;If we don&amp;#8217;t want to count &lt;span class="caps"&gt;HTML&lt;/span&gt; information in our set of words, we can
set &lt;code&gt;strip_html&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, and we&amp;#8217;ll take all the &lt;span class="caps"&gt;HTML&lt;/span&gt; tags out before
tokenizing.&lt;/p&gt;
&lt;p&gt;Lastly we&amp;#8217;ll strip out any &amp;#8220;stopwords&amp;#8221; from the set. Stopwords are
highly common, therefore low information words, like &amp;#8220;a&amp;#8221;, &amp;#8220;the&amp;#8221;, &amp;#8220;he&amp;#8221;,
etc. Below I&amp;#8217;ll use &lt;code&gt;stopwords&lt;/code&gt;, downloaded from &lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#8217;s corpus library,
with a minor modifications to deal with this. (In other programs I&amp;#8217;ve
used the stopwords exported from R&amp;#8217;s &lt;code&gt;tm&lt;/code&gt; package.)&lt;/p&gt;
&lt;p&gt;Note that because our tokenizer splits contractions (&amp;#8220;she&amp;#8217;ll&amp;#8221; → &amp;#8220;she&amp;#8221;,
&amp;#8220;ll&amp;#8221;), we&amp;#8217;d like to drop the ends (&amp;#8220;ll&amp;#8221;). Some of these may be picked up
in &lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#8217;s &lt;code&gt;stopwords&lt;/code&gt; list, others we&amp;#8217;ll manually add. It&amp;#8217;s an
imperfect, but easy solution. There are more sophisticated ways of
dealing with this which are overkill for our purposes.&lt;/p&gt;
&lt;p&gt;Tokenizing, as perhaps you can tell, is a non-trivial operation. &lt;span class="caps"&gt;NLTK&lt;/span&gt;
has a host of other tokenizing functions of varying sophistication, and
even lets you define your own tokenizing rule using regex.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_msg_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="n"&gt;strip_html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;Returns the set of unique words contained in an e-mail message.&lt;/span&gt;
&lt;span class="sd"&gt;Excludes&lt;/span&gt;
&lt;span class="sd"&gt;any that are in an optionally-provided list.&lt;/span&gt;

&lt;span class="sd"&gt;&lt;span class="caps"&gt;NLTK&lt;/span&gt;&amp;#39;s &amp;#39;wordpunct&amp;#39; tokenizer is used, and this will break contractions.&lt;/span&gt;
&lt;span class="sd"&gt;For example, don&amp;#39;t -&amp;amp;gt; (don, &amp;#39;, t). Therefore, it&amp;#39;s advisable to&lt;/span&gt;
&lt;span class="sd"&gt;supply&lt;/span&gt;
&lt;span class="sd"&gt;a stopwords list that includes contraction parts, like &amp;#39;don&amp;#39; and &amp;#39;t&amp;#39;.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="c"&gt;# Strip out weird &amp;#39;3D&amp;#39; artefacts.&lt;/span&gt;
&lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;3D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Strip out html tags and attributes and html character codes,&lt;/span&gt;
&lt;span class="c"&gt;# like &amp;#39;&amp;amp;amp;nbsp;&amp;#39;  and &amp;#39;&amp;amp;amp;lt;&amp;#39;.&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;strip_html&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;amp;lt;(.|&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;n)\*?&amp;amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;amp;amp;&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;w+;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# wordpunct_tokenize doesn&amp;#39;t split on underscores. We don&amp;#39;t&lt;/span&gt;
&lt;span class="c"&gt;# want to strip them, since the token first_name may be informative&lt;/span&gt;
&lt;span class="c"&gt;# moreso than &amp;#39;first&amp;#39; and &amp;#39;name&amp;#39; apart. But there are tokens with&lt;/span&gt;
&lt;span class="nb"&gt;long&lt;/span&gt;
&lt;span class="c"&gt;# underscore strings (e.g. &amp;#39;name_&amp;#39;). We&amp;#39;ll just&lt;/span&gt;
&lt;span class="n"&gt;replace&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt;
&lt;span class="c"&gt;# multiple underscores with a single one, since &amp;#39;name_&amp;#39; is&lt;/span&gt;
&lt;span class="n"&gt;probably&lt;/span&gt;
&lt;span class="c"&gt;# not distinct from &amp;#39;name_&amp;#39; or &amp;#39;name_&amp;#39; in identifying spam.&lt;/span&gt;
&lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;_+&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Note, remove &amp;#39;=&amp;#39; symbols before tokenizing, since these&lt;/span&gt;
&lt;span class="c"&gt;# sometimes occur within words to indicate, e.g., line-wrapping.&lt;/span&gt;
&lt;span class="n"&gt;msg_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordpunct_tokenize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;=&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;n&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;

&lt;span class="c"&gt;# Get rid of stopwords&lt;/span&gt;
&lt;span class="n"&gt;msg_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;msg_words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;difference&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stopwords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Get rid of punctuation tokens, numbers, and single letters.&lt;/span&gt;
&lt;span class="n"&gt;msg_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;msg_words&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;[a-zA-Z]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt;
&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;gt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;msg_words&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Making a &lt;code&gt;(features, label)&lt;/code&gt; list&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;NaiveBayesClassifier&lt;/code&gt; function trains on data that&amp;#8217;s of the form
&lt;code&gt;[(features1, label1), features2, label2), ..., (featuresN, labelN)]&lt;/code&gt;
where &lt;code&gt;featuresi&lt;/code&gt; is a dictionary of features for e-mail &lt;code&gt;i&lt;/code&gt; and
&lt;code&gt;labeli&lt;/code&gt; is the label for e-mail &lt;code&gt;i&lt;/code&gt; (&lt;code&gt;spam&lt;/code&gt; or &lt;code&gt;ham&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;features_from_messages&lt;/code&gt; iterates through the messages
creating this list, but calls an outside function to create the features
for each e-mail. This makes the function modular in case we decide to
try out some other method of extracting features from the e-mails
besides the set of word. It then combines the features to the e-mail&amp;#8217;s
label in a tuple and adds the tuple to the list.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;word_indicator&lt;/code&gt; function calls &lt;code&gt;get_msg_words()&lt;/code&gt; to get an e-mail&amp;#8217;s
words as a set, then creates a dictionary with entries &lt;code&gt;{word: True}&lt;/code&gt;
for each word in the set. This is a little counter-intuitive (since we
don&amp;#8217;t have &lt;code&gt;{word: False}&lt;/code&gt; entries for words not in the set) but
&lt;code&gt;NaiveBayesClassifier&lt;/code&gt; knows how to handle it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;features_from_messages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Make a (features, label) tuple for each message in a list of a certain,&lt;/span&gt;
&lt;span class="sd"&gt;    label of e-mails (&amp;#39;spam&amp;#39;, &amp;#39;ham&amp;#39;) and return a list of these tuples.&lt;/span&gt;

&lt;span class="sd"&gt;    Note every e-mail in &amp;#39;messages&amp;#39; should have the same label.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;features_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;messages&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;features_labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features_labels&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;word_indicator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Create a dictionary of entries {word: True} for every unique&lt;/span&gt;
&lt;span class="sd"&gt;    word in a message.&lt;/span&gt;

&lt;span class="sd"&gt;    Note **kwargs are options to the word-set creator,&lt;/span&gt;
&lt;span class="sd"&gt;    get_msg_words().&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;msg_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_msg_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;msg_words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Training and evaluating the classifier&lt;/h2&gt;
&lt;p&gt;With those functions defined, we can apply them to the training and
testing spam and ham messages.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_train_test_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Make (feature, label) lists for each of the training&lt;/span&gt;
&lt;span class="sd"&gt;    and testing lists.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;train_spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features_from_messages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_spam_messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;spam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;train_ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features_from_messages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_easyham_messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;ham&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;train_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_spam&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;train_ham&lt;/span&gt;

    &lt;span class="n"&gt;test_spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features_from_messages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_spam_messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;spam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;test_ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features_from_messages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_easyham_messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;ham&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;test_hardham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features_from_messages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_hardham_messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;#39;ham&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;train_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_spam&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_ham&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_hardham&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that the training set we&amp;#8217;ll use to train the classifier combines
both the spam and easy ham training sets (since we need both types of
e-mail to train it).&lt;/p&gt;
&lt;p&gt;Finally, let&amp;#8217;s write a function to train the classifier and check how
accurate it is on the test data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;check_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;    Train the classifier on the training spam and ham, then check its&lt;/span&gt;
&lt;span class="sd"&gt;    accuracy&lt;/span&gt;
&lt;span class="sd"&gt;    on the test data, and show the classifier&amp;#39;s most informative features.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="c"&gt;# Make training and testing sets of (features, label) data&lt;/span&gt;
    &lt;span class="n"&gt;train_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_spam&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_ham&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_hardham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \\
    &lt;span class="n"&gt;make_train_test_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_extractor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# Train the classifier on the training set&lt;/span&gt;
    &lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NaiveBayesClassifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# How accurate is the classifier on the test sets?&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Test Spam accuracy: {0:.2f}%&amp;#39;&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_spam&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Test Ham accuracy: {0:.2f}%&amp;#39;&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_ham&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Test Hard Ham accuracy: {0:.2f}%&amp;#39;&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_hardham&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

    &lt;span class="c"&gt;# Show the top 20 informative features&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show_most_informative_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The function also prints out the results of &lt;code&gt;NaiveBayesClassifiers&lt;/code&gt;&amp;#8216;s
handy &lt;code&gt;show_most_informative_features&lt;/code&gt; method. This shows which features
are most unique to one label or another. For example, if &amp;#8220;viagra&amp;#8221; shows
up in 500 of the spam e-mails, but only 2 of the &amp;#8220;ham&amp;#8221; e-mails in the
training set, then the method will show that &amp;#8220;viagra&amp;#8221; is one of the most
informative features with a &lt;code&gt;spam:ham&lt;/code&gt; ratio of 250:1.&lt;/p&gt;
&lt;p&gt;So how do we do? I&amp;#8217;ll check two versions. The first uses the &lt;span class="caps"&gt;HTML&lt;/span&gt; info
in the e-mails in the classifier:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;check_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_indicator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which gives:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;98.71&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Ham&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;97.07&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Hard&lt;/span&gt; &lt;span class="n"&gt;Ham&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;13.71&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Most&lt;/span&gt; &lt;span class="n"&gt;Informative&lt;/span&gt; &lt;span class="n"&gt;Features&lt;/span&gt;
    &lt;span class="n"&gt;align&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;          &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;119.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;tr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;             &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;115.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;td&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;             &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;111.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;arial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;          &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;107.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;cellpadding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;    &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;97.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;cellspacing&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;    &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;94.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;            &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;80.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;bgcolor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;67.4&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;href&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;67.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;sans&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;62.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;colspan&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;61.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;font&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;61.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;valign&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;         &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;60.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;br&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;             &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;59.6&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;verdana&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;57.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;nbsp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;57.4&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;          &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;54.4&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;ff0000&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;         &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;53.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;ffffff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;         &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;50.6&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;border&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;         &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;49.6&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The classifier does a really good job for spam and easy ham, but it&amp;#8217;s
pretty miserable for hard ham. This may be because hard ham messages
tend to be &lt;span class="caps"&gt;HTML&lt;/span&gt;-formatted while easy ham messages aren&amp;#8217;t. Note how much
the classifier relies on &lt;span class="caps"&gt;HTML&lt;/span&gt; information&amp;#8212;nearly all the most
informative features are &lt;span class="caps"&gt;HTML&lt;/span&gt;-related.&lt;/p&gt;
&lt;p&gt;If we try just using the text of the messages, without the &lt;span class="caps"&gt;HTML&lt;/span&gt;
information, we lose a tiny bit of accuracy in identifying spam but do
much better with the hard ham.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;check_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_indicator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stopwords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strip_html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;shows&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Spam&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;96.64&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Ham&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;98.64&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;Hard&lt;/span&gt; &lt;span class="n"&gt;Ham&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;56.05&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;Most&lt;/span&gt; &lt;span class="n"&gt;Informative&lt;/span&gt; &lt;span class="n"&gt;Features&lt;/span&gt;
    &lt;span class="n"&gt;dear&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;          &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;41.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;aug&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;38.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;guaranteed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;    &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;35.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;assistance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;    &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;29.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;groups&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;27.9&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;mailings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;      &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;25.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;sincerely&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;     &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;23.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;          &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;23.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;mortgage&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;      &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;21.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;sir&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;21.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;sponsor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;       &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;20.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;article&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;       &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;20.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;assist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;19.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;income&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;18.6&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;tue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;18.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;mails&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;         &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;18.3&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;iso&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;           &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;17.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;admin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;         &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;17.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;monday&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;        &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;17.7&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="n"&gt;earn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;          &lt;span class="n"&gt;spam&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ham&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;17.0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Check out the most informative features; they make a lot of sense. Note
mostly spammers address you with &amp;#8220;Dear&amp;#8221; and &amp;#8220;Sir&amp;#8221; and sign off with
&amp;#8220;Sincerely,&amp;#8221;. (Probably those Nigerian princes; they tend to be polite.)
Other spam flags that gel with our intuition are &amp;#8220;guaranteed&amp;#8221;,
&amp;#8220;mortgage&amp;#8221;, &amp;#8220;assist&amp;#8221;, &amp;#8220;assistance&amp;#8221;, and &amp;#8220;income.&amp;#8221;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So we&amp;#8217;ve built a simple but decent spam classifier with just a tiny
amount of code. &lt;span class="caps"&gt;NLTK&lt;/span&gt; provides a wealth of tools for doing this sort of
thing more seriously including ways to extract more sophisticated
features and more complex classifiers.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Better typography for IPython notebooks</title><link href="http://slendermeans.org/better-typography-for-ipython-notebooks.html" rel="alternate"></link><updated>2012-12-05T05:34:00-05:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-12-05:better-typography-for-ipython-notebooks.html</id><summary type="html">&lt;p&gt;&lt;em&gt;(Warning: ignorant rant coming up)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Like everyone else who&amp;#8217;s ever used it, I love the &lt;a href="http://ipython.org/ipython-doc/rel-0.13.1/interactive/htmlnotebook.html"&gt;IPython
notebook.&lt;/a&gt; It&amp;#8217;s not only an awesomely productive environment to work
in, it&amp;#8217;s also the most powerful weapon in the Python evangelist&amp;#8217;s
arsenal (suck it, Matlab).&lt;/p&gt;
&lt;p&gt;I also think it&amp;#8217;s not hard to imagine a world where scientific papers
are all just literate programs. And the notebook is probably one of the
best tools for literate programming around in any language. The
intregration of markdown and LaTeX/MathJax into the notebook is just
fantastic.&lt;/p&gt;
&lt;p&gt;But it does have one weakness as a literate programming tool. The
default typography is ugly as sin.&lt;/p&gt;
&lt;p&gt;There are several issues, but two major ones are easily fixable.&lt;/p&gt;
&lt;h2&gt;Long lines&lt;/h2&gt;
&lt;p&gt;By far the biggest issue is that the text and input cells extend to 100%
of the window width. Most people keep their browser windows open wider
than is comfortable reading width, so you end up with long hard-to-read
lines of text in the markdown cells.&lt;/p&gt;
&lt;p&gt;And for the code, it would be nice to have the code cell discourage you
from long lines. The variable width cells don&amp;#8217;t. I&amp;#8217;m an 80-character
anal retentive, and even I have trouble in the notebook getting a sense
of when a line is too long.&lt;/p&gt;
&lt;p&gt;When you write a script in a text editor, there&amp;#8217;s lots of previous code
in the viewable window, so your eye gets a sense of the &amp;#8216;right-margin&amp;#8217;
of the code. (Not to mention many editors will indicate the 80- or
whatever-character column, so you know exactly when to break). But in
the notebook, your code is typically broken up into smaller blocks, and
those blocks are interspersed with output and other cells. It&amp;#8217;s hard to
get a visual sense of the right margin.&lt;/p&gt;
&lt;h2&gt;Ugly fonts&lt;/h2&gt;
&lt;p&gt;Text and markdown cells are typically rendered in Helvetica or Arial.
Helvetica is a fine font, obviously, but it&amp;#8217;s not really suitable for
paragraphs of text (how many books, magazines, newspapers, or academic
papers do you see with body text typeset in Helvetica?). And combined
with the small size and long lines makes it hard to read and just plain
ugly. I don&amp;#8217;t think I have to say anything about Arial.&lt;/p&gt;
&lt;p&gt;The way I use the notebook&amp;#8212;with markdown cells used for long stretches
of explanatory text and result interpretation&amp;#8212;it&amp;#8217;s better to have the
text cells render in a serif font. This way it stands out from the code
and output cells more. Serif fonts also have more distinctive italics,
and integrate better with LaTeX/MathJax math.&lt;/p&gt;
&lt;p&gt;Code cells and interpreter output cells render in whatever your default
monospace font is. That&amp;#8217;s typically Courier or Courier New. This is
fine, but really, this is the 21st century&amp;#8212;we can do &lt;a href="http://blogs.adobe.com/typblography/2012/09/source-code-pro.html"&gt;a lot better&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Update: one more thing&lt;/h2&gt;
&lt;p&gt;I realize I&amp;#8217;ve made one other change that I think is important. The
default ordered list in the notebook uses roman numerals (I, &lt;span class="caps"&gt;II&lt;/span&gt;, &lt;span class="caps"&gt;III&lt;/span&gt;,
&amp;#8230;). I almost always want arabic numerals (1, 2, 3, &amp;#8230;) instead. We
can change this in the file &lt;code&gt;renderedhtml.css&lt;/code&gt; with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nc"&gt;.rendered_html&lt;/span&gt; &lt;span class="nt"&gt;ol&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="k"&gt;list-style&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="k"&gt;decimal&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;margin&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;1em&lt;/span&gt; &lt;span class="m"&gt;2em&lt;/span&gt;&lt;span class="p"&gt;;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(Also check the comments for other, and typically better ways to make
changes.) You can also modify sub-levels &lt;code&gt;ol ol&lt;/code&gt;, &lt;code&gt;ol ol ol&lt;/code&gt;, etc.
Ideally I&amp;#8217;d like to have nested numbers 1.1, 1.1.1, but this isn&amp;#8217;t
straightforward so I haven&amp;#8217;t implemented it. If anyone has tips, I&amp;#8217;d be
thrilled to hear them.&lt;/p&gt;
&lt;h2&gt;Fixing it (locally, at least)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;(Warning: I don&amp;#8217;t know what I&amp;#8217;m doing. Don&amp;#8217;t make any of these changes,
or any others, without backing up the files first.)&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(&lt;strong&gt;Update&lt;/strong&gt;: Matthias Bussonnier has an &lt;a href="http://http://nbviewer.ipython.org/urls/raw.github.com/Carreau/posts/master/Blog1.ipynb"&gt;informative post&lt;/a&gt;
showing the right way to make these changes. If you make the &lt;span class="caps"&gt;CSS&lt;/span&gt; changes
I describe below, do it the way he advises, not through the files I
describe here.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The notebook is served through the browser, so its frontend is basically
just &lt;span class="caps"&gt;HTML&lt;/span&gt;, Javascript, and &lt;span class="caps"&gt;CSS&lt;/span&gt;. The typography and appearance of the
notebook is nearly all driven by &lt;span class="caps"&gt;CSS&lt;/span&gt; files located where IPython is
stored on your system. This will differ based on your &lt;span class="caps"&gt;OS&lt;/span&gt; and your Python
distribution. On my mac, with the AnacondaCE distribution, the
stylesheets are located
in &lt;code&gt;/Users/cvogel/anaconda/lib/python2.7/site-packages/IPython/frontend/html/notebook/static&lt;/code&gt;.
There are several subfolders there, including one called &lt;code&gt;/css&lt;/code&gt; and
&lt;code&gt;/codemirror&lt;/code&gt;. You can also take a look at the stylesheet files by
firing up a notebook, and using your browser&amp;#8217;s inspector. If your
browser (e.g. Chrome) lets you edit stylesheets on the fly in the
inspector, you can try out changes relatively safely.&lt;/p&gt;
&lt;p&gt;Here are the edits I&amp;#8217;ve made on my system to address the issues above.
First, in the /css folder, in the file called notebook.css&lt;/p&gt;
&lt;p&gt;1. Set code input cells to be narrower (code that runs past the width
will be invisible). I try to set this for about 80 characters plus some
buffer. There&amp;#8217;s not way to set width as number of characters in &lt;span class="caps"&gt;CSS&lt;/span&gt;, so
you may have to experiment to see what ex-widths works with your font.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="nc"&gt;.input&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;105ex&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c"&gt;/* about 80 chars + buffer */&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;​2. Fixing markdown/text cells. I make changes to the font, width, and
linespacing. I&amp;#8217;m using Charis &lt;span class="caps"&gt;SIL&lt;/span&gt;, a font based on the classic Bitstream
Charter, and freely available &lt;a href="http://scripts.sil.org/cms/scripts/page.php?site_id=nrsi&amp;amp;id=CharisSILfont"&gt;here&lt;/a&gt;. Shortening the lines and adding
some line space (120% to 150% of point size is usually a good range) for
legibility.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="nc"&gt;.text_cell&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;105ex&lt;/span&gt; &lt;span class="c"&gt;/* instead of 100%, */&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="nc"&gt;.text_cell_render&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="c"&gt;/*font-family: &amp;quot;Helvetica Neue&amp;quot;, Arial, Helvetica, Geneva, sans-serif;*/&lt;/span&gt;
&lt;span class="k"&gt;font-family&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Charis &lt;span class="caps"&gt;SIL&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="k"&gt;serif&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c"&gt;/* Make non-code text serif. */&lt;/span&gt;
&lt;span class="k"&gt;line-height&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;145&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c"&gt;/* added for some line spacing of text. */&lt;/span&gt;
&lt;span class="k"&gt;width&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;105ex&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c"&gt;/* instead of &amp;#39;inherit&amp;#39; for shorter lines */&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;​3. Add styles to specify sizes for headers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;/* Set the size of the headers */&lt;/span&gt;
&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="nc"&gt;.text_cell_render&lt;/span&gt; &lt;span class="nt"&gt;h1&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;font-size&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;18pt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nt"&gt;div&lt;/span&gt;&lt;span class="nc"&gt;.text_cell_render&lt;/span&gt; &lt;span class="nt"&gt;h2&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;font-size&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;14pt&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, in the `/codemirror/lib subfolder, there&amp;#8217;s a file called
codemirror.css. In here we can change the font used for code, both input
and interpreter output. I&amp;#8217;m using Consolas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nc"&gt;.CodeMirror&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="k"&gt;font-family&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Consolas&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="k"&gt;monospace&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Obviously these changes only affect notebooks you view on your local
machine, and whoever views your notebooks on their own machine, or on
&lt;a href="http://nbviewer.ipython.org"&gt;nbviewer&lt;/a&gt; will see the default style.&lt;/p&gt;
&lt;p&gt;Here are before and after shots of these changes:&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/ipynb_unstyled.png"&gt;
  &lt;img src="../images/ipynb_unstyled.png" width = 500px /&gt;
&lt;/a&gt;
&lt;a href="../images/ipynb_styled.png"&gt;
  &lt;img src="../images/ipynb_styled.png" width = 500px /&gt;&lt;/a&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Fixing it (globally?)&lt;/h2&gt;
&lt;p&gt;So this is all cute right? And it&amp;#8217;s nice that we can do some
customizations to the notebook, but, you know, big deal.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;d argue this is actually more important than just aesthetic tinkering.
The IPython notebook is becoming a one-stop-shop for exploration,
collaboration, publication, distribution, and replication in data
analysis. Like I said above, I think it&amp;#8217;s not unreasonable that
notebooks could replace a large class of scientific papers. But to do
that, it has to perform as well as all the fragmented tools that
researchers are currently using. Otherwise, people are going to keep
pasting their code and results into Word and Latex documents. In other
words, the notebook has to work not just as an interactive environment,
but also as a static document. The IPython team realizes this, which is
why tools like &lt;a href="https://github.com/ipython/nbconvert"&gt;nbconvert&lt;/a&gt; exist.&lt;/p&gt;
&lt;p&gt;People are doing amazing things in the notebook. The typography should
encourage people to read them, and not just serve as suped-up comments.&lt;/p&gt;
&lt;p&gt;Tools are often strongly associated with aesthetic characteristics that
are only peripheral to the tool itself. ggplot can make charts that look
however you want, but when people think of ggplot, they think of the
gray background and the Color Brewer palette. And while main selling
point of ggplot is its abstraction of the graph-making process, I think
it was the distinctive and attractive style of its graphs that made it
catch on so successfully. On the opposite end of the spectrum, when
people think of Stata graphics, they think of &lt;a href="http://www.survey-design.com.au/distrib2.png"&gt;this&lt;/a&gt;, and wince. And
Latex will typeset documents with whatever crazy font you want, but in
everyone&amp;#8217;s mind, Latex &amp;#8660; Computer Modern (for better or worse).
Design defaults are important: they&amp;#8217;re marketing and they encourage good
habits by your users. It&amp;#8217;d be a shame to have it be that people think of
the IPython notebook and picture long lines of small, single-spaced
Helvetica Neue.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s an insanely powerful tool. It&amp;#8217;d be awesome if it were beautiful
too, and that goal seems eminently do-able.&lt;/p&gt;</summary><category term="python"></category><category term="typography"></category></entry><entry><title>How do you speed up 40,000 weighted least squares calculations? Skip 36,000 of them.</title><link href="http://slendermeans.org/lowess-speed.html" rel="alternate"></link><updated>2012-05-14T23:46:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-05-14:lowess-speed.html</id><summary type="html">&lt;p&gt;Despite having finished all the programming for Chapter 2 of &lt;span class="caps"&gt;MLFH&lt;/span&gt; a
while ago, there&amp;#8217;s been a long hiatus since the&lt;a href="../ml4h-ch2-p1.html"&gt;first post on that
chapter&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;(S)lowess&lt;/h2&gt;
&lt;p&gt;Why the delay? The second part of the code focuses on two procedures:
lowess scatterplot smoothing, and logistic regression. When implementing
the former in &lt;a href="http://statsmodels.sourceforge.net/devel/generated/statsmodels.nonparametric.api.lowess.html#statsmodels.nonparametric.api.lowess"&gt;statsmodels&lt;/a&gt;, I found that it was running &lt;em&gt;dog slow&lt;/em&gt; on
the data&amp;#8212;in this case a scatterplot of 10,000 height-vs.-weight points.
Indeed, for these 10,000 points, lowess, run with the default
parameters, required about 23 seconds. After importing modules and
defining variables according to my &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH2"&gt;IPython notebook&lt;/a&gt;, we can run
&lt;code&gt;timeit&lt;/code&gt; on the function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This results in&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;42.6&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;on the machine I&amp;#8217;m writing this on  (a Windows laptop with a 2.67 GHz i5
processor; timings are faster, but still in the 30 sec. range on my 2.5
GHz i7 Macbook).&lt;/p&gt;
&lt;p&gt;An R user&amp;#8212;or really a user of any other statistical package&amp;#8212;is going
to be confused here. We&amp;#8217;re all used to lowess being a relatively
instantaneous procedure. It&amp;#8217;s an oft-used option for graphics packages
like Lattice and ggplot2 &amp;#8212; and it doesn&amp;#8217;t take 20-30 seconds to
generate a plot with a lowess curve superimposed. So what&amp;#8217;s the deal? Is
something wrong with the statsmodels implementation?&lt;/p&gt;
&lt;h2&gt;The naive lowess algorithm&lt;/h2&gt;
&lt;p&gt;Short answer: no. Long answer: yeah, kinda. Let&amp;#8217;s start by looking at
the lowess algorithm in general, sticking to the 2-D y-vs.-x scatterplot
case. (I don&amp;#8217;t really find multi-dimensional lowess useful anyway; maybe
others put it to frequent use. If so, I&amp;#8217;d like to hear about it).&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s say we have data &lt;em&gt;{x&lt;sub&gt;1&lt;/sub&gt;, &amp;#8230;, x&lt;sub&gt;n&lt;/sub&gt;}&lt;/em&gt; and &lt;em&gt;{y&lt;sub&gt;1&lt;/sub&gt;, &amp;#8230;, y&lt;sub&gt;n&lt;/sub&gt;}&lt;/em&gt;. The
idea is to fit a set of values &lt;em&gt;{y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;, &amp;#8230;, y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;n&lt;/sub&gt;}&lt;/em&gt; where each is the
prediction at &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; from a weighted regression using a fixed
neighborhood of points around &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;. The weighting scheme puts less
weight on points that are far from &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;. The regression can be linear,
or polynomial, but linear is typical, and lowess procedures that use
polynomials with more than 2 degrees are rare.&lt;/p&gt;
&lt;p&gt;After we get this first set of fits, we usually run the regressions a
few more times, each time modifying the weights to take into account
residuals from the previous fit. These &amp;#8220;robustifying&amp;#8221; iterations apply
successively less weight to outlying points in the data, reducing their
influence on the final curve.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s the recipe:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the number of neighbors, &lt;em&gt;k&lt;/em&gt;, to use in each local
    regression, and the number of robustifying iterations.&lt;/li&gt;
&lt;li&gt;Sort the data, both &lt;em&gt;x &lt;/em&gt;and &lt;em&gt;y&lt;/em&gt;,&lt;em&gt; &lt;/em&gt;by the order of the &lt;em&gt;x&lt;/em&gt;-values.&lt;/li&gt;
&lt;li&gt;For each &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; in &lt;em&gt;{x&lt;sub&gt;1&lt;/sub&gt;, &amp;#8230; x&lt;sub&gt;n&lt;/sub&gt;}&lt;/em&gt;:&lt;ol&gt;
&lt;li&gt;Find the &lt;em&gt;k&lt;/em&gt; points nearest to &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; (the &lt;em&gt;neighborhood&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Calculate the weights for each &lt;em&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; in the neighborhood. This
    requires:&lt;ol&gt;
&lt;li&gt;Calculating the distance between each &lt;em&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; and &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; and
    applying a weighting function to these distances.&lt;/li&gt;
&lt;li&gt;Take the weights calculated from the previous fit&amp;#8217;s
    residuals (if this is not the first fit) and multiply them
    by the distance weights.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Run a regression of the &lt;em&gt;y&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt;s on the &lt;em&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt;s in the
    neighborhood, using the weights calculated in part B above.
    Predict &lt;em&gt;y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Calculate the residuals from this fitted series of &lt;em&gt;{y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;, &amp;#8230;,
    y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;n&lt;/sub&gt;}&lt;/em&gt;, and compute a weight from each of them.&lt;/li&gt;
&lt;li&gt;Repeat 3 and 4 for the specified number of robustifying iterations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clearly, this is an expensive procedure. For 10,000 points and 3
robustifying iterations (which is the default in R and statsmodels),
you&amp;#8217;re calculating weights and running regressions 40,000 times (1
initial fit + 3 robustifying iterations).  Running R&amp;#8217;s &lt;code&gt;lm.fit&lt;/code&gt; (which
is the lean, fast engine under &lt;code&gt;lm&lt;/code&gt;) 40,000 times costs about 11
seconds. Add on all the costs from weight calculations&amp;#8212;which will
happen 40,000 &amp;times; &lt;em&gt;k&lt;/em&gt; times, since a weight needs to be calculated for
each point&amp;#8217;s neightbor&amp;#8212;-and it&amp;#8217;s not surprising that the statsmodels
version is as slow as it is. It is an inherently expensive algorithm.&lt;/p&gt;
&lt;h2&gt;Cheating our way to a faster lowess&lt;/h2&gt;
&lt;p&gt;The question is, why is R&amp;#8217;s lowess so fast? The answer is that R&amp;#8212;-and
most other implementations, going back to Clevelands &lt;a href="http://www.netlib.org/go/lowess.f"&gt;lowess.f&lt;/a&gt; Fortan
program&amp;#8212;don&amp;#8217;t perform lowess calculations on all that data.&lt;/p&gt;
&lt;p&gt;If you look at the &lt;a href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lowess.html"&gt;R help file for lowess&lt;/a&gt;, you&amp;#8217;ll see that in
addition to the parameters we&amp;#8217;d expect&amp;#8212;the data &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;; a
parameter to determine the size of the neighborhood; and the number of
robustifying iterations&amp;#8212;there&amp;#8217;s an argument called &lt;code&gt;delta&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The idea behind &lt;code&gt;delta&lt;/code&gt; is the following: &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; that are close together
aren&amp;#8217;t very interesting. If we&amp;#8217;ve already calculated &lt;em&gt;y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt; from the
neighborhood of data around &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;, and |&lt;em&gt;x&lt;sub&gt;i+1&lt;/sub&gt;&lt;/em&gt; - &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;| &amp;lt; &lt;code&gt;delta&lt;/code&gt;,
then we don&amp;#8217;t really need to calculate &lt;em&gt;y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;i+1&lt;/sub&gt;&lt;/em&gt;. It&amp;#8217;s bound to be near &lt;em&gt;y&lt;sup&gt;*&lt;/sup&gt;&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Instead let&amp;#8217;s go out to an &lt;em&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/em&gt; that&amp;#8217;s farther away from &lt;em&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/em&gt;&amp;#8212;-say
the farthest one still within &lt;code&gt;delta&lt;/code&gt; distance. Let&amp;#8217;s fit another
weighted regression here. All those points in between&amp;#8212;within that delta
distance&amp;#8212;can be approximated by a line going between the two regression
fits we made.   Then, just keep skipping along in these delta-sized
steps&amp;#8212;back-filling the predictions by linear interpolation as we
go&amp;#8212;until the end of the data.&lt;/p&gt;
&lt;p&gt;How much work have we saved ourselves? Assume as above 10,000 points and
4 iterations. If the &lt;em&gt;x&lt;/em&gt;&amp;#8216;s are uniformly distributed along the axis, and
we take &lt;code&gt;delta&lt;/code&gt; to be &lt;code&gt;0.01 * (max(x) - min(x))&lt;/code&gt; (which is the default
value in R), then we&amp;#8217;re only running 100 regressions per iteration, or
400 overall. Compared to the 40,000 that statsmodels is running, we can
see why R is much faster. It&amp;#8217;s cheating!&lt;/p&gt;
&lt;p&gt;This kind of approximating is fine, really. It&amp;#8217;s just assuming that, if
our model is &lt;em&gt;y = f(x) + e&lt;/em&gt; and &lt;em&gt;f(x)&lt;/em&gt; is what we&amp;#8217;re trying to estimate
with lowess, we can take the linear approximation of it in small
neighborhoods.&lt;/p&gt;
&lt;h2&gt;Implementing a faster lowess in Python&lt;/h2&gt;
&lt;p&gt;Algorithms for lowess written in low level languages aren&amp;#8217;t hard to
find. In addition to Cleveland&amp;#8217;s &lt;a href="http://www.netlib.org/go/lowess.f"&gt;Fortran implementation&lt;/a&gt;,
there&amp;#8217;s also a &lt;a href="http://svn.r-project.org/R/trunk/src/library/stats/src/lowess.c"&gt;C version&lt;/a&gt; used by R (which is basically a direct
translation of Cleveland&amp;#8217;s, but without all the pesky commenting to let
you know what it&amp;#8217;s doing).&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/statsmodels/statsmodels/blob/master/statsmodels/nonparametric/smoothers_lowess.py"&gt;statsmodel version&lt;/a&gt; though, is nicely organized&amp;#8212;broken into
sub-functions with  clear names, and exploiting vectorized operations.
But it&amp;#8217;s slowness is not because it doesn&amp;#8217;t exploit the &lt;code&gt;delta&lt;/code&gt; trick.
It also runs some expensive operations, like a call to SciPy&amp;#8217;s &lt;code&gt;lstsq&lt;/code&gt;
function in each tight loop.&lt;/p&gt;
&lt;p&gt;So, in addition to adding the delta trick, we&amp;#8217;d like to speed up those
calculations in the tight loop (part 3 in the list above) as much as
possible. Luckily, Cython lets us split the difference.&lt;/p&gt;
&lt;p&gt;My Cython version of lowess is in my github repo, &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH2/lowess%20work"&gt;here&lt;/a&gt;, in the file
cylowess.py. There&amp;#8217;s also an IPython notebook demonstrating it in
action, and files comprising a testing suite, comparing its output to
R&amp;#8217;s.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s take a look at some real squiggly data to see how it works. The
Silverman motorcycle collision data, which is available as &lt;code&gt;mcycle&lt;/code&gt; in
R&amp;#8217;s &lt;code&gt;MASS&lt;/code&gt; package, is great test data for non-parametric curve fitting
procedures. In addition to not having any simple parametric shape, it&amp;#8217;s
got some edge case issues that can cause problems, like repeated
x-values.&lt;/p&gt;
&lt;p&gt;This plot compares my lowess implementation with statsmodels&amp;#8217; and R&amp;#8217;s:&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/motorcycle-lowess-comparisons.png"&gt;
  &lt;img src="../images/motorcycle-lowess-comparisons.png" width=350px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The aggregate difference between R&amp;#8217;s lowess and mine?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;R and New Lowess &lt;span class="caps"&gt;MAD&lt;/span&gt;: &lt;/span&gt;&lt;span class="si"&gt;%5.2e&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r_lowess&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;new_lowess&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;


&lt;span class="n"&gt;R&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;New&lt;/span&gt; &lt;span class="n"&gt;Lowess&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MAD&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;1.62e-13&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So it looks like it works.&lt;/p&gt;
&lt;p&gt;Now let&amp;#8217;s look at some timings. I&amp;#8217;ll create some test data: 10,000
points, where &lt;code&gt;x&lt;/code&gt; is uniformly distributed on [0, 20], and
&lt;code&gt;y = sin(x) + N(0, 0.5)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Statsmodel&amp;#8217;s lowess:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;smlw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;22.8&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The new Cythonized lowess:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;cyl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;10.8&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is without the &lt;code&gt;delta&lt;/code&gt; trick. Skimming the fat off of those
tight-looped operations and Cythonizing them cut the run time in half.
11 seconds still sucks, though, so let&amp;#8217;s see what &lt;code&gt;delta&lt;/code&gt; gets us.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;timeit&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;cyl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;loops&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;125&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt; &lt;span class="n"&gt;per&lt;/span&gt; &lt;span class="n"&gt;loop&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Much better. That&amp;#8217;s the kind of time skipping 36,000 weighted
least-squares calculations will save you. Given that this is some curvy
data, is all this linear interpolation acceptable? I&amp;#8217;ll re-run both with
a better level of the &lt;code&gt;frac&lt;/code&gt; parameter; the default is 2/3, but I&amp;#8217;ll
reduce it to 1/10 to use smaller neighborhoods in the regression and
allow for more curvature. Here&amp;#8217;s the plot:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sm_lowess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;smlw&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;frac&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;new_lowess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cyl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lowess&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;frac&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;delta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/sine-10k-pts-lowess-compare.png"&gt;
  &lt;img src="../images/sine-10k-pts-lowess-compare.png" width=400px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Which looks just as good as the non-interpolated version, but doesn&amp;#8217;t
leave you twiddling your thumbs.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;After all this, we have a version of lowess that&amp;#8217;s competitive with R&amp;#8217;s
&lt;code&gt;lowess&lt;/code&gt; function. R also has a much richer &lt;code&gt;loess&lt;/code&gt; function, for which
there&amp;#8217;s no real statmodels equivalent. &lt;code&gt;loess&lt;/code&gt; is a full-blown class
from which one can make predictions and compute confidence intervals,
among other things. It also allows for fitting a higher-dimensional
surface, not just a curve. But I have a day job, so that&amp;#8217;s all for some
other time. This kind of simple lowess is typically enough for most
needs.&lt;/p&gt;
&lt;p&gt;With this obsessive compulsive diversion into the guts of lowess out of
the way, I&amp;#8217;ll wrap up Chapter 2 of &lt;span class="caps"&gt;MLFH&lt;/span&gt; in my next post.&lt;/p&gt;</summary><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 2, Part 1: Summary stats and density estimators</title><link href="http://slendermeans.org/ml4h-ch2-p1.html" rel="alternate"></link><updated>2012-05-01T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-05-01:ml4h-ch2-p1.html</id><summary type="html">&lt;p&gt;Chapter 2 of &lt;span class="caps"&gt;MLFH&lt;/span&gt; summarizes techniques for exploring your data:
determining data types, computing quantiles and other summary
statistics, and plotting simple exploratory graphics. I&amp;#8217;m not going to
replicate it in its entirety; I&amp;#8217;m just going to hit some of the more
involved or interesting parts. The IPython notebook I created for this
chapter, which lives &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH2"&gt;here&lt;/a&gt;, contains more code than I&amp;#8217;ll present on
the blog.&lt;/p&gt;
&lt;p&gt;This part&amp;#8217;s highlights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pandas objects, as we&amp;#8217;ve seen before, have methods that provide
    simple summary statistics.&lt;/li&gt;
&lt;li&gt;The plotting methods in Pandas let you pass parameters to the
    Matplotlib functions they call. I&amp;#8217;ll use this feature to mess around
    with histogram bins.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;gaussian_kde&lt;/code&gt; (kernel density estimator) function in
    &lt;code&gt;scipy.stats.kde&lt;/code&gt; provides density estimates similar to R&amp;#8217;s
    &lt;code&gt;density&lt;/code&gt; function for Gaussian kernels. The &lt;code&gt;kdensity&lt;/code&gt; function, in
    &lt;code&gt;statsmodels.nonparametric.kde&lt;/code&gt; provides that and other kernels, but
    given the state of &lt;code&gt;statsmodels&lt;/code&gt;&amp;#8216; documentation, you would probably
    only find this function by accident. It&amp;#8217;s also substantially slower
    than &lt;code&gt;gaussian_kde&lt;/code&gt; on large data. *&lt;em&gt;[Not quite so! See
    update at the end.]&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Height and weight data&lt;/h2&gt;
&lt;p&gt;The data analyzed in this chapter are the sexes, heights and weights, of
10,000 people. The raw file is a &lt;span class="caps"&gt;CSV&lt;/span&gt; that I import using &lt;code&gt;read_table&lt;/code&gt; in
Pandas:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;read_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/01_heights_weights_genders.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Inspecting the data with &lt;code&gt;head&lt;/code&gt;,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;gives us:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; &lt;span class="n"&gt;Gender&lt;/span&gt;    &lt;span class="n"&gt;Height&lt;/span&gt;     &lt;span class="n"&gt;Weight&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;73.847017&lt;/span&gt; &lt;span class="mf"&gt;241.893563&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;68.781904&lt;/span&gt; &lt;span class="mf"&gt;162.310473&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;74.110105&lt;/span&gt; &lt;span class="mf"&gt;212.740856&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;71.730978&lt;/span&gt; &lt;span class="mf"&gt;220.042470&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;69.881796&lt;/span&gt; &lt;span class="mf"&gt;206.349801&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;67.253016&lt;/span&gt; &lt;span class="mf"&gt;152.212156&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;68.785081&lt;/span&gt; &lt;span class="mf"&gt;183.927889&lt;/span&gt;
&lt;span class="mi"&gt;7&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;68.348516&lt;/span&gt; &lt;span class="mf"&gt;167.971110&lt;/span&gt;
&lt;span class="mi"&gt;8&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;67.018950&lt;/span&gt; &lt;span class="mf"&gt;175.929440&lt;/span&gt;
&lt;span class="mi"&gt;9&lt;/span&gt;  &lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mf"&gt;63.456494&lt;/span&gt; &lt;span class="mf"&gt;156.399676&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So it looks like heights are in inches, and weights are in pounds. It
also looks like the dataset is evenly split between men and women, since&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Gender&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Gender&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;results in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Gender&lt;/span&gt;
&lt;span class="n"&gt;Female&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;span class="n"&gt;Male&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The data are simple, clean, and appear to have imported correctly. So,
we can start looking at some simple summaries.&lt;/p&gt;
&lt;h2&gt;Numeric summaries, especially quantiles&lt;/h2&gt;
&lt;p&gt;The first part of Chapter 2 covers the basic summary statistics: means,
medians, variances, and quantiles. The authors hand-roll the mean,
median, and variance functions to see how each is calculated. All of
these methods are available as methods to Pandas series, or as NumPy
functions (which are typically what&amp;#8217;s called by equivalent Pandas
methods).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;describe&lt;/code&gt; method of Pandas series and data frames, which we saw in
&lt;a href="../ml4h-ch1-p3.html"&gt;Part 3 of Chapter 1&lt;/a&gt;, gives summary statistics. The summary stats for
the height variable are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;heights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Height&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="mf"&gt;10000.000000&lt;/span&gt;
&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="mf"&gt;66.367560&lt;/span&gt;
&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="mf"&gt;3.847528&lt;/span&gt;
&lt;span class="nb"&gt;min&lt;/span&gt; &lt;span class="mf"&gt;54.263133&lt;/span&gt;
&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;63.505620&lt;/span&gt;
&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;66.318070&lt;/span&gt;
&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;69.174262&lt;/span&gt;
&lt;span class="nb"&gt;max&lt;/span&gt; &lt;span class="mf"&gt;78.998742&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The heights all lay within a reasonable range, with no apparent outliers
from bad data. The default quantile range in &lt;code&gt;describe&lt;/code&gt; is 50%, so we
get the 75th and 25th percentiles. This can be changed with the
&lt;code&gt;percentile_width&lt;/code&gt; argument; for example, &lt;code&gt;percentile_width = 90&lt;/code&gt; would
give the 95th and 5th percentiles.&lt;/p&gt;
&lt;p&gt;There doesn&amp;#8217;t seem to be a direct analog to R&amp;#8217;s &lt;code&gt;range&lt;/code&gt; function, which
calculates the difference between the maximum and minimum value of a
vector, nor for the &lt;code&gt;quantile&lt;/code&gt;, which can calculate the quantiles at any
given a series of probabilities. These are easy enough to replicate
though.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note: &lt;/strong&gt;Nathaniel Smith, in comments, points out that R&amp;#8217;s &lt;code&gt;range&lt;/code&gt;
function doesn&amp;#8217;t do this either, but just returns the min and max of a
vector. There &lt;em&gt;is&lt;/em&gt; a function for this in NumPy, though: the
&lt;code&gt;my_range&lt;/code&gt; function below gives the same result as would
&lt;code&gt;np.ptp(heights.values)&lt;/code&gt;. &lt;code&gt;ptp&lt;/code&gt; is the &amp;#8220;peak-to-peak&amp;#8221; (min-to-max)
function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Range is trivial:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;my_range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;Difference between the max and min of an array or Series&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Calling this, we get a range of 78.99 &amp;minus; 54.26 = 24.63 inches.&lt;/p&gt;
&lt;p&gt;Next, a &lt;code&gt;quantiles&lt;/code&gt; function to mimic R&amp;#8217;s. We can just make a wrapper
around the &lt;code&gt;quantile&lt;/code&gt; method, mapping it along a sequence of provided
probabilities.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;my_quantiles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;Calculate quantiles of a series.&lt;/span&gt;

&lt;span class="sd"&gt;Parameters:&lt;/span&gt;
&lt;span class="sd"&gt;-----------&lt;/span&gt;
&lt;span class="sd"&gt;s : a pandas Series&lt;/span&gt;
&lt;span class="sd"&gt;prob : a tuple (or other iterable) of probabilities at&lt;/span&gt;
&lt;span class="sd"&gt;which to compute quantiles. Must be an iterable,&lt;/span&gt;
&lt;span class="sd"&gt;even for a single probability (e.g. prob = (0.50,)&lt;/span&gt;
&lt;span class="sd"&gt;not prob = 0.50).&lt;/span&gt;

&lt;span class="sd"&gt;Returns:&lt;/span&gt;
&lt;span class="sd"&gt;--------&lt;/span&gt;
&lt;span class="sd"&gt;A pandas series with the probabilities as an index.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;q&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quantile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the default argument gives quartiles. We can get deciles by
calling:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;my_quantiles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which spits out:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt; &lt;span class="mf"&gt;54.263133&lt;/span&gt;
&lt;span class="mf"&gt;0.1&lt;/span&gt; &lt;span class="mf"&gt;61.412701&lt;/span&gt;
&lt;span class="mf"&gt;0.2&lt;/span&gt; &lt;span class="mf"&gt;62.859007&lt;/span&gt;
&lt;span class="mf"&gt;0.3&lt;/span&gt; &lt;span class="mf"&gt;64.072407&lt;/span&gt;
&lt;span class="mf"&gt;0.4&lt;/span&gt; &lt;span class="mf"&gt;65.194221&lt;/span&gt;
&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="mf"&gt;66.318070&lt;/span&gt;
&lt;span class="mf"&gt;0.6&lt;/span&gt; &lt;span class="mf"&gt;67.435374&lt;/span&gt;
&lt;span class="mf"&gt;0.7&lt;/span&gt; &lt;span class="mf"&gt;68.558072&lt;/span&gt;
&lt;span class="mf"&gt;0.8&lt;/span&gt; &lt;span class="mf"&gt;69.811620&lt;/span&gt;
&lt;span class="mf"&gt;0.9&lt;/span&gt; &lt;span class="mf"&gt;71.472149&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="mf"&gt;78.998742&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the &lt;code&gt;quantiles&lt;/code&gt; function I&amp;#8217;ve written is a little awkward
when dealing with a single quantile. Because the list comprehension
that computes the qunatiles requires that the &lt;code&gt;prob&lt;/code&gt; argument be an
iterable, you would have to pass a list, tuple, array or other
iterable with a single value. You can&amp;#8217;t just pass it a float. I&amp;#8217;ve hit
this issue a few times writing Python functions–where it&amp;#8217;s difficult
to make code robust to both iterable and singleton arguments. If
anyone has tips on this (should I really be doing type checking?), I&amp;#8217;d
be thrilled to hear them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Histograms&lt;/h2&gt;
&lt;p&gt;Next the authors mess around with histograms and density plots to
explore the distribution of the data. Noting that different bin sizes
for histograms can affect how we perceive the data&amp;#8217;s distribution, they
plot histograms for a few different bin widths.&lt;/p&gt;
&lt;p&gt;In Matplotlib, bins are not specified by their width, as is possible
ggplot. We can either give Matplotlib the number of bins we want it to
plot, or specify the actual bin-edge locations. It&amp;#8217;s not difficult to
translate a desired bin width into either one of these types of
argument. I&amp;#8217;ll provide the sequence of bins.&lt;/p&gt;
&lt;p&gt;First, 1-inch bins:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;bins1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bins1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/height_hist_bins1.png"&gt;
  &lt;img src= "../images/height_hist_bins1.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note how I&amp;#8217;m using the Pandas &lt;code&gt;hist&lt;/code&gt; method, which, using a &lt;code&gt;**kwargs&lt;/code&gt;
argument, can pass parameters to the Matplotlib plotting functions.
Next, 5-inch bins:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;bins5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bins5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/height_hist_bins5.png"&gt;
  &lt;img src= "../images/height_hist_bins5.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And finally, 0.001-inch bins:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;bins001&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mf"&gt;.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bins001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;steelblue&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;height_hist_bins001&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;png&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/height_hist_bins001.png"&gt;
  &lt;img src= "../images/height_hist_bins001.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These all match the figures in the book, so I&amp;#8217;m probably doing it right.&lt;/p&gt;
&lt;h2&gt;Kernel density estimators in SciPy and statsmodels&lt;/h2&gt;
&lt;p&gt;R&amp;#8217;s &lt;code&gt;density&lt;/code&gt; function computes kernel density estimates. The default
kernel is Gaussian, but you can also use Epanechnikov, rectangular,
triangular, biweight, cosine kernels.&lt;/p&gt;
&lt;p&gt;In Python, it looks like you have two options for kernel density. The
first is &lt;code&gt;gaussian_kde&lt;/code&gt; from the &lt;code&gt;scipy.stats.kde&lt;/code&gt; module. This provides
a Gaussian kernel density estimate only. The other is &lt;code&gt;kdensity&lt;/code&gt; in the
&lt;code&gt;statsmodels.nonparametric.kde&lt;/code&gt; module, which provides alternative
kernels similar to R.&lt;/p&gt;
&lt;p&gt;I actually wasn&amp;#8217;t aware of the &lt;code&gt;kdensity&lt;/code&gt; function for a while, until I
stumbled upon a mention of it on a mailing list archive. I couldn&amp;#8217;t find
it in the statsmodels &lt;a href="http://statsmodels.sourceforge.net/"&gt;documentation&lt;/a&gt;. Statsmodels, generally, seems
to have a lot of undocumented functionality; not surprising for a young,
rapidly-expanding project.&lt;/p&gt;
&lt;p&gt;Playing with both functions, I found some pros and cons for each.
Obviously &lt;code&gt;kdensity&lt;/code&gt; provides an option of kernels, whereas
&lt;code&gt;gaussian_kde&lt;/code&gt; does not. &lt;code&gt;kdensity&lt;/code&gt; also generates simpler output than
&lt;code&gt;gaussian_kde&lt;/code&gt;. &lt;code&gt;kdensity&lt;/code&gt; provides a tuple of two arrays–the grid of
points at which the density was estimated, and the estimated density of
those points. &lt;code&gt;gaussian_kde&lt;/code&gt; provides an object that you have to
evaluate on a set of points to get an array of estimated densities. So
essentially, you&amp;#8217;re calling it twice, and I don&amp;#8217;t see much point to that
redundancy.&lt;/p&gt;
&lt;p&gt;On the other hand &lt;code&gt;kdensity&lt;/code&gt; gets &lt;em&gt;much&lt;/em&gt; slower than &lt;code&gt;gaussian_kde&lt;/code&gt; as
the number of points increases. For the 10,000 points in the = &lt;code&gt;heights&lt;/code&gt;
array, &lt;code&gt;gaussian_kde&lt;/code&gt; took about 3.3 seconds to output the array of
estimated densities. &lt;code&gt;kdensity&lt;/code&gt; wasn&amp;#8217;t finished after several minutes. I
haven&amp;#8217;t looked carefully at the source code of the two functions, but I
assume &lt;code&gt;kdensity&lt;/code&gt;&amp;#8216;s problem is that at some point it creates a temporary
&lt;code&gt;NxN&lt;/code&gt; array, which for &lt;code&gt;N = 10,000&lt;/code&gt; is going to gum things up. Setting
the &lt;code&gt;gridsize&lt;/code&gt; argument in &lt;code&gt;kdensity&lt;/code&gt; to something even as large as
&lt;code&gt;5000&lt;/code&gt;, cuts the size of the temporary array in half, and reduces the
running time to about 3 seconds.&lt;/p&gt;
&lt;p&gt;This is probably worth exploring in a future post. In the meantime, I&amp;#8217;m
going stick with &lt;code&gt;gaussian_kde&lt;/code&gt; and plot some densities.
&lt;strong&gt;Note:&lt;/strong&gt; See the update below. I&amp;#8217;ve
updated the &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH2" title="Chapter 2 github repo"&gt;IPython notebook&lt;/a&gt; for this chapter to use Statsmodels&amp;#8217;
&lt;span class="caps"&gt;KDE&lt;/span&gt; class instead of SciPy.]&lt;/p&gt;
&lt;p&gt;First, heights:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;density&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gaussian_kde&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="n"&gt;density&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/heights_density.png"&gt;
  &lt;img src= "../images/heights_density.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The sorting of the &lt;code&gt;heights&lt;/code&gt; array is to make the lines connect nicely.
Otherwise, the lines will connect from point-to-point in the order they
occur in the array; we want the density curve to connect points
left-to-right.&lt;/p&gt;
&lt;p&gt;Notice the slight bi-modality in the figure. What we&amp;#8217;re likely seeing is
a mixture of male and female distributions. We can plot those
separately.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Pull out male and female heights as arrays over which to compute densities&lt;/span&gt;
&lt;span class="n"&gt;heights_m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;heights_f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;density_m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gaussian_kde&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;density_f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gaussian_kde&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_m&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;density_m&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_m&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_f&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;density_f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights_f&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/height_density_bysex.png"&gt;
  &lt;img src= "../images/height_density_bysex.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We also have a weight variable we can plot.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;weights_m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;weights_f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;heights_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Gender&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Weight&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="n"&gt;density_m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gaussian_kde&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;density_f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gaussian_kde&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_m&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;density_m&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_m&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Male&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_f&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;density_f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_f&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Female&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/weight_density_bysex.png"&gt;
  &lt;img src= "../images/weight_density_bysex.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To finish up, let&amp;#8217;s move each density plot to its own subplot, to match
Figure 2-11 on page 51.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ncols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sharex&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hspace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_f&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;density_f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_f&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Female&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;xaxis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tick_top&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_m&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;density_m&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights_m&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Male&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="../images/weight_density_bysex_sublot.png"&gt;
  &lt;img src= "../images/weight_density_bysex_subplot.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here I&amp;#8217;m using the &lt;code&gt;subplots&lt;/code&gt; function, same as in &lt;a href="../ml4h-ch1-p5.html"&gt;Part 5 of Chapter
1&lt;/a&gt;, and sharing the x-axis to make clear the difference between the
distributions&amp;#8217; central tendencies.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ll wrap up Chapter 2 in the next post, where I&amp;#8217;ll look at lowess
smoothing in Statsmodels, and get a little taste of logistic regression.&lt;/p&gt;
&lt;h2&gt;Update!&lt;/h2&gt;
&lt;p&gt;Statsmodels honcho skipper seabold sets me straight in the comments.
While the &lt;code&gt;kdensity&lt;/code&gt; function is slow, statsmodels has an implementation
which uses Fast Fourier Transforms for Gaussian kernels and is
substantially faster than Scipy&amp;#8217;s &lt;code&gt;gaussian_kde&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the heights array:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Create a &lt;span class="caps"&gt;KDE&lt;/span&gt; object&lt;/span&gt;
&lt;span class="n"&gt;heights_kde&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nonparametric&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;KDE&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;heights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Estimate the density by fitting the object (default Gaussian kernel via &lt;span class="caps"&gt;FFT&lt;/span&gt;)&lt;/span&gt;
&lt;span class="n"&gt;heights_kde&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can then plot this vector of estimated densities,
&lt;code&gt;heights_kde.density&lt;/code&gt; against the points in &lt;code&gt;heights_kde.support&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve updated the &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH2" title="Chapter 2 github repo"&gt;IPython notebook&lt;/a&gt; for this chapter to use
Statsmodels&amp;#8217; &lt;span class="caps"&gt;KDE&lt;/span&gt; throughout, so check it out for more detail.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 1, Part 5: Trellis graphs.</title><link href="http://slendermeans.org/ml4h-ch1-p5.html" rel="alternate"></link><updated>2012-04-27T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-04-27:ml4h-ch1-p5.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post will wrap up Chapter 1 of &lt;span class="caps"&gt;MLFH&lt;/span&gt;. The only task left is to
replicate the authors&amp;#8217; trellis graph on p. 26. The plot is made up of 50
panels, one for each &lt;span class="caps"&gt;U.S.&lt;/span&gt; state, with each panel plotting the number of
&lt;span class="caps"&gt;UFO&lt;/span&gt; sightings by month in that state.&lt;/p&gt;
&lt;p&gt;The key takeaways from this part are, unfortunately, a bunch of gripes
about Matplotlib. Since I can&amp;#8217;t transmit, blogospherically, the migraine
I got over the two afternoons I spent wrestling with this graph, let me
just try to succinctly list my grievances.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Out-of-the-box, Matplotlib graphs are uglier than those produced by
    either lattice or ggplot in R: The default color cycle is made up of
    dark primary colors. Tick marks and labels are poorly placed in
    anything but the simplest graphs. Non-data graph elements, like
    bounding boxes and gridlines, are too prominent and take focus away
    from the data elements.&lt;/li&gt;
&lt;li&gt;The &lt;span class="caps"&gt;API&lt;/span&gt; is deeply confusing and difficult to remember. You have
    various objects that live in various containers. To make adjustments
    to graphs, you have to remember what container the thing you want to
    adjust lives in, remember what the object and its property is
    called, and then remember how Matplotlib&amp;#8217;s &lt;em&gt;getting&lt;/em&gt; and &lt;em&gt;setting&lt;/em&gt;
    procedures work.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;pyplot&lt;/code&gt; set of commands is supposed to provide convenience
    functions, but these abstractions seem to leak early and often. Once
    you need to make finer adjustments, you&amp;#8217;re back to the underlying
    &lt;span class="caps"&gt;API&lt;/span&gt; nightmare.&lt;/li&gt;
&lt;li&gt;The documentation is both clear and comprehensive. But where it is
    clear, it is not comprehensive, and where it is comprehensive, it is
    not clear. For example, the &lt;a href="http://matplotlib.sourceforge.net/users/artists.html"&gt;Artist tutorial&lt;/a&gt; is a pretty clear
    big picture of Matplotlib&amp;#8217;s &lt;span class="caps"&gt;API&lt;/span&gt;. Once you need any detail, though,
    you&amp;#8217;re dealing with &lt;a href="http://matplotlib.sourceforge.net/api/artist_api.html#module-matplotlib.lines"&gt;this&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Creating trellis graphs requires way more manual work than in either
    lattice or ggplot. The &lt;code&gt;supblot&lt;/code&gt; functionality of Matplotlib is
    highly flexible, but in most cases, the user is going to want the
    code to do the thinking for them and not manually place every graph
    (or do a bunch of bookkeeping with loops).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that off my chest, let me say that I have a ton of respect for
Matplotlib&amp;#8217;s developers. It is a massively complex library, and clearly
very powerful and flexible. I have no doubt that Matplotlib gurus can do
amazing things. I&amp;#8217;m just trying to convey the non-guru&amp;#8217;s perspective.
Graphing libraries are difficult to design because they must be
incredibly flexible and allow users to manipulate all of the myriad
parts of the graph, but at the same time, they can&amp;#8217;t overwhelm users
with detail when the flexibility isn&amp;#8217;t needed. How anyone does
it–especially in an open-source project–I don&amp;#8217;t know.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s also possible that I&amp;#8217;m just &lt;em&gt;Doing it Wrong&lt;/em&gt;, and in fact there are
easy ways to do all the things I&amp;#8217;ve complained about. If that&amp;#8217;s the
case, I hope someone reading this will enlighten me.&lt;/p&gt;
&lt;h2&gt;Trellis graphs in R and Matplotlib&lt;/h2&gt;
&lt;p&gt;In my opinion, trellis graphs are the &amp;#8220;killer app&amp;#8221; of multivariate data
visualization. I produce trellis line and scatter plots more than almost
any other kind of visualization. As such, it&amp;#8217;s important for me to be
able to easily produce quality trellis graphs.&lt;/p&gt;
&lt;p&gt;Trellis graphs are easy to create in R. The two most popular high-level
graphing packages in R, lattice and ggplot, both have simple methods for
creating them. Indeed, creating trellis graphs is lattice&amp;#8217;s &lt;em&gt;raison
d&amp;#8217;etre&lt;/em&gt;, and the functionality and interface design in the package
revolves around dealing with trellis graph and the panels within. In
ggplot, the trellis is not such a central focus, but it still has
easy-to-use methods for making and modifying trellis graphs (which it
refers to as &amp;#8220;faceted&amp;#8221; graphs).&lt;/p&gt;
&lt;p&gt;For example, the graph we want to make is a one liner in lattice:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;xyplot&lt;span class="p"&gt;(&lt;/span&gt;sightings &lt;span class="o"&gt;~&lt;/span&gt; year_month &lt;span class="o"&gt;|&lt;/span&gt; us_state&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; sightings_counts&lt;span class="p"&gt;,&lt;/span&gt;
type &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;l&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; layout &lt;span class="o"&gt;=&lt;/span&gt; c&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once you get the hang of R&amp;#8217;s formula expressions–which doesn&amp;#8217;t take
long–this is an easy, expressive way to create a trellis graph. The
authors use ggplot, which I find a bit less natural, but is still very
easy.&lt;/p&gt;
&lt;p&gt;Part of what makes trellis graphs to straightforward in R is that the
concept of factors, and their use as conditioning variables, is so
well-baked into the language. Matplotlib is essentially a plotting
utility for NumPy, so it&amp;#8217;s designed to plot arrays, not rich data
structures. Without factors, without a notion of conditioning, and to a
lesser extent, without formulas, trellis graphs just don&amp;#8217;t come
naturally.&lt;/p&gt;
&lt;p&gt;Pandas, though, has structures that, if a plotting library was designed
to understand them, might provide for easy trellis-ing. Even though
Pandas doesn&amp;#8217;t have factors, I could see, for example, a &lt;code&gt;plot&lt;/code&gt; method
for Pandas&amp;#8217; &lt;code&gt;groupby&lt;/code&gt; objects that produces trellis graphs by default.&lt;/p&gt;
&lt;h2&gt;Plotting the &lt;span class="caps"&gt;UFO&lt;/span&gt; trellis graph&lt;/h2&gt;
&lt;p&gt;With all that throat-clearing out of the way, let&amp;#8217;s get down to plotting
the graph. The authors plot 50 state panels, with a 10-by-5 layout.
Since I&amp;#8217;ve included &lt;span class="caps"&gt;D.C.&lt;/span&gt; in my data, I have to plot 51 panels. You can
fit this in a 17-by-3 layout, but that&amp;#8217;s pretty awkward. I&amp;#8217;d like to
have 4 columns instead, but to fit 51 graphs, I&amp;#8217;ll need 13 columns.
That&amp;#8217;s 52 subplots, meaning the 13th row won&amp;#8217;t have graphs in every
column, only the first three. I&amp;#8217;m going to call these last three graphs
the &lt;code&gt;hangover&lt;/code&gt; graphs, and I&amp;#8217;m going to define it as its own variable to
help inform the layout procedures I run later.&lt;/p&gt;
&lt;p&gt;Here are the layout parameters, then:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;nrow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;ncol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;hangover&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;us_states&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;ncol&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now let me get the &amp;#8220;framing&amp;#8221; objects in place: the figure, the subplot
layout, and the titles.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ncol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sharey&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;figsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Monthly &lt;span class="caps"&gt;UFO&lt;/span&gt; Sightings by &lt;span class="caps"&gt;U.S.&lt;/span&gt; State&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;January 1990 through August 2010&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wspace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hspace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;subplots&lt;/code&gt; function is some recently-implement syntactic sugar
around Matplotlib&amp;#8217;s &lt;code&gt;subplot&lt;/code&gt; functionality (see the section on &amp;#8220;Easy
Pythonic Subplots&amp;#8221; &lt;a href="http://matplotlib.sourceforge.net/users/whats_new.html#easy-pythonic-subplots"&gt;here&lt;/a&gt;). The &lt;code&gt;sharey&lt;/code&gt; argument tells Matplotlib
that the panels should all share the same y axis. Technically I want it
to share an x axis too, but Matplotlib kept throwing errors when I tried
to use the &lt;code&gt;sharex&lt;/code&gt; argument with dates on the x-axis. Give the data,
the panels will end up sharing an x axis anyway, so this argument isn&amp;#8217;t
necessary. The function returns two objects: &lt;code&gt;fig&lt;/code&gt; refers to the overall
figure container, and &lt;code&gt;axes&lt;/code&gt; is an array containing each of the
subplot/panel objects – so &lt;code&gt;axes[0, 0]&lt;/code&gt; is the first panel.&lt;/p&gt;
&lt;p&gt;Now the rest of the code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;num_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ncol&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;linestyle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num_state&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;51&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;st&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;us_states&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_state&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;sightings_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;st&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;st&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;upper&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transAxes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;verticalalignment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;top&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;num_state&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c"&gt;# Make extra subplots invisible&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visible&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;xtl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;ytl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_yticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c"&gt;# X-axis tick labels:&lt;/span&gt;
&lt;span class="c"&gt;# Turn off tick labels for all the the bottom-most&lt;/span&gt;
&lt;span class="c"&gt;# subplots. This includes the plots on the last row, and&lt;/span&gt;
&lt;span class="c"&gt;# if the last row doesn&amp;#39;t have a subplot in every column&lt;/span&gt;
&lt;span class="c"&gt;# put tick labels on the next row up for those last&lt;/span&gt;
&lt;span class="c"&gt;# columns.&lt;/span&gt;
&lt;span class="c"&gt;#&lt;/span&gt;
&lt;span class="c"&gt;# Y-axis tick labels:&lt;/span&gt;
&lt;span class="c"&gt;# Put left-axis labels on the first column of subplots,&lt;/span&gt;
&lt;span class="c"&gt;# odd rows. Put right-axis labels on the last column&lt;/span&gt;
&lt;span class="c"&gt;# of subplots, even rows.&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;nrow&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;nrow&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hangover&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;hangover&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xtl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visible&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ytl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visible&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;ncol&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yaxis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tick_right&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xtl&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;90.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s walk through this:&lt;/p&gt;
&lt;p&gt;First, set up a counter to keep track of what state we&amp;#8217;re plotting. This
is a little un-Pythonic, but given what I do inside the loop, I couldn&amp;#8217;t
think of a better way.&lt;/p&gt;
&lt;p&gt;Now, for each row, column in the 13-by-4 array of panels (and this code
works for any row/column combination, as long as rows &amp;times; columns &amp;gt;=
51):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Assign the panel (&amp;#8220;axis&amp;#8221;) associated with this row, column pair to
    its own variable.&lt;/li&gt;
&lt;li&gt;Draw gray gridlines in the panel.&lt;/li&gt;
&lt;li&gt;Go to the state in the &lt;code&gt;us_state&lt;/code&gt; list corresponding to the current
    value of the state counter.&lt;/li&gt;
&lt;li&gt;Select this state out of the &lt;code&gt;sightings_counts&lt;/code&gt; series and plot its
    data in the current panel. Then, put a text label with the state&amp;#8217;s
    initials in the upper left corner.&lt;/li&gt;
&lt;li&gt;If I&amp;#8217;ve gone through all the states, and the state counter variable
    is greater than 51, then make the panel invisible.&lt;/li&gt;
&lt;li&gt;Assign the x- and y-axis &lt;code&gt;ticklabel&lt;/code&gt; objects for the current panel
    to variables. We&amp;#8217;re going to manipulate their attributes.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now some tricky stuff. I want do the following things to the tick
    labels:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I want to turn off the x-axis tick labels for all but the
    bottom-most panels, taking into account the hangover.&lt;/li&gt;
&lt;li&gt;I want to alternate the y-axis tick labels so that they are on
    the left for odd-numbered rows, and on the right for
    even-numbered rows. Having labels on both sides makes the graph
    easier to read, but having them on the same side on every row
    leads to overcrowding and overlapping.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, I want the x-axis tick labels rotated 90 degrees. This
    gives space to put as many as possible on the graph without
    overcrowding (here, we can label every two years).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&amp;#8217;s the result:&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/ufo_ts_bystate.png"&gt;
    &lt;img src="../images/ufo_ts_bystate.png" width=500px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Not bad, I think. And maybe even better than the out-of-the-box version
you get with ggplot. But it was a tremendous amount of work, and I don&amp;#8217;t
know if I&amp;#8217;m going to be able to decipher this code six months from now.
It&amp;#8217;s just a tremendous amount of bookkeeping I have to do keeping track
of what panel I&amp;#8217;m in and where it&amp;#8217;s located in the layout. There ought
to be a function that does this for me.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So that&amp;#8217;s it for Chapter 1 of &lt;span class="caps"&gt;MLFH&lt;/span&gt;. Overall, I was pleasantly surprised
by Pandas and how easy it made loading, cleaning, and manipulating data.
While there are a couple of things from R that I missed, there were
several other things I though were easier and more flexible with Pandas.&lt;/p&gt;
&lt;p&gt;On the other hand, going from lattice and ggplot to Matplotlib is like
taking a time machine back to the early &amp;#8216;90s. After reading the
documentation and experimenting for several days, I still don&amp;#8217;t think
I&amp;#8217;m sure how it works. Hopefully I&amp;#8217;ll get the hang of it as I go
forward.&lt;/p&gt;
&lt;p&gt;My take is the Python data analysis community is aware of its
&amp;#8220;visualization gap&amp;#8221; vis-a-vis R, and there are tools in the works to
solve this issue. I&amp;#8217;ve heard whispers about &amp;#8220;ggplot for Python&amp;#8221; or &amp;#8220;D3
for Python.&amp;#8221; Everything is still in the early stages, and it will
probably be a while before better tools are available.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m also a little uncertain about the &amp;#8220;x for Python&amp;#8221; notion of creating
graphing libraries. Matplotlib&amp;#8217;s &lt;code&gt;pyplot&lt;/code&gt; is essentially a &amp;#8220;Matlab for
Python&amp;#8221; approach to graphics, and I don&amp;#8217;t know that works to its credit.
I&amp;#8217;d much rather have a solid, Pythonic graphing library that lets me
easily make publication-quality versions of the workhorse data graphics,
than have something that apes the latest faddish graphing tool. There
are a lot of smart people working on the problem, though, and I&amp;#8217;m really
excited to see what happens.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers, Chapter 1, Part 4: Data aggregation and reshaping.</title><link href="http://slendermeans.org/ml4h-ch1-p4.html" rel="alternate"></link><updated>2012-04-26T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-04-26:ml4h-ch1-p4.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the &lt;a href="../ml4h-ch1-p3.html"&gt;last part&lt;/a&gt; I made some simple summaries of the cleaned &lt;span class="caps"&gt;UFO&lt;/span&gt;
data: basic descriptive statistics and historgrams. At the very end, I
did some simple data aggregation by summing up the sightings by date,
and plotted the resulting time series. In this part, I&amp;#8217;ll go further
with the aggregation, totalling sightings by state and month.&lt;/p&gt;
&lt;p&gt;This takeaway from this part is that Pandas dataframes have some
powerful methods for aggregating and manipulating data. I&amp;#8217;ll show
&lt;code&gt;groupby&lt;/code&gt;, &lt;code&gt;reindex&lt;/code&gt;, hierarchical indices, and &lt;code&gt;stack&lt;/code&gt; and &lt;code&gt;unstack&lt;/code&gt; in
action.&lt;/p&gt;
&lt;h2&gt;The shape of data: the long and the wide of it&lt;/h2&gt;
&lt;p&gt;The first step in aggregating and reshaping data is to figure out the
final form you want the data to be in. This form is basically defined by
&lt;em&gt;content&lt;/em&gt; and &lt;em&gt;shape&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We know what we want the content to be: an entry in the data should give
the number of sightings in a state/month combination.&lt;/p&gt;
&lt;p&gt;We have two choices for the shape: wide or long. The wide version of
this data would have months as the rows and states as the columns; it
would be a 248 by 51 table with the number of sigthings as entries. This
is a really natural way to shape the data if we were presenting a table
for example.&lt;/p&gt;
&lt;p&gt;One of things I&amp;#8217;ve picked up from my years of using R, though, is a
preference for long data. This is because R&amp;#8217;s &lt;code&gt;factors&lt;/code&gt; and &lt;code&gt;formulas&lt;/code&gt;
with easy conditioning make it easier to work with long data. The most
common example is using &lt;code&gt;lattice&lt;/code&gt; plots. To generate a lattice plot of
&lt;code&gt;y&lt;/code&gt; over &lt;code&gt;x&lt;/code&gt; with panels defined by a level of the variable &lt;code&gt;f&lt;/code&gt;, you
just call &lt;code&gt;xyplot(y ~ x | f)&lt;/code&gt;. For this to work though, the data must be
long, with &lt;code&gt;f&lt;/code&gt; a column of factors, and the &lt;code&gt;x&lt;/code&gt; column will likely be
some values repeated for each level of &lt;code&gt;f&lt;/code&gt;. This seems kind of redundant
and unwieldy when you&amp;#8217;re used to tables and spreadsheets, but it becomes
more natural when you starting working with tools like &lt;code&gt;lattice&lt;/code&gt; or
&lt;code&gt;ggplot&lt;/code&gt;, using more panel data, or doing more &lt;a href="http://vita.had.co.nz/papers/plyr.html"&gt;&lt;em&gt;split-apply-combine&lt;/em&gt;&lt;/a&gt;
or &lt;em&gt;map-reduce&lt;/em&gt; types of procedures.&lt;/p&gt;
&lt;p&gt;Because Pandas dataframes are so organized around indices, and because
Pandas allows for hierarchical indexing, we&amp;#8217;ll find that it will be a
good strategy to shape data in a way that provides for informative
indices. This will give us access to a host of powerful methods to
manipulate the dataframe. In this case, as we&amp;#8217;ll see, by making the data
long, we&amp;#8217;ll be able to push most of the information into the dataframe&amp;#8217;s
index.&lt;/p&gt;
&lt;p&gt;The long version of our &lt;span class="caps"&gt;UFO&lt;/span&gt; data would have rows defined by a
state/month pair, and a column recording the number of sightings for
that pair. In R–as the authors do in the book–you&amp;#8217;ll have a dataframe
with three columns. The first two are the &lt;em&gt;factor&lt;/em&gt; variables &lt;code&gt;USState&lt;/code&gt;
and &lt;code&gt;YearMonth&lt;/code&gt;. (I&amp;#8217;m not actually sure these are technically factor
variables in the authors&amp;#8217; implementation, but they are conceptually).
The third is the sightings count.&lt;/p&gt;
&lt;p&gt;In Pandas, since the state and month pairs identify unique observations,
it&amp;#8217;s natural to make these indices of the dataframe. Pandas supports
hierarchical indexing by using unique tuples–here a tuple would be
&lt;em&gt;(state, month)&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Aggregating the data&lt;/h2&gt;
&lt;p&gt;Now that we&amp;#8217;ve decided the form of the data, let&amp;#8217;s implement all this.&lt;/p&gt;
&lt;p&gt;The first step is to create a year-month variable. I do this just by
taking the date of each sighting, and calculating a new date with the
same year and month, but set to the first of the month. This is just
another &lt;code&gt;map&lt;/code&gt; operation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;year_month&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;date_occurred&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The authors approach this problem a little differently,
using R&amp;#8217;s &lt;code&gt;strftime&lt;/code&gt; function to turn the dates into a string of the form
&lt;code&gt;YYYY-MM&lt;/code&gt;. I prefer to keep them numeric (it makes time series
 plots more sensible), but either way works. My choice of the first
day of the month is arbitrary, and just serves to collect the dates into
groups.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then we want to sum up the sightings by state and month. To do this,
I&amp;#8217;ll use Pandas &lt;code&gt;groupby&lt;/code&gt; method. &lt;code&gt;groupby&lt;/code&gt;, as you&amp;#8217;d expect, works like
&lt;span class="caps"&gt;SQL&lt;/span&gt;&amp;#8217;s &lt;code&gt;GROUP BY&lt;/code&gt; statement.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sightings_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="s"&gt;&amp;#39;year_month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;year_month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can almost read this statement as an &lt;code&gt;SQL&lt;/code&gt; query:
&lt;code&gt;SELECT COUNT(year_month) GROUP BY us_state, year_month&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;groupby&lt;/code&gt; method applied to the data frame results in a
&lt;code&gt;DataFrameGroupBy&lt;/code&gt; object, which isn&amp;#8217;t much to look at but contains all
the information we need to perform calculations by groups of the
variables we passed to the method. Calling the &lt;code&gt;year_month&lt;/code&gt; column
results in a similar &lt;code&gt;SeriesGroupBy&lt;/code&gt; object. Finally, calling the
&lt;code&gt;count&lt;/code&gt; method counts how many non-null observations of &lt;code&gt;year_month&lt;/code&gt;
there are in each level. The final output is a Series of the counts with
a hierarchal index of the groupby variables.&lt;/p&gt;
&lt;p&gt;To aggregate their data in R, the authors use the &lt;code&gt;ddply&lt;/code&gt; function,
which provides similar groupby-type functionality. I find the &lt;code&gt;plyr&lt;/code&gt;
functions less intuitive and expressive than Pandas&amp;#8217; syntax. But, the
&lt;code&gt;plyr&lt;/code&gt; functions are a big improvement over R&amp;#8217;s &lt;code&gt;apply&lt;/code&gt; functions for
complicated calculations.&lt;/p&gt;
&lt;p&gt;As the authors do on p. 22, let&amp;#8217;s check out the first few Alaska
sightings.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;First few &lt;span class="caps"&gt;AK&lt;/span&gt; sightings in data:&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;sightings_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ak&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This spits out:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;First&lt;/span&gt; &lt;span class="n"&gt;few&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AK&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;sightings&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;year_month&lt;/span&gt;
&lt;span class="mi"&gt;1990&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;1990&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;03&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;1990&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;1993&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;1994&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;02&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="mi"&gt;1994&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that I have one more observation than the authors do–February 1994.
As discussed in &lt;a href="../ml4h-ch1-p2.html"&gt;Part 2&lt;/a&gt;, the authors&amp;#8217; cleaning methodology is going
to cut any observations where the &lt;span class="caps"&gt;U.S.&lt;/span&gt; city part of the location data
has commas in it. My methodology won&amp;#8217;t lose those observations. That
seems to be what&amp;#8217;s happened here. Looking at that record with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;Extra&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AK&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;sighting&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;us_state&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;ak&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;year_month&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1994&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt; &lt;span class="err"&gt;\&lt;/span&gt;\
&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;year_month&lt;/span&gt;&lt;span class="sc"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;shows that indeed, my extra observation has a comma in the city record:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Extra&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AK&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;sighting&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;year_month&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;
&lt;span class="mi"&gt;5508&lt;/span&gt; &lt;span class="mi"&gt;1994&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;02&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mo"&gt;01&lt;/span&gt; &lt;span class="n"&gt;Savoonga&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;St&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Lawrence&lt;/span&gt; &lt;span class="n"&gt;Island&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AK&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Indexing tricks&lt;/h2&gt;
&lt;p&gt;When we perform the &lt;code&gt;groupby&lt;/code&gt; calculations, the resulting series is
missing rows where there were no &lt;span class="caps"&gt;UFO&lt;/span&gt; sightings in a state/month. This
makes sense of course – &lt;code&gt;groupby&lt;/code&gt; goes through the data, finds all the
state/month combinations, and turns them into discrete levels within
which to perform calculations. If there are no sightings in a state in a
month, &lt;code&gt;groupby&lt;/code&gt; won&amp;#8217;t know to turn that combination into a level.&lt;/p&gt;
&lt;p&gt;So, basically, we want to add those levels back into the data and set
the associated sightings count to zero. There are two ways to do this in
Pandas. The first uses Pandas&amp;#8217; &lt;code&gt;reindex&lt;/code&gt; methods. I&amp;#8217;ll create a &amp;#8220;full&amp;#8221;
index with every combination of states and months:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ym_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1990&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2011&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; \&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2010&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;full_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;us_states&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ym_list&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;ym_list&lt;/span&gt; \&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;us_states&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;full_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MultiIndex&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_tuples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;states&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;year_month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first line is just a list comprehension that creates a list of all
the months in the data, from January 1990 to August 2010. The second
line creates 51&amp;times;248 tuples of (state, month) pairs. (I created the list
of states, &lt;code&gt;us_states&lt;/code&gt;, in &lt;a href="../ml4h-ch1-p2.html"&gt;Part 2&lt;/a&gt;.) The third line creates a Pandas
hierarchical index out of these tuples. Hierarchical indices in Pandas
can take names that label the levels of the index.&lt;/p&gt;
&lt;p&gt;Next, I&amp;#8217;ll reindex the &lt;code&gt;sightings_counts&lt;/code&gt; series with this full index.
Pandas will conform the dataset to the new index we give it, dropping
elements whose index level is not in the new index, and making elements
for new index levels not in the original. By default Pandas fills in
these new elements with &lt;code&gt;NA&lt;/code&gt;, but we can tell it to fill these values
with zero, and end up with the series we&amp;#8217;re looking for.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sightings_counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sightings_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reindex&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;full_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fill_value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Stacking and unstacking data&lt;/h2&gt;
&lt;p&gt;There&amp;#8217;s another way to get the full time series out of the groupby
calculations. Instead of creating the full index of state/month
combinations, I can use a trick using Pandas &lt;code&gt;stack&lt;/code&gt; and &lt;code&gt;unstack&lt;/code&gt;
methods. &lt;code&gt;stack&lt;/code&gt; and &lt;code&gt;unstack&lt;/code&gt; turn data from wide to long and vice
versa, similar to the &lt;code&gt;melt&lt;/code&gt; and &lt;code&gt;cast&lt;/code&gt; methods in R&amp;#8217;s &lt;code&gt;reshape2&lt;/code&gt;
package.&lt;/p&gt;
&lt;p&gt;The idea is to first widen (&lt;code&gt;unstack&lt;/code&gt;) the data, so that we have states
as columns and months as rows. This will force the data to have the
248&amp;times;51 entries we&amp;#8217;re looking for (assuming that there&amp;#8217;s a sighting in
at least one state every month between January 1990 and August 2010).
For the entries in this data frame where there are no
sightings–state/months not present in the long data–Pandas will fill in
&lt;code&gt;NA&lt;/code&gt;. I&amp;#8217;ll tell Pandas to fill it with zero instead, and then &lt;code&gt;stack&lt;/code&gt;
the data again to put it back in long form. Since there is now a number
(sometimes zero) for every state/month pair, this new long dataset will
have all the rows we need. Here&amp;#8217;s the code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;sightings_counts1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;year_month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;year_month&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;sightings_counts1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sightings_counts1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s check that we get the same dataset from both methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Check they&amp;#39;re the same shape and values.&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Shape using handmade MultiIndex:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sightings_counts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Shape using unstack/stack method:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sightings_counts1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Sum absolute difference:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sightings_counts1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;
&lt;span class="n"&gt;sightings_counts&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I check the sum-of-absolute-differences between the series, instead of
 checking for strict equality, to give some leeway for floating point
error (even though these should be integers, there might be some type
conversion that happens through these methods). Either way, looks like
we have the same result from both methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Shape&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;handmade&lt;/span&gt; &lt;span class="n"&gt;MultiIndex&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12648&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="n"&gt;Shape&lt;/span&gt; &lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;unstack&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;stack&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12648&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="n"&gt;Sum&lt;/span&gt; &lt;span class="n"&gt;absolute&lt;/span&gt; &lt;span class="n"&gt;difference&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ve got the data just how I want it to plot time series of &lt;span class="caps"&gt;UFO&lt;/span&gt;
sightings by state. There were actually very few lines of code in this
part. But those few lines of code were doing a lot of work, and
represented one of the toughest parts of working with data: getting it
in the right shape. It wasn&amp;#8217;t long ago that reshaping data was always
and everywhere a huge hassle. It still is in some languages (&lt;em&gt;*cough*
&lt;span class="caps"&gt;SAS&lt;/span&gt; *cough* Stata *cough*&lt;/em&gt;). The combination of hierarchical
indexing and &lt;code&gt;stack&lt;/code&gt; and &lt;code&gt;unstack&lt;/code&gt; methods in Pandas make doing this in
Python actually pleasant.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m finally going to wrap up Chapter 1 in the next part, in which I
create a plot to match the authors&amp;#8217; trellis plot of sightings time
series by state. It&amp;#8217;s going to be a real Matplotlib adventure.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Shades of Time: I don’t buy it, and that’s why it’s so great.</title><link href="http://slendermeans.org/shades-of-time.html" rel="alternate"></link><updated>2012-04-22T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-04-22:shades-of-time.html</id><summary type="html">&lt;p&gt;Over the weekend &lt;a href="http://drewconway.com/zia"&gt;Drew Conway&lt;/a&gt; posted about a data analysis project
he&amp;#8217;d just completed called &lt;a href="http://www.drewconway.com/zia/?p=2874"&gt;&lt;em&gt;Shades of Time&lt;/em&gt;&lt;/a&gt;. Very briefly, he took a
&lt;a href="http://www.reddit.com/r/datasets/comments/s0fld/all_time_magazine_covers_march_1923_to_march_2012/"&gt;dataset&lt;/a&gt; of Time magazine covers from 1923 to March 2012, then used
some Python libraries to identify the faces in the covers and identify
the skin tone of each face. The result is a really great
interactive &lt;a href="http://labs.drewconway.com/time/"&gt;visualization&lt;/a&gt; implemented in &lt;a href="http://mbostock.github.com/d3/"&gt;d3.js&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From looking at this data, Drew, with some caveats, observes that &amp;#8220;it
does appear that the variance in skin tones have [sic] changed over
time, and in fact the tones are getting darker.&amp;#8221; He also notes that
there are more faces on covers in later years.&lt;/p&gt;
&lt;h2&gt;Why I don&amp;#8217;t believe it&lt;/h2&gt;
&lt;p&gt;There&amp;#8217;s no real statistical testing done here–no formal quantification
how skin-tone representation on covers is changing over time. Instead, I
think he&amp;#8217;s drawing his conclusion on the vizualization alone, especially
the scatterplot in the bottom panel that seems to show more darker tones
appearing later in the date (starting in the 70&amp;#8217;s, the skin-tone
dispersion in his data starts to increase).&lt;/p&gt;
&lt;p&gt;He notes that there are difficulties in both identifying faces and skin
tones. After going through his analysis, I think these algorithms are
fragile enough, and the categorization of faces and skin tones is poor
enough, that I don&amp;#8217;t really buy his conclusion that cover face diversity
is increasing.&lt;/p&gt;
&lt;p&gt;For example, I reviewed many of the data classified with a dark skin
tone that seemed to be contributing to the visual impression of
increasing diversity. A good number of them weren&amp;#8217;t faces at all, but
objects like guns, or parts of the word &amp;#8220;&lt;span class="caps"&gt;TIME&lt;/span&gt;.&amp;#8221;&lt;/p&gt;
&lt;p&gt;Many others were famous white guys. Here&amp;#8217;s a list I made from my cursory
review:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;James Taylor (1971)&lt;/li&gt;
&lt;li&gt;Archie Bunker/Carrol O&amp;#8217;Connor (1973)&lt;/li&gt;
&lt;li&gt;Joni Mitchell (1974)&lt;/li&gt;
&lt;li&gt;Gerald Ford (1974, 1975)&lt;/li&gt;
&lt;li&gt;Francisco Franco (1975)&lt;/li&gt;
&lt;li&gt;Jimmy Carter (1976)&lt;/li&gt;
&lt;li&gt;Queen Elizabeth (1976)&lt;/li&gt;
&lt;li&gt;John Irving (1981)&lt;/li&gt;
&lt;li&gt;Ronald Reagan (1985)&lt;/li&gt;
&lt;li&gt;Willem Defoe, Charlie Sheen (1987)&lt;/li&gt;
&lt;li&gt;Ollie North (1987)&lt;/li&gt;
&lt;li&gt;Dan Rather (1988)&lt;/li&gt;
&lt;li&gt;Michael Eisner (1988)&lt;/li&gt;
&lt;li&gt;Statue In Congress (1990)&lt;/li&gt;
&lt;li&gt;Garth Brooks (1992)&lt;/li&gt;
&lt;li&gt;Roger Keith Coleman (1992)&lt;/li&gt;
&lt;li&gt;Serbian Detention Camp Prisoners (1992)&lt;/li&gt;
&lt;li&gt;Michael Chrichton (1995)&lt;/li&gt;
&lt;li&gt;Bill Clinton (I know he&amp;#8217;s the first black president, but I don&amp;#8217;t think that should count) (1998)&lt;/li&gt;
&lt;li&gt;Monica Lewinsky (1998)&lt;/li&gt;
&lt;li&gt;John Travolta (1998)&lt;/li&gt;
&lt;li&gt;Slobodan Milosevic (1999)&lt;/li&gt;
&lt;li&gt;Ted Kaczynski (1999)&lt;/li&gt;
&lt;li&gt;John McCain (2000)&lt;/li&gt;
&lt;li&gt;Jerry Levin (2000)&lt;/li&gt;
&lt;li&gt;George Bush (2000)&lt;/li&gt;
&lt;li&gt;Francis Collins (2000)&lt;/li&gt;
&lt;li&gt;Yoda (2002)&lt;/li&gt;
&lt;li&gt;Trent Lott (2002)&lt;/li&gt;
&lt;li&gt;Joe Wilson (2003)&lt;/li&gt;
&lt;li&gt;Brad Pitt (2004)&lt;/li&gt;
&lt;li&gt;John Kerry (2004)&lt;/li&gt;
&lt;li&gt;George Bush (2004)&lt;/li&gt;
&lt;li&gt;Bono (2006)&lt;/li&gt;
&lt;li&gt;Bill Gates (2006)&lt;/li&gt;
&lt;li&gt;Jesus Christ (2006)&lt;/li&gt;
&lt;li&gt;John McCain (2006)&lt;/li&gt;
&lt;li&gt;Rick Warren (2008)&lt;/li&gt;
&lt;li&gt;Sarah Palin (2008)&lt;/li&gt;
&lt;li&gt;Lloyd Blankfein (2009)&lt;/li&gt;
&lt;li&gt;Tom Hanks (2010)&lt;/li&gt;
&lt;li&gt;Jonathan Franzen (2010)&lt;/li&gt;
&lt;li&gt;George Washington (2010)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, no classification algorithm is perfect, and these covers are
complicated, heterogeneous inputs. But just from eyeballing it, this one
seems so inaccurate on this data, that I don&amp;#8217;t trust that the observed
dispersion is the result of more correctly classified darker faces on
covers.&lt;/p&gt;
&lt;h2&gt;Why it&amp;#8217;s still awesome&lt;/h2&gt;
&lt;p&gt;While I don&amp;#8217;t think the classification process here is accurate enough
to let us draw inferences about skin tone diversity, the fact that I
could come to this conclusion after 30 minutes of poking around on a web
site really says some interesting things about the process and
presentation of the project.&lt;/p&gt;
&lt;p&gt;For one, I think it&amp;#8217;s a fantastic use of dynamic visualization. I don&amp;#8217;t
think any aesthetic aspect of it is novel or noteworthy, instead I think
it&amp;#8217;s innovative on a more meta level. Often times we think for
visualizations as serving one of two processes. The first is pre-model:
exploration of raw data to suggest questions, patterns, or models. The
second is post-model: presentation of results or model diagnostics.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been skeptical of d3 and similar frameworks, because I&amp;#8217;ve rarely
seen dynamic or interactive graphs that do a much better job at these
two types of tasks than static graphs. At least not so much better as to
justify the added costs of producing them and delivering them to an
audience. Also, a lot of what I&amp;#8217;ve seen that&amp;#8217;s been represented as cool
stuff you can do with d3–or Processing, or whatever–is mostly pretty
junk; stuff like busy stream graphs and chord graphs and other things
I&amp;#8217;d put in the high-effort/low-reward quadrant of Kaiser Fung&amp;#8217;s
&lt;a href="http://statisticsforum.wordpress.com/2011/07/31/one-difference-between-statistical-graphics-and-infoviz-is-the-return-on-effort/"&gt;return-on-effort matrix&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The visualization for &lt;em&gt;Shades of Time&lt;/em&gt;, though, is impressive to me
because it&amp;#8217;s not really exploring raw data, or presenting
results–instead it&amp;#8217;s illustrating the &lt;em&gt;process&lt;/em&gt; of analyzing the data.
To get the list above, I started from the time series chart at the
bottom that seemed to show increasing diversity. Then I noted the points
in that chart that I felt were most influencing that conclusion. I could
then find them in the scrolling chart on the left, click, see on the
right panel what raw data (what image on what cover) generated that
point, and determine whether the classifier was giving a meaningful
result.&lt;/p&gt;
&lt;p&gt;After going through it long enough, I decided there really wasn&amp;#8217;t enough
meaningful output coming from the classifier for me to comfortably
believe Drew&amp;#8217;s observation. Nonetheless, I think it&amp;#8217;s incredibly novel
and useful to have a visualization that lets me so easily do a
mini-replication of the analysis. This one lets you walk through the
major steps, from raw data (the covers in the right panel) to
quantification/classification (the skin tone tiles in the left panel) to
aggregation and interpretation (the time series scatter plot on the
bottom).&lt;/p&gt;
&lt;p&gt;It really makes me rethink some of the possibilities of interactive
graphics. This isn&amp;#8217;t just a &lt;a href="http://www.nytimes.com/interactive/2008/02/23/movies/20080223_REVENUE_GRAPHIC.html"&gt;stream graph of box office receipts&lt;/a&gt; or
the &lt;a href="http://www.babynamewizard.com/"&gt;Baby Name Wizard&lt;/a&gt;, which are mostly just raw data explorers. I
think it suggests a whole different application and conceptual framework
for interactive graphics. That is, how do we illustrate to an audience
the &lt;em&gt;process&lt;/em&gt; by which we went from raw data to conclusions, and let
them follow along and investigate that process?&lt;/p&gt;</summary><category term="python"></category></entry><entry><title>Machine Learning for Hackers Chapter 1, Part 3: Simple summaries and plots.</title><link href="http://slendermeans.org/ml4h-ch1-p3.html" rel="alternate"></link><updated>2012-04-19T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-04-19:ml4h-ch1-p3.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;See &lt;a href="../ml4h-ch1-p1.html"&gt;Part 1&lt;/a&gt; and &lt;a href="../ml4h-ch1-p2.html"&gt;Part 2&lt;/a&gt; for previous work.&lt;/p&gt;
&lt;p&gt;In this part, I&amp;#8217;ll replicate the authors&amp;#8217; exploration of the &lt;span class="caps"&gt;UFO&lt;/span&gt;
sighting dates via histograms. The key takeaways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The plotting methods in Pandas are easy and useful.&lt;/li&gt;
&lt;li&gt;Unlike R &lt;code&gt;Dates&lt;/code&gt;, Python &lt;code&gt;datetimes&lt;/code&gt; aren&amp;#8217;t compatible with a lot of
    mathematical operations. We&amp;#8217;ll see that you can&amp;#8217;t apply quantile or
    histogram methods to them directly.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Quick data summary methods and datetime complications.&lt;/h2&gt;
&lt;p&gt;For those playing along at home, I&amp;#8217;m at p. 19 of the book. The first
thing the authors do here is get a statistical summary of the sighting
dates in the data, which are recorded in the &lt;code&gt;DateOccurred&lt;/code&gt; variable
(which I&amp;#8217;ve named &lt;code&gt;date_occurred&lt;/code&gt; in my code). This is easy in R using
the &lt;code&gt;summary&lt;/code&gt; function, which provides the minimum, maximum, and
quartiles of the data by default.&lt;/p&gt;
&lt;p&gt;Pandas has similar functionality, in a method called &lt;code&gt;describe&lt;/code&gt;, which
gives the same for numeric variables, plus the count of non-null values
and the mean and standard deviation. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;outputs what we&amp;#8217;d expect from a series of randomly-generated standard
normals:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="mf"&gt;100.000000&lt;/span&gt;
&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.149274&lt;/span&gt;
&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="mf"&gt;1.011230&lt;/span&gt;
&lt;span class="n"&gt;min&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.521374&lt;/span&gt;
&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.790867&lt;/span&gt;
&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.167813&lt;/span&gt;
&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;0.596617&lt;/span&gt;
&lt;span class="n"&gt;max&lt;/span&gt; &lt;span class="mf"&gt;2.231157&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we apply this to the &lt;code&gt;date_occurred&lt;/code&gt; series, though, we get something
different.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;results in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="mi"&gt;52134&lt;/span&gt;
&lt;span class="n"&gt;unique&lt;/span&gt; &lt;span class="mi"&gt;8786&lt;/span&gt;
&lt;span class="n"&gt;top&lt;/span&gt; &lt;span class="mi"&gt;1999&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;
&lt;span class="n"&gt;freq&lt;/span&gt; &lt;span class="mi"&gt;185&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;because Pandas treats &lt;code&gt;datetime&lt;/code&gt; series as non-numeric variables (which
they technically are).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: To compute quantiles for numeric series, Pandas uses SciPy&amp;#8217;s
&lt;code&gt;scoreatpercentile&lt;/code&gt; function, which in turn relies on a simple linear
interpolation function (&lt;code&gt;_interpolate&lt;/code&gt; in &lt;code&gt;scipy.stats&lt;/code&gt;). &lt;code&gt;datetime&lt;/code&gt;
objects don&amp;#8217;t play well with this function, since when you take the
difference between two &lt;code&gt;datetimes&lt;/code&gt; you don&amp;#8217;t get a number, but instead
a &lt;code&gt;timedelta&lt;/code&gt; tuple, that you can&amp;#8217;t perform mathematical operations on
until you unpack it. The &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; methods will work on
&lt;code&gt;datetimes&lt;/code&gt;, though.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can get around this by extracting the years from the variable, which
will be integers.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;years&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;date_occurred&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;print&lt;/span&gt; &lt;span class="n"&gt;years&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;results in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="mf"&gt;52134.000000&lt;/span&gt;
&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="mf"&gt;2000.572237&lt;/span&gt;
&lt;span class="n"&gt;std&lt;/span&gt; &lt;span class="mf"&gt;10.889045&lt;/span&gt;
&lt;span class="n"&gt;min&lt;/span&gt; &lt;span class="mf"&gt;1400.000000&lt;/span&gt;
&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;1999.000000&lt;/span&gt;
&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;2003.000000&lt;/span&gt;
&lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mf"&gt;2007.000000&lt;/span&gt;
&lt;span class="n"&gt;max&lt;/span&gt; &lt;span class="mf"&gt;2010.000000&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which is a little precise for year data, but how is Pandas to know? At
any rate, we come to the same conclusion as the authors: that three
quarters of the sightings occurred in 1999 or later, and the earliest
date in the data is in 1400. (If we check, we&amp;#8217;ll see this sighting
occurred in Texas, so it&amp;#8217;s certainly an error).&lt;/p&gt;
&lt;p&gt;Plotting histograms&lt;/p&gt;
&lt;p&gt;The authors then plot a histogram of the dates in the data. Like with
&lt;code&gt;quantile&lt;/code&gt;, the &lt;code&gt;hist&lt;/code&gt; plot method (which just calls a Matplotlib
histogram) doesn&amp;#8217;t work with &lt;code&gt;datetime&lt;/code&gt; data. If we try&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;we&amp;#8217;ll get an error complaining that &lt;code&gt;datetime&lt;/code&gt; can&amp;#8217;t be compared with
&lt;code&gt;float&lt;/code&gt;. So, I&amp;#8217;ll just work with the years instead of the full
&lt;code&gt;datetime&lt;/code&gt;. I can generate the plot with a call to the series&amp;#8217; &lt;code&gt;hist&lt;/code&gt;
method, one of several plotting methods for Pandas objects that makes it
extremely easy to get quick plots of them.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;years&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;years&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;years&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;30.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;steelblue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Histogram of years with &lt;span class="caps"&gt;U.S.&lt;/span&gt; &lt;span class="caps"&gt;UFO&lt;/span&gt; sightings&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;All years in data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;quick_hist_all_years.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I explicitly set the bins to match the ggplot defaults used in the book.
We get this plot, which basically matches the authors&amp;#8217;:&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/quick_hist_all_years2.png"&gt;
  &lt;img src="../images/quick_hist_all_years2.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The authors then focus on only data after 1990, using R&amp;#8217;s &lt;code&gt;subset&lt;/code&gt;
function to remove earlier observations from the data. This is
straightforward in Pandas. I&amp;#8217;ll also extract another series with the
years of this subset of dates.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; \&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1990&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;years_post90&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After subsetting, the authors have 46,347 rows left in the data. Looking
at the &lt;code&gt;shape&lt;/code&gt; attribute of the subsetted data frame, we have 46,780.
We&amp;#8217;ve picked up some observations from D.C., as well as from our more
expansive method of finding &lt;span class="caps"&gt;U.S.&lt;/span&gt; locations.&lt;/p&gt;
&lt;p&gt;Another histogram of the subset data looks similar to the authors&amp;#8217; chart
on p. 23, but since I&amp;#8217;m only histogramming over years, I lose some
resolution.&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/quick_hist_post90.png"&gt;
  &lt;img src="../images/quick_hist_post90.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;While the histogram is fine for a quick look at the distribution of
dates, it&amp;#8217;s not a very accurate picture of how sightings evolve over
time: the binning really destroys too much information. It makes more
sense just to do a time-series plot of total sightings by date. We can
do that with some data aggregation and an easy call to the &lt;code&gt;plot&lt;/code&gt; method
in Pandas.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;post90_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo_us&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;post90_count&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Number of &lt;span class="caps"&gt;U.S.&lt;/span&gt; &lt;span class="caps"&gt;UFO&lt;/span&gt; sightings&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;nJanuary 1990 through August 2010&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;post90_count_ts.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This uses Pandas&amp;#8217; awesome &lt;code&gt;groupby&lt;/code&gt; method, which I&amp;#8217;ll discuss more in
the next part. We get the following figure:&lt;/p&gt;
&lt;p&gt;&lt;a href="../images/post90_count_ts.png"&gt;
  &lt;img src="../images/post90_count_ts.png" width=450px /&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Based on this graph, it looks like there&amp;#8217;s a seasonal component to
sightings, which wasn&amp;#8217;t apparent in the histogram. There are also a few
large spikes, especially around the end of the millenium.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This part was a relatively easy one. The next part will focus on data
aggregation using &lt;code&gt;groupby&lt;/code&gt; and &lt;code&gt;reindex&lt;/code&gt; methods. Then I&amp;#8217;ll wrap up
with with replicating the authors&amp;#8217; trellis graph.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 1, Part 2: Cleaning date and location data</title><link href="http://slendermeans.org/ml4h-ch1-p2.html" rel="alternate"></link><updated>2012-04-18T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-04-18:ml4h-ch1-p2.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the &lt;a href="../ml4h-ch1-p1.html"&gt;previous post&lt;/a&gt;, I loaded the raw &lt;span class="caps"&gt;UFO&lt;/span&gt; data into a Pandas data
frame after cleaning up some irregularities in the text file. Since
we&amp;#8217;re ultimately concerned with analyzing &lt;span class="caps"&gt;UFO&lt;/span&gt; sightings over time and
space, the next step is to clean those variables and prepare them for
analysis and vizualization.&lt;/p&gt;
&lt;p&gt;Some Python techniques to note in this part are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Like in the last part, Python string methods are going to come in
    really handy, and be a simple, expressive solution to a lot of
    problems.&lt;/li&gt;
&lt;li&gt;When those aren&amp;#8217;t enough, Python has a pretty straightforward set of
    functions for implementing regular expressions.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;map()&lt;/code&gt; method in Pandas can be used to &amp;#8220;vectorize&amp;#8221; functions
    along a Series (i.e. a data frame column) and is similar to R&amp;#8217;s
    &lt;code&gt;apply&lt;/code&gt;. In general, using a NumPy &lt;code&gt;ufunc&lt;/code&gt; (vectorized function) is
    preferable, but not all operations can be expressed in &lt;code&gt;ufunc&lt;/code&gt;s.
    This is especially true for non-numeric operations, such as for
    strings or dates.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Cleaning dates: mapping and subsetting.&lt;/h2&gt;
&lt;p&gt;The first two columns of the data are dates in &lt;code&gt;YYMMDDD&lt;/code&gt; format, and
Pandas imported them as integers. R has a function, &lt;code&gt;as.Date&lt;/code&gt; that will
operate on a vector of date strings, converting them to numeric dates.
In Python, the &lt;code&gt;strptime&lt;/code&gt; function in the &lt;code&gt;datetime&lt;/code&gt; module performs the
same function, but it not vectorized the way &lt;code&gt;as.Date&lt;/code&gt; is. (Note that R
also has a &lt;code&gt;strptime&lt;/code&gt; that converts date strings to &lt;span class="caps"&gt;POSIX&lt;/span&gt; class object).
Therefore, we have to use the &lt;code&gt;map&lt;/code&gt; method.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ymd_convert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;Convert dates in the imported &lt;span class="caps"&gt;UFO&lt;/span&gt; data.&lt;/span&gt;
&lt;span class="sd"&gt;Clean entries will look like &lt;span class="caps"&gt;YYYMMDD&lt;/span&gt;. If they&amp;#39;re not clean, return &lt;span class="caps"&gt;NA&lt;/span&gt;.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;cnv_dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strptime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;%Y%m&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;cnv_dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;

&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;cnv_dt&lt;/span&gt;

&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ymd_convert&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_reported&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_reported&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ymd_convert&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that &lt;code&gt;map&lt;/code&gt; here is like R&amp;#8217;s &lt;code&gt;apply&lt;/code&gt; function (this is a little
confusing, since Python also has an &lt;code&gt;apply&lt;/code&gt; method that is &lt;em&gt;not&lt;/em&gt; like
R&amp;#8217;s). Since series&amp;#8212;columns in Pandas data frames&amp;#8212;are  just NumPy
&lt;code&gt;ndarrays&lt;/code&gt; underneath, only NumPy &lt;code&gt;ufuncs&lt;/code&gt; will operate on them in a
vectorized (fast, elementwise) fashion. Base Python functions, and any
more complicated functions you create from them, will have to be
explicitly mapped. This is a little different from R, where, since the
fundamental object in the language is the vector, functions are more
likely vectorized than not. Nonetheless, NumPy &lt;code&gt;ufuncs&lt;/code&gt; do cover the
gamut of mathematical operations, and for other cases, the &lt;code&gt;map&lt;/code&gt; method
is easy enough to implement.&lt;/p&gt;
&lt;p&gt;Then we just get rid of the rows with one date or the other not in
proper &lt;code&gt;YYYMMDD&lt;/code&gt; format.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Get rid of the rows that couldn&amp;#39;t be conformed to datetime.&lt;/span&gt;
&lt;span class="n"&gt;ufo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;notnull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_reported&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;notnull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The subsetting of the data frame is done by indexing it with a boolean
vector. Since the &lt;code&gt;df[ ]&lt;/code&gt; operation returns rows, the&lt;/p&gt;
&lt;p&gt;One can also subset an R data frame this way. R though, also has a
&lt;code&gt;subset&lt;/code&gt; function, with the syntax:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;!&lt;/span&gt;&lt;span class="ow"&gt;is&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;date.reported&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="err"&gt;!&lt;/span&gt;&lt;span class="ow"&gt;is&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;date.occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;being equivalent to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ufo &lt;span class="o"&gt;=&lt;/span&gt; subset&lt;span class="p"&gt;(&lt;/span&gt;ufo&lt;span class="p"&gt;,&lt;/span&gt; where &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;date_reported&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;date.occurred&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The general &lt;code&gt;subset&lt;/code&gt; syntax is:
&lt;code&gt;df.new = subset(df.orig, where = condition, select = columns)&lt;/code&gt;.
Since &lt;code&gt;subset&lt;/code&gt; looks for the variables referenced in the &lt;code&gt;where&lt;/code&gt; and
&lt;code&gt;select&lt;/code&gt; arguments in the &lt;code&gt;df.orig&lt;/code&gt; environment, there&amp;#8217;s no need to call
them as &lt;code&gt;df.orig[ , 'var']&lt;/code&gt; or &lt;code&gt;df.orig$var&lt;/code&gt;. There are other useful
commands that work like this: &lt;code&gt;with&lt;/code&gt;, &lt;code&gt;within&lt;/code&gt;, and &lt;code&gt;transform&lt;/code&gt;, for
example.&lt;/p&gt;
&lt;p&gt;I find the &lt;code&gt;subset&lt;/code&gt; function in R more expressive and easier to read
than the boolean masking method, and I miss there being a Pandas
equivalent.&lt;/p&gt;
&lt;h2&gt;Cleaning locations: string functions and regular expressions&lt;/h2&gt;
&lt;p&gt;Cleaning the date variables was relatively easy. Locations are trickier,
and the authors don&amp;#8217;t do a particularly thorough job of it. (No knock on
them, reading several pages of text cleaning would be deadly boring, and
they&amp;#8217;te just illustrating some techniques). I&amp;#8217;ll suggest a slightly
better method that will pick up some extra data, but even that could
probably be improved if we were concerned about getting every bit of
information out of this dataset.&lt;/p&gt;
&lt;p&gt;The authors assume that valid &lt;span class="caps"&gt;U.S.&lt;/span&gt; locations are going to be in &amp;#8220;City,
&lt;span class="caps"&gt;ST&lt;/span&gt;&amp;#8221; format (e.g., &amp;#8220;Iowa City, &lt;span class="caps"&gt;IA&lt;/span&gt;&amp;#8221;). Anything else is going to be dropped
as either an international record, or not worth cleaning.&lt;/p&gt;
&lt;p&gt;They write a function that takes a location record and checks that it
fits this pattern by seeing if R&amp;#8217;s &lt;code&gt;strsplit&lt;/code&gt; function splits it into
two elements at a comma. If so, the function returns a vector containing
the two elements, otherwise it returns a vector with two &lt;code&gt;NAs&lt;/code&gt; (though
not quite, see the note below). They then use R&amp;#8217;s &lt;code&gt;lapply&lt;/code&gt; to apply the
function elementwise, and collect the resulting vectors in a list. Then
there are some tricks to get the list into an &lt;code&gt;Nx2&lt;/code&gt; matrix, and then put
each column of the matrix into a variable in the data frame as &lt;code&gt;USCity&lt;/code&gt;
and &lt;code&gt;USState&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: the authors wrap &lt;code&gt;strsplit&lt;/code&gt; in &lt;code&gt;tryCatch&lt;/code&gt; assuming that the
former will throw an error if there are no commas in the string. My
testing shows that&amp;#8217;s not the case, and &lt;code&gt;strsplit&lt;/code&gt; will just return the
original string. The &lt;code&gt;tryCatch&lt;/code&gt; wrapper doesn&amp;#8217;t have any effect, and
that line of code doesn&amp;#8217;t appear to drop locations without commas as
the authors intend. This isn&amp;#8217;t really a problem, since they later
subset on records with valid &lt;span class="caps"&gt;U.S.&lt;/span&gt; states, and that ultimately drops
the no-comma location records.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It&amp;#8217;s easy to write a similar function in Python, using the &lt;code&gt;split&lt;/code&gt;
method of string objects.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_location&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="n"&gt;split_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;clean_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;split_location&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;split_location&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;clean_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;clean_location&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is near-direct translation of the authors&amp;#8217; &lt;code&gt;get.location&lt;/code&gt; function.
Note the &lt;code&gt;strip&lt;/code&gt; method and the list comprehension replace the &lt;code&gt;gsub&lt;/code&gt;
function the authors use to remove beginning and trailing white space
from the extracted city and states.&lt;/p&gt;
&lt;p&gt;But a quick look at the data shows that there are lots of valid &lt;span class="caps"&gt;U.S.
&lt;/span&gt;locations that will get dropped with this method. Specifically, the city
part of the location contains commas in many records, so the split
methods will return more than two elements and we will drop them as
invalid. Let&amp;#8217;s check out some cases with the following code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;multi_commas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;location&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; \&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Number of entries w/ multiple commas&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;multi_commas&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;location&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;multi_commas&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This returns:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;entries&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;multiple&lt;/span&gt; &lt;span class="n"&gt;commas&lt;/span&gt; &lt;span class="mi"&gt;1055&lt;/span&gt;
&lt;span class="mi"&gt;1473&lt;/span&gt; &lt;span class="n"&gt;Aquaduct&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;near&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;over&lt;/span&gt; &lt;span class="n"&gt;desert&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;before&lt;/span&gt; &lt;span class="n"&gt;entering&lt;/span&gt; &lt;span class="n"&gt;California&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;CA&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;1985&lt;/span&gt; &lt;span class="n"&gt;Redding&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;northeast&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;over&lt;/span&gt; &lt;span class="n"&gt;Millville&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;approximately&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;CA&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;2108&lt;/span&gt; &lt;span class="n"&gt;Farmington&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;SE&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deserted&lt;/span&gt; &lt;span class="n"&gt;area&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Hwy&lt;/span&gt; &lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;NM&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;2160&lt;/span&gt; &lt;span class="n"&gt;Stouthill&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;community&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="n"&gt;city&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="n"&gt;miles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;TN&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;TN&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;2242&lt;/span&gt; &lt;span class="n"&gt;Highway&lt;/span&gt; &lt;span class="mi"&gt;71&lt;/span&gt; &lt;span class="n"&gt;between&lt;/span&gt; &lt;span class="n"&gt;Clearmont&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Missouri&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;Maryville&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Missou&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MO&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;2257&lt;/span&gt; &lt;span class="n"&gt;Bayfield&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;near&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Lake&lt;/span&gt; &lt;span class="n"&gt;Superior&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;south&lt;/span&gt; &lt;span class="n"&gt;shore&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;WI&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;2287&lt;/span&gt; &lt;span class="n"&gt;Unidentified&lt;/span&gt; &lt;span class="n"&gt;object&lt;/span&gt; &lt;span class="n"&gt;sig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;VIC&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Australia&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="mi"&gt;2297&lt;/span&gt; &lt;span class="n"&gt;Garfield&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;VIC&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Australia&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="mi"&gt;2384&lt;/span&gt; &lt;span class="n"&gt;Northeast&lt;/span&gt; &lt;span class="n"&gt;Cape&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AFS&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;St&lt;/span&gt; &lt;span class="n"&gt;Lawrence&lt;/span&gt; &lt;span class="n"&gt;Island&lt;/span&gt;&lt;span class="p"&gt;,,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;AK&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;2458&lt;/span&gt; &lt;span class="n"&gt;Flisa&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Sol&lt;/span&gt;&lt;span class="err"&gt;ø&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Hedemark&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Norway&lt;/span&gt;&lt;span class="p"&gt;),[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So there are over a thousand location records with more than one comma,
and out of the first ten, seven are valid &lt;span class="caps"&gt;U.S.&lt;/span&gt; locations.&lt;/p&gt;
&lt;p&gt;To save these records, I&amp;#8217;ll try another method, using regular
expressions to search for locations that end with &amp;#8220;, &lt;span class="caps"&gt;ST&lt;/span&gt;&amp;#8221;-type patterns.
Since we&amp;#8217;re going to ultimately use &lt;code&gt;map&lt;/code&gt; to check this pattern for
every row in the data, I&amp;#8217;ll &lt;em&gt;compile&lt;/em&gt; the pattern first, which typically
speeds up repeated searches.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;us_state_pattern&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;, [A-Z][A-Z]\$&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;IGNORECASE&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, I&amp;#8217;ll create a function that takes a location record as input, and
applies the regex search to it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_location2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;strip_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;us_state_search&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;us_state_pattern&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;strip_location&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;us_state_search&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;clean_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;us_city&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;strip_location&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;us_state_search&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
        &lt;span class="n"&gt;us_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;strip_location&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;us_state_search&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;clean_location&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;us_city&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;us_state&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;clean_location&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;sourcecode&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To follow this, note that if the regex pattern isn&amp;#8217;t found, then the
&lt;code&gt;search&lt;/code&gt; method returns &lt;code&gt;None&lt;/code&gt;, otherwise it returns a search object
with several useful attributes. One of them is &lt;code&gt;start&lt;/code&gt;, which indicates
where in the string the pattern starts. To extract the city, we just
take all the characters in the string up to &lt;code&gt;start&lt;/code&gt;. The state will
start 2 characters later (since we don&amp;#8217;t want the comma or space in
front). The function, like the previous one, finally returns a two
element list with either a city and a state, or two blanks for records
that didn&amp;#8217;t match the pattern.&lt;/p&gt;
&lt;p&gt;I again use &lt;code&gt;map&lt;/code&gt; to apply this function elementwise to the location
column:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;location_lists&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;location&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;get_location2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This returns a series of two-element lists. I use list comprehensions to
extract the first and second elements out to individual lists, which I
assign to &lt;code&gt;us_city&lt;/code&gt; and &lt;code&gt;us_state&lt;/code&gt; variables in the data frame. It
sounds complicated, but in Python it&amp;#8217;s just two fairly readable lines of
code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_city&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;city&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;city&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;st&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;location_lists&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;st&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;city&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;st&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;location_lists&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last step in cleaning the location data is to weed out any locations
that fit the &amp;#8220;City, &lt;span class="caps"&gt;ST&lt;/span&gt;&amp;#8221; pattern, but were not in &lt;span class="caps"&gt;U.S.&lt;/span&gt; states–Canadian
provinces for example. The authors do this in a straightforward way by
making a list of the 50 &lt;span class="caps"&gt;U.S.&lt;/span&gt; states and using R&amp;#8217;s &lt;code&gt;match&lt;/code&gt; function to
see where the &lt;span class="caps"&gt;U.S.&lt;/span&gt; state variable matches a state in the list. They then
subset the data frame to records where there is a match.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The authors leave &lt;span class="caps"&gt;D.C.&lt;/span&gt; out of the list of states. It looks
like there are about 90 records with &lt;span class="caps"&gt;D.C.&lt;/span&gt; in the state column.
Unfortunately a couple of these aren&amp;#8217;t Washington, D.C., but are South
American &amp;#8220;Distrito Capitals.&amp;#8221; I&amp;#8217;ll add &lt;span class="caps"&gt;D.C.&lt;/span&gt; into the list and
subsequent analyses, keeping in mind there are a few false positives.
(This may be true for other states as well, like I said at the start,
this cleaning isn&amp;#8217;t 100% accurate.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;NumPy has an equivalent to the &lt;code&gt;match&lt;/code&gt; function, though the name is a
little more awkward: &lt;code&gt;in1d&lt;/code&gt;. Below, I assign anything records in
&lt;code&gt;us_state&lt;/code&gt; that doesn&amp;#8217;t have a match in the state list a blank string,
then drop them out of the data.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;in1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;us_states&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_city&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;in1d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;us_states&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;ufo_us&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;us_state&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;to_list&lt;/code&gt; is necessary because Pandas requires a list argument to
&lt;code&gt;[ ]&lt;/code&gt;, and &lt;code&gt;in1d&lt;/code&gt; returns a NumPy array.&lt;/p&gt;
&lt;p&gt;And that&amp;#8217;s that. In the next post I&amp;#8217;ll start exploring the data
graphically.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry><entry><title>Machine Learning for Hackers Chapter 1, Part 1: Loading data</title><link href="http://slendermeans.org/ml4h-ch1-p1.html" rel="alternate"></link><updated>2012-04-14T04:00:00-04:00</updated><author><name>Carl</name></author><id>tag:slendermeans.org,2012-04-14:ml4h-ch1-p1.html</id><summary type="html">&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;This is my first &lt;em&gt;Will it Python?&lt;/em&gt; post. These posts document
my experiences trying to port complete and interesting R projects to
Python. I&amp;#8217;m beginning by going through the recently published &lt;a href="http://shop.oreilly.com/product/0636920018483.do"&gt;&lt;em&gt;Machine
Learning for Hackers&lt;/em&gt;&lt;/a&gt; (&lt;span class="caps"&gt;MLFH&lt;/span&gt;) by &lt;a href="http://www.drewconway.com"&gt;Drew Conway&lt;/a&gt; and &lt;a href="http://johnmyleswhite.com"&gt;John Miles
White&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More information on the posts is &lt;a href="../pages/will-it-python.html"&gt;here&lt;/a&gt;, and archives are &lt;a href="../category/will-it-python.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The first chapter of &lt;span class="caps"&gt;MLFH&lt;/span&gt; is a gentle introduction to loading,
manipulating and graphing data in R. To keep the tutorial interesting,
the authors have found a fun dataset of &lt;a href="https://github.com/johnmyleswhite/ML_for_Hackers/tree/master/01-Introduction/data/ufo"&gt;&lt;span class="caps"&gt;UFO&lt;/span&gt; sightings&lt;/a&gt; to work
through.&lt;/p&gt;
&lt;p&gt;Since this chapter is mainly devoted to loading and manipulating data, a
lot of the R functionality they exploit is going to have an analog in
&lt;a href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;. Even though there&amp;#8217;s not too much exciting going on in this
chapter, it&amp;#8217;s a great way to explore how basic data tasks get done in
Python. It turns out there are some interesting differences between how
R and Python handle even this simple stuff.&lt;/p&gt;
&lt;p&gt;In this first post, I&amp;#8217;ll focus on just getting the data into the work
environment. The complete code for the chapter is located in a Github
repo, &lt;a href="https://github.com/carljv/Will_it_Python/tree/master/MLFH/CH1"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Data with inconsistent column lengths: break or compensate?&lt;/h2&gt;
&lt;p&gt;The raw data is contained in a tab-separated file and the authors use
R&amp;#8217;s &lt;code&gt;read.delim()&lt;/code&gt; function to read it into an R dataframe. The data seem
to load smoothly, and there are no errors or warnings. There are no
headers in the data, so the authors set the &lt;code&gt;headers&lt;/code&gt; argument
of &lt;code&gt;read.delim()&lt;/code&gt; to &lt;code&gt;FALSE&lt;/code&gt; and name the columns of dataframe after
it&amp;#8217;s loaded.&lt;/p&gt;
&lt;p&gt;The same procedure in Python uses the &lt;code&gt;read_table()&lt;/code&gt; function in Pandas:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/ufo/ufo_awesome.tsv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;na_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This, though, will raise an exception, complaining that there are the
&amp;#8220;wrong number of columns.&amp;#8221; R loaded the data without complaint, so
what&amp;#8217;s going on?&lt;/p&gt;
&lt;p&gt;It turns out that &lt;code&gt;read_table()&lt;/code&gt; is right to complain. Let&amp;#8217;s use
Python&amp;#8217;s basic file &lt;span class="caps"&gt;IO&lt;/span&gt; to read each line of the file, and separate the
line into columns by splitting it at tab characters. We&amp;#8217;d expect each
line to have six columns. As soon as we hit a line that doesn&amp;#8217;t, I&amp;#8217;ll
break the line-reading loop, and print out the line number and the
columns it was split into. This will tell us where the first (if any)
bad line is in the file, and give a look at what&amp;#8217;s wrong with it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;inpath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;data/ufo/ufo_awesome.tsv&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;inf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;splitline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;splitline&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;first_bad_line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;splitline&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;First bad row:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first_bad_line&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;

&lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This code prints the following output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;First&lt;/span&gt; &lt;span class="n"&gt;bad&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;754&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="mi"&gt;19950704&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;19950706&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;Orlando&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;FL&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;min&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="n"&gt;would&lt;/span&gt; &lt;span class="n"&gt;like&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;report&lt;/span&gt; &lt;span class="n"&gt;three&lt;/span&gt; &lt;span class="n"&gt;yellow&lt;/span&gt; &lt;span class="n"&gt;oval&lt;/span&gt; &lt;span class="n"&gt;lights&lt;/span&gt; &lt;span class="n"&gt;which&lt;/span&gt; &lt;span class="n"&gt;passed&lt;/span&gt; &lt;span class="n"&gt;over&lt;/span&gt;
&lt;span class="n"&gt;Orlando&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Florida&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;July&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1995&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="n"&gt;aproximately&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt; &lt;span class="n"&gt;These&lt;/span&gt;
&lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="k"&gt;sizeof&lt;/span&gt; &lt;span class="n"&gt;Venus&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;which&lt;/span&gt; &lt;span class="n"&gt;they&lt;/span&gt; &lt;span class="n"&gt;passed&lt;/span&gt; &lt;span class="n"&gt;close&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt; &lt;span class="n"&gt;Two&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;them&lt;/span&gt; &lt;span class="n"&gt;traveled&lt;/span&gt;
&lt;span class="n"&gt;one&lt;/span&gt; &lt;span class="n"&gt;after&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;otherat&lt;/span&gt; &lt;span class="n"&gt;exactly&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;same&lt;/span&gt; &lt;span class="n"&gt;speed&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="n"&gt;heading&lt;/span&gt;
&lt;span class="n"&gt;south&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;southeast&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;third&lt;/span&gt; &lt;span class="n"&gt;oneappeared&lt;/span&gt; &lt;span class="n"&gt;about&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;minute&lt;/span&gt; &lt;span class="n"&gt;later&lt;/span&gt; &lt;span class="n"&gt;following&lt;/span&gt;
&lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;same&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="n"&gt;two&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;Thewhole&lt;/span&gt; &lt;span class="n"&gt;sighting&lt;/span&gt; &lt;span class="n"&gt;lasted&lt;/span&gt; &lt;span class="n"&gt;about&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="n"&gt;minutes&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;There&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;other&lt;/span&gt; &lt;span class="n"&gt;witnesses&lt;/span&gt; &lt;span class="n"&gt;oldenough&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;report&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;sighting&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;My&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;year&lt;/span&gt; &lt;span class="n"&gt;old&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;year&lt;/span&gt; &lt;span class="n"&gt;old&lt;/span&gt; &lt;span class="n"&gt;children&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;theones&lt;/span&gt; &lt;span class="n"&gt;who&lt;/span&gt; &lt;span class="n"&gt;called&lt;/span&gt; &lt;span class="n"&gt;my&lt;/span&gt;
&lt;span class="n"&gt;attention&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;quot&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;moving&lt;/span&gt; &lt;span class="n"&gt;stars&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;quot&lt;/span&gt;&lt;span class="p"&gt;;.&lt;/span&gt; &lt;span class="n"&gt;These&lt;/span&gt; &lt;span class="n"&gt;objects&lt;/span&gt; &lt;span class="n"&gt;moved&lt;/span&gt;
&lt;span class="n"&gt;fasterthan&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;airplane&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;did&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;resemble&lt;/span&gt; &lt;span class="n"&gt;an&lt;/span&gt; &lt;span class="n"&gt;aircraft&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;moving&lt;/span&gt;
&lt;span class="n"&gt;much&lt;/span&gt; &lt;span class="n"&gt;slowerthan&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;shooting&lt;/span&gt; &lt;span class="n"&gt;star&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;As&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;them&lt;/span&gt; &lt;span class="n"&gt;being&lt;/span&gt; &lt;span class="n"&gt;fireworks&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;their&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;
&lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;too&lt;/span&gt; &lt;span class="n"&gt;regularand&lt;/span&gt; &lt;span class="n"&gt;coordinated&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;If&lt;/span&gt; &lt;span class="n"&gt;anybody&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;saw&lt;/span&gt; &lt;span class="n"&gt;this&lt;/span&gt; &lt;span class="n"&gt;phenomenon&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;please&lt;/span&gt; &lt;span class="n"&gt;contact&lt;/span&gt; &lt;span class="n"&gt;me&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="n"&gt;ler&lt;/span&gt;&lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="n"&gt;gnv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ifas&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ufl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edu&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So we see that in row 754 of the file, we came across a line with seven
columns (six tabs). The sixth column of the data is a &amp;#8220;long&amp;#8221; description
of the &lt;span class="caps"&gt;UFO&lt;/span&gt; sighting, and here it looks like there was a tab character
within the long description, creating extraneous columns.&lt;/p&gt;
&lt;p&gt;Why didn&amp;#8217;t R have a problem with this line? We can see what happened if
we look on page 15 of the &lt;span class="caps"&gt;MLFH&lt;/span&gt;. There the authors show rows of the data
where the first column–the date of the sigthing–doesn&amp;#8217;t match a date
format. The first instance of a bad observation in the first column of
the R data is &lt;code&gt;ler@gnv.ifas.ufl.edu&lt;/code&gt;, which we just saw is actually the
first instance of a spurious seventh column. Apparently, &lt;code&gt;read.delim()&lt;/code&gt;
is inferring the number of columns from the first few rows, then pushing
any extra columns to a new row.&lt;/p&gt;
&lt;p&gt;I think I much prefer the Pandas behavior here to R&amp;#8217;s. Even though R
actually did get the data loaded with no fuss, it ended up mangling it
pretty badly. Given the size of the dataset, the rarity of these bad
rows, and the authors&amp;#8217; cleaning process, it may not have mattered much
at the end of the analysis. But that&amp;#8217;s not going to be true in every
case – and here, R isn&amp;#8217;t even throwing a warning to indicate that
something might be fishy with the raw data.&lt;/p&gt;
&lt;p&gt;Note though, that if the authors had used &lt;code&gt;read.delim()&lt;/code&gt; with a
&lt;code&gt;col.names&lt;/code&gt; argument, then R would have raised an error when it came
across a row with more columns than were indicated by the supplied list
of column names.&lt;/p&gt;
&lt;p&gt;This is a pretty boring problem, but an important one. To sum up:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Lesson 1&lt;/strong&gt;: R&amp;#8217;s &lt;code&gt;read.delim()&lt;/code&gt; without either &lt;code&gt;header = TRUE&lt;/code&gt; or a
&lt;code&gt;col.names&lt;/code&gt; argument is dangerous. If you have to load the data to
figure out what the column names should be, try loading it again with
the column names you&amp;#8217;ve assigned.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Preparing the raw data to load into a data frame.&lt;/h2&gt;
&lt;p&gt;Now that we&amp;#8217;ve discovered irregularities in the raw data that are
preventing it from fitting neatly into a data frame, we have to fix
them.&lt;/p&gt;
&lt;p&gt;There are two options, both involve processing the file line-by-line.
First, we can take the data in the columns after the sixth and append
them to the end of the data in the sixth column. The sixth column is a
long text discription of the event, and the extra columns are likely to
be continuations of that description. But, we don&amp;#8217;t actually end up
caring about the long description in our analysis, so I&amp;#8217;ll take a second
approach and just delete those extra columns.&lt;/p&gt;
&lt;p&gt;The procedure is encapsulated in the function below. It reads lines from
the original file, &lt;code&gt;inpath&lt;/code&gt;, cleans them, and writes the result to
&lt;code&gt;outpath&lt;/code&gt;. Note that this function doesn&amp;#8217;t actually return anything;
it&amp;#8217;s just a side-effect on the &lt;code&gt;outpath&lt;/code&gt; file.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;ufotab_to_sixcols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;outpath&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="sd"&gt;Keep only the first 6 columns of data from messy &lt;span class="caps"&gt;UFO&lt;/span&gt; &lt;span class="caps"&gt;TSV&lt;/span&gt; file.&lt;/span&gt;

&lt;span class="sd"&gt;The &lt;span class="caps"&gt;UFO&lt;/span&gt; data set is only supposed to have six columns. But...&lt;/span&gt;

&lt;span class="sd"&gt;The sixth column is a long written description of the &lt;span class="caps"&gt;UFO&lt;/span&gt; sighting, and&lt;/span&gt;
&lt;span class="sd"&gt;sometimes is broken by tab characters which create extra columns.&lt;/span&gt;

&lt;span class="sd"&gt;For these records, we only keep the first six columns. This typically&lt;/span&gt;
&lt;span class="sd"&gt;cuts off some of the long description.&lt;/span&gt;

&lt;span class="sd"&gt;Sometimes a line has less than six columns. These are not written to&lt;/span&gt;
&lt;span class="sd"&gt;the output file (i.e., they&amp;#39;re dropped from the data). These records&lt;/span&gt;
&lt;span class="sd"&gt;are usually so comprimised as to be uncleanable anyway.&lt;/span&gt;

&lt;span class="sd"&gt;This function has (is) a side effect on the outpath file, to which it&lt;/span&gt;
&lt;span class="sd"&gt;writes output.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;inf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;outf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;splitline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# Skip short lines, which are dirty beyond repair, anyway.&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;splitline&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="k"&gt;continue&lt;/span&gt;

&lt;span class="n"&gt;newline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;splitline&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c"&gt;# Records that have been truncated won&amp;#39;t end in a newline character&lt;/span&gt;
&lt;span class="c"&gt;# so add one.&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;newline&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;newline&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;outf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newline&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;outf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This function performs the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the input file for reading and the output file for writing.&lt;/li&gt;
&lt;li&gt;Read a line from the original file.&lt;/li&gt;
&lt;li&gt;Split the line into columns at the tab characters using the
    &lt;code&gt;split()&lt;/code&gt; method.&lt;/li&gt;
&lt;li&gt;If line is split into less than six columns, ignore this line and go
    read the next one.&lt;/li&gt;
&lt;li&gt;Otherwise rejoin the first six columns of the split line back
    together with tab characters using the &lt;code&gt;join()&lt;/code&gt; method. This results
    in &lt;code&gt;newline&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If there&amp;#8217;s not a line break character at the end of &lt;code&gt;newline&lt;/code&gt; (which
    will happen if we&amp;#8217;ve cut off the ending column because it was past
    the sixth column), then add one on.&lt;/li&gt;
&lt;li&gt;Write &lt;code&gt;newline&lt;/code&gt; to the output file.&lt;/li&gt;
&lt;li&gt;Repeat 2-7 with the next line of the input file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that step 4 means that short lines with less than six columns (5
tabs) don&amp;#8217;t get written to the cleaned file. I haven&amp;#8217;t investigated in
depth why some rows are too short and whether there&amp;#8217;s a way to fix those
rows instead of tossing them out, but it&amp;#8217;s unlikely the fix would be
simple or reliable.&lt;/p&gt;
&lt;p&gt;I run the function to create a cleaned-up tab-separated file called
&lt;code&gt;ufo_awesome_6col.tsv&lt;/code&gt;. (The path to the input file, &lt;code&gt;inpath&lt;/code&gt;, was
already defined).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;outpath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;data/ufo/ufo_awesome_6col.tsv&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ufotab_to_sixcols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inpath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;outpath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Trying &lt;code&gt;read_table()&lt;/code&gt; again.&lt;/h2&gt;
&lt;p&gt;Now I&amp;#8217;ll try using Pandas and &lt;code&gt;read_table()&lt;/code&gt; again to load the file into
a data frame. (Since I know what the column names are supposed to be,
I&amp;#8217;ll just pass them to the function instead of adding them later.)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;data/ufo/ufo_awesome_6col.tsv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\\&lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;na_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="n"&gt;header&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;date_occurred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;date_reported&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="s"&gt;&amp;#39;location&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;short_desc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;duration&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="s"&gt;&amp;#39;long_desc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And this now runs without a hitch. We&amp;#8217;ll use the &lt;code&gt;head()&lt;/code&gt; and
&lt;code&gt;to_string()&lt;/code&gt; methods of a Pandas data frame to compare the first six
rows of the data to what&amp;#8217;s shown in the table on p. 14 of &lt;span class="caps"&gt;MLFH&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ufo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;formatters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
                       &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;long_desc&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The dictionary in the &lt;code&gt;formatters&lt;/code&gt; argument tells &lt;code&gt;to_string()&lt;/code&gt; to only
print the first 21 characters in the long description. The result is the
following table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;   &lt;span class="n"&gt;date_occurred&lt;/span&gt;  &lt;span class="n"&gt;date_reported&lt;/span&gt;              &lt;span class="n"&gt;location&lt;/span&gt;  &lt;span class="n"&gt;short_desc&lt;/span&gt; &lt;span class="n"&gt;duration&lt;/span&gt;                &lt;span class="n"&gt;long_desc&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;       &lt;span class="mi"&gt;19951009&lt;/span&gt;       &lt;span class="mi"&gt;19951009&lt;/span&gt;         &lt;span class="n"&gt;Iowa&lt;/span&gt; &lt;span class="n"&gt;City&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;IA&lt;/span&gt;&lt;/span&gt;         &lt;span class="n"&gt;NaN&lt;/span&gt;      &lt;span class="n"&gt;NaN&lt;/span&gt;    &lt;span class="n"&gt;Man&lt;/span&gt; &lt;span class="n"&gt;repts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;witnessing&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;       &lt;span class="mi"&gt;19951010&lt;/span&gt;       &lt;span class="mi"&gt;19951011&lt;/span&gt;         &lt;span class="n"&gt;Milwaukee&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;WI&lt;/span&gt;&lt;/span&gt;         &lt;span class="n"&gt;NaN&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;     &lt;span class="n"&gt;Man&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;Hwy&lt;/span&gt; &lt;span class="mi"&gt;43&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;SW&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;       &lt;span class="mi"&gt;19950101&lt;/span&gt;       &lt;span class="mi"&gt;19950103&lt;/span&gt;           &lt;span class="n"&gt;Shelton&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;WA&lt;/span&gt;&lt;/span&gt;         &lt;span class="n"&gt;NaN&lt;/span&gt;      &lt;span class="n"&gt;NaN&lt;/span&gt;     &lt;span class="n"&gt;Telephoned&lt;/span&gt; &lt;span class="n"&gt;Report&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;&lt;span class="caps"&gt;CA&lt;/span&gt;&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;       &lt;span class="mi"&gt;19950510&lt;/span&gt;       &lt;span class="mi"&gt;19950510&lt;/span&gt;          &lt;span class="n"&gt;Columbia&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;MO&lt;/span&gt;&lt;/span&gt;         &lt;span class="n"&gt;NaN&lt;/span&gt;    &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;   &lt;span class="n"&gt;Man&lt;/span&gt; &lt;span class="n"&gt;repts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;son&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;apos&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;       &lt;span class="mi"&gt;19950611&lt;/span&gt;       &lt;span class="mi"&gt;19950614&lt;/span&gt;           &lt;span class="n"&gt;Seattle&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;WA&lt;/span&gt;&lt;/span&gt;         &lt;span class="n"&gt;NaN&lt;/span&gt;      &lt;span class="n"&gt;NaN&lt;/span&gt;    &lt;span class="n"&gt;Anonymous&lt;/span&gt; &lt;span class="n"&gt;caller&lt;/span&gt; &lt;span class="n"&gt;rept&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;       &lt;span class="mi"&gt;19951025&lt;/span&gt;       &lt;span class="mi"&gt;19951024&lt;/span&gt;  &lt;span class="n"&gt;Brunswick&lt;/span&gt; &lt;span class="n"&gt;County&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;ND&lt;/span&gt;&lt;/span&gt;         &lt;span class="n"&gt;NaN&lt;/span&gt;   &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;   &lt;span class="n"&gt;Sheriff&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;apos&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="n"&gt;office&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And this matches the authors&amp;#8217; table on p. 14. So we&amp;#8217;re off to a good
start. In the next post we&amp;#8217;ll clean this data up some more and do some
munging to get at the information we&amp;#8217;re interested in.&lt;/p&gt;</summary><category term="machine learning"></category><category term="python"></category><category term="R"></category></entry></feed>