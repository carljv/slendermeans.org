<!DOCTYPE html>
<html lang="en">

<head>
    <title> Slender Means </title>
    <meta charset="utf-8">

    <link rel="shortcut icon" type="image/x-icon" href="http://slendermeans.org/theme/images/favicon.ico">

    <link href="http://slendermeans.org/theme/css/tango_code.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/webfonts.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/layout.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/type.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/color-decor.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/fontawesome/font-awesome.min.css" rel="stylesheet">
</head>

<body>
  <header id="top-matter">
    <!-- The Top Matter is comprised of the site Logo (which includes an image,
    the site slogan and the site name), and the Main Navigation menu. -->

    <hgroup id="logo">
        <a href="http://slendermeans.org/index.html">
        <!-- The graphic/title logo is a link that points home. The logo contains the site slogan and title, the text of which are styled in logo.css -->
        <h1 id="site-name">
          <span id="S">s</span><span id="lender">lender</span>
          <span id="M">m</span><span id="eans">eans</span>
        </h1>
        <h2 id="site-tag"> i'd like to drop my trousers to the world i am a man of means of </h2>
      </a>
    </hgroup>
    <nav id="main-nav">
    <!-- The navigation bar with links to site pages -->
        <ul>
          <!-- Home and Archives are listed explicitly -->
          <li><a href="http://slendermeans.org/index.html">Home</a></li>
          <li><a href="http://slendermeans.org/archives.html">Archives</a></li>

          <!-- Others are in PAGES variable, though we exclude the colophon
          page, whose link will go at the site's end-matter footer -->
              <li > <a href="http://slendermeans.org/pages/about.html"> About </a></li>
              <li > <a href="http://slendermeans.org/pages/blogroll.html"> Blogroll </a></li>
              <li > <a href="http://slendermeans.org/pages/will-it-python.html"> Will it  Python? </a></li>
        </ul>
    </nav>
  </header>
      <!-- For pages that have a title describing their content -->

  <div id="content">
    <article class="h-entry">
      <h1 class="p-name"><a href = "http://slendermeans.org/ml4h-ch6.html" rel = "bookmark"> <i>Machine Learning for Hackers</i> Chapter 6: Regression models with regularization </a></h1>

      <!-- Post info has date, author, and category -->
      <footer class="article-panel post-info">
        <ul>
          <li><time class="dt-published" datetime="2013-02-08T20:07:00" pubdate> February 08, 2013 </time></li>
            <li> <address class="p-author h-card"><a href="http://slendermeans.org/author/carl.html"> Carl </a></address></li>
              <li><a class = "p-category" href="http://slendermeans.org/category/will-it-python.html"> Will it Python </a></li>
        </ul>
      </footer>

      <div class="e-content">
        <p>In my opinion, Chapter 6 is the most important chapter in <em>Machine
Learning for Hackers</em>. It introduces the fundamental problem of machine
learning: overfitting and the bias-variance tradeoff. And it
demonstrates the two key tools for dealing with it: regularization and
cross-validation.</p>
<p>It&#8217;s also a fun chapter to write in Python, because it lets me play with
the fantastic <a href="http://scikit-learn.org/stable/">scikit-learn</a> library. scikit-learn is loaded with
hi-tech machine learning models, along with convenient &#8220;pipeline&#8221;-type
functions that facilitate the process of cross-validating and selecting
hyperparameters for models. Best of all, it&#8217;s <a href="http://scikit-learn.org/stable/">very well
documented</a>.</p>
<h2>Fitting a sine wave with polynomial regression</h2>
<p>The chapter starts out with a useful toy example&#8212;trying to fit a curve
to data generated by a sine function over the interval [0, 1] with added
Gaussian noise. The natural way to fit nonlinear data like this is using
a polynomial function, so that the output, <em>y</em> is a function of powers
of the input <em>x</em>. But there are two problems with this.</p>
<p>First, we can generate highly correlated regressors by taking powers of
<em>x</em>, leading to noisy parameter estimates. The input <em>x</em> are evenly
space numbers on the interval [0, 1]. So <em>x</em> and <em>x<sup>2</sup></em> are going to
have a correlation over 95%. Similar with <em>x<sup>2</sup></em> and <em>x<sup>3</sup></em>. The
solution to this is to use <em>orthogonalized</em> polynomial functions:
tranformations of x that, when summed, result in polynomial functions,
but are orthogonal (therefore uncorrelated) with each other.</p>
<p>Luckily, we can easily calculate these transformations using patsy. The
<code>C(x, Poly)</code> transform computes orthonormal polynomial functions of <em>x</em>,
then we&#8217;ll extract out various orders of the polynomial. So
<code>Xpoly[:, :2]</code> selects out the 0th and 1st order functions, then when
summed will give us a first order polynomial (i.e. linear). Similarly
<code>Xpoly[: :4]</code> gives us the 0th through 3rd order functions, which sum up
to a cubic polynomial.</p>
<div class="highlight"><pre><span class="n">sin_data</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">({</span><span class="s">&#39;x&#39;</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)})</span>
<span class="n">sin_data</span><span class="p">[</span><span class="s">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> \<span class="o">*</span> <span class="n">pi</span> \<span class="o">*</span> <span class="n">sin_data</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">])</span> <span class="o">+</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">sin_data</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sin_data</span><span class="p">[</span><span class="s">&#39;y&#39;</span><span class="p">]</span>
<span class="n">Xpoly</span> <span class="o">=</span> <span class="n">dmatrix</span><span class="p">(</span><span class="s">&#39;C(x, Poly)&#39;</span><span class="p">)</span>
<span class="n">Xpoly1</span> <span class="o">=</span> <span class="n">Xpoly</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">Xpoly3</span> <span class="o">=</span> <span class="n">Xpoly</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">]</span>
<span class="n">Xpoly5</span> <span class="o">=</span> <span class="n">Xpoly</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">6</span><span class="p">]</span>
<span class="n">Xpoly25</span> <span class="o">=</span> <span class="n">Xpoly</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">26</span><span class="p">]</span>
</pre></div>


<p>The problem we encounter now is how to choose what order polynomial to
fit to the data. Any data can be fit well (i.e. have a high R^2^) if we
use a high enough order polynomial. But we will start to over-fit our
data; capturing noise specific to our sample, leading to poor
predictions on new data. The graph below shows the fits to the data of a
straight line, a 3rd-order polynomial, a 5th-order polynomial, and a
25th-order polynomial. Notice how the last fit gives us all kinds of
degrees of freedom to capture specific datapoints, and the excessive
&#8220;wiggles&#8221; look like we&#8217;re fitting to noise.</p>
<p><a href="../images/sine_wave_polyfits.png">
  <img src="../images/sine_wave_polyfits.png" width = 450px />
</a></p>
<p>In machine learning, this problem is solved with
<em>regularization</em>&#8212;penalizing large parameter estimates in a way that,
hopefully, shrinks down the coefficients on all but the most important
inputs. Here&#8217;s where scikit-learn shines.</p>
<h2>Preventing overfitting with regularization</h2>
<p>The penalty parameter in a regularized regression is typically found via
cross-validation; for each candidate penalty one repeatedly fits the
model on subsets on the data, and the penalty value that gives the best
fit across the cross-validation &#8220;folds&#8221; is chosen. In the book, the
authors hand-code up a cross-validation scheme, looping over possible
penalties and subsets of the data and recording the MSEs.</p>
<p>In scikit-learn you can usually automate the cross-validation procedure,
by one of a couple of ways. Many models have a <code>CV</code>version, or, if not,
you can wrap your model in a function like <a href="http://scikit-learn.org/stable/modules/grid_search.html"><code>GridSearchCV</code></a> which is a
convenience function around all the looping and fit-recording entailed
in a cross-validation. Here I&#8217;ll use the <a href="http://scikit-learn.org/stable/modules/linear_model.html"><code>LassoCV</code></a> function, which
performs cross-validation for a <span class="caps">LASSO</span>-penalized linear regression.</p>
<div class="highlight"><pre><span class="n">lasso_model</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="n">copy_X</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">lasso_fit</span> <span class="o">=</span> <span class="n">lasso_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xpoly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">11</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">lasso_path</span> <span class="o">=</span> <span class="n">lasso_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xpoly</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">11</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<p>The first line sets up the model by specifying some options. The only
interesting one here is <code>cv</code>, which specifies how many cross-validation
folds to run on each penalty-parameter value. The second line fits the
model: here&#8217;s I&#8217;m going to run a 10th-order polynomial regression, and
let the <span class="caps">LASSO</span> penalty shrink away all but the most important orders.
Finally, <code>lasso_path</code> provides the objective function that our penalty
parameter is suppose to optimize in the cross-validations (typically
<span class="caps">RMSE</span>).</p>
<p>After running the <code>fit()</code> method, <code>LassoCV</code> will provide useful output
attributes, including the &#8220;optimal&#8221; penalty parameter, stored in
<code>.alpha_</code>. Note that scikit-learn refers to the penalty parameter as
<code>alpha</code>, while R&#8217;s <code>glmnet</code>, which the authors use to implement the
<span class="caps">LASSO</span> model, calls it <code>lambda</code>. I&#8217;m more accustomed to the penalty
parameter being denoted with lambda myself. Note also that <code>glmnet</code> uses
<code>alpha</code> elsewhere.</p>
<div class="highlight"><pre><span class="c"># Plot the average <span class="caps">MSE</span> across folds</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lasso_fit</span><span class="o">.</span><span class="n">alphas_</span><span class="p">),</span>
<span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">lasso_fit</span><span class="o">.</span><span class="n">mse_path_</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;<span class="caps">RMSE</span> (avg. across folds)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r&#39;\$-</span><span class="se">\\</span><span class="s">log(</span><span class="se">\\</span><span class="s">lambda)\$&#39;</span><span class="p">)</span>
<span class="c"># Indicate the lasso parameter that minimizes the average <span class="caps">MSE</span> across</span>
<span class="n">folds</span><span class="o">.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">lasso_fit</span><span class="o">.</span><span class="n">alpha_</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#39;red&#39;</span><span class="p">)</span>
</pre></div>


<p><a href="../images/lasso_cv_fits_poly.png">
  <img src="../images/lasso_cv_fits_poly.png" width = 450px />
</a></p>
<p>The value of the penalty parameter itself isn&#8217;t all that meaningful. So
let&#8217;s take a look at what the resulting coefficient estimates are when
we apply the penalty.</p>
<div class="highlight"><pre><span class="k">print</span> <span class="s">&#39;Deg. Coefficient&#39;</span>
<span class="k">print</span> <span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">lasso_fit</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lasso_fit</span><span class="o">.</span><span class="n">coef_</span><span class="p">])</span>

<span class="n">Deg</span><span class="o">.</span> <span class="n">Coefficient</span>
  <span class="mi">0</span>    <span class="o">-</span><span class="mf">0.003584</span>
  <span class="mi">1</span>    <span class="o">-</span><span class="mf">5.359452</span>
  <span class="mi">2</span>     <span class="mf">0.000000</span>
  <span class="mi">3</span>     <span class="mf">4.689958</span>
  <span class="mi">4</span>    <span class="o">-</span><span class="mf">0.000000</span>
  <span class="mi">5</span>    <span class="o">-</span><span class="mf">0.547131</span>
  <span class="mi">6</span>    <span class="o">-</span><span class="mf">0.047675</span>
  <span class="mi">7</span>     <span class="mf">0.124998</span>
  <span class="mi">8</span>     <span class="mf">0.133224</span>
  <span class="mi">9</span>    <span class="o">-</span><span class="mf">0.171974</span>
 <span class="mi">10</span>     <span class="mf">0.090685</span>
</pre></div>


<p>So the <span class="caps">LASSO</span>, after selecting a penalty parameter via cross-validation,
results in essentially a 3rd-order polynomial model: <em>y = -5.4x +
4.7x^3^</em>. This makes sense since, as we saw above, we&#8217;d captured the
important features of the data by the time we&#8217;d fit a 3rd order
polynomial.</p>
<h2>Predicting O&#8217;Reilly book sales using back-cover descriptions</h2>
<p>Next I&#8217;ll use the same model to tackle some real data. We have the sales
ranks of the top-100 selling O&#8217;Reilly books. We&#8217;d like to see if we use
the text on the back-cover description of the book to predict its rank.
So the output variable is the rank of the book (reversed so that 100 is
the top-selling book, and 1 is the 100th best-selling book), while the
input variables are all the terms that appear in these 100 books&#8217; back
covers. For each book the value of an input variable is the number of
times the term appears on its back cover. Many of the input values will
be zero (for example, the term &#8220;javascript&#8221; will occur many times in a
book about javascript, but zero times in every other book).</p>
<p>So the matrix of input variables is just our old friend, the
term-document matrix. Creating this (using any of the methods described
in the posts for [chapter 3][] or [chapter 4][]), we can just apply
<code>LassoCV</code> again.</p>
<div class="highlight"><pre><span class="n">lasso_model</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">lasso_fit</span> <span class="o">=</span> <span class="n">lasso_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">desc_tdm</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">ranks</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>


<p>Because of the size and nature of the input data, this runs pretty
slowly (about 3-5 minutes for me). And, because there seems to be no
good prediction model to be had here, the model doesn&#8217;t alway converge.
If we do get a convergent run, we find the <span class="caps">CV</span> procedure wants us to
shrink all the coefficients to zero: no input is worth keeping per the
<span class="caps">LASSO</span>. (Note that since the x-axis in the graph is -log(penalty), moving
left on the axis, towards 0, means more regularization.) This is the
same result the authors find.</p>
<p><a href="../images/lasso_cv_fits_text.png">
  <img src="../images/lasso_cv_fits_text.png" width = 450px />
</a></p>
<h1>Logistic regression with cross-validation</h1>
<p>With the previous model a bust, the authors regroup and try to fit a
more simple output variable: a binary indicator of whether the book is
in the top-50 sellers or not. Since they&#8217;re modeling a 0/1 outcome, they
use a logistic regression. Like the linear models we used above, we can
also apply regularizers to logistic regression.</p>
<p>In the book, the authors again code up an explicit cross-validation
procedure. The <a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch6/ch6.ipynb">notebook</a> for this chapter has some code that
replicates their procedure, but here I&#8217;ll discuss a version that uses
scikit-learn&#8217;s <code>GridCV</code> function, which automates the cross-validation
procedure for us. (the term &#8220;grid&#8221; is a little confusing here, since
we&#8217;re only optimizing over one variable, the penalty parameter; the term
&#8220;grid&#8221; is a little more intuitive in a 2-or-more-dimension search).</p>
<div class="highlight"><pre><span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">penalty</span> <span class="o">=</span> <span class="s">&#39;l1&#39;</span><span class="p">),</span>
<span class="n">c_grid</span><span class="p">,</span>
<span class="n">score_func</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">zero_one_score</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="n">n_cv_folds</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainy</span><span class="p">)</span>
</pre></div>


<p>We initialize the <code>GridCV</code> procedure by telling it:</p>
<ul>
<li>What model we&#8217;re using: logistic, with a penalty parameter <code>C</code>, initialized at 1.0, using the L1 (<span class="caps">LASSO</span>) penalty.</li>
<li>A grid/array of parameter value candidates to search over: here values of <code>C</code>.</li>
<li>A score function to optimize: before we were using the <span class="caps">RMSE</span> of the regression, here we&#8217;ll use a correct classification rate, given by <code>zero_one_score</code>, in scikit-learn&#8217;s <code>metrics</code> module.</li>
<li>The number of cross-validation folds to performs; this defined elsewhere in the variable <code>n_cv_folds</code></li>
</ul>
<p>Then I fit the model on training data (a random subset of 80). After
running this, We can check what value it chose for the penalty
parameter, <code>C</code>, and what the in-sample error-rate for this value was.</p>
<div class="highlight"><pre><span class="n">clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_score_</span>
<span class="p">({</span><span class="s">&#39;C&#39;</span><span class="p">:</span> <span class="mf">0.29377144516536563</span><span class="p">},</span> <span class="mf">0.375</span><span class="p">)</span>
</pre></div>


<p>And again, let&#8217;s plot the error rates against values of <code>C</code> to vizualize
how regularization affects the model accuracy.</p>
<div class="highlight"><pre><span class="n">rates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">clf</span><span class="o">.</span><span class="n">grid_scores_</span><span class="p">])</span>
<span class="n">stds</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_cv_folds</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span>
<span class="n">clf</span><span class="o">.</span><span class="n">grid_scores_</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">rates</span> <span class="o">-</span> <span class="n">stds</span><span class="p">,</span> <span class="n">rates</span> <span class="o">+</span> <span class="n">stds</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#39;steelblue&#39;</span><span class="p">,</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="o">.</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cs</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="s">&#39;o-k&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&#39;Avg. error rate across folds&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;C (regularization parameter)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Avg. error rate (and +/- 1 s.e.)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&#39;best&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
</pre></div>


<p><a href="../images/logistic_cv_errors.png">
  <img src="../images/logistic_cv_errors.png" width = 450px />
</a></p>
<p>After fitting to the training set, we can predict on the test set and
and see how accurate the model is on new data using the
<code>classification_report</code> function.</p>
<div class="highlight"><pre><span class="n">print</span> <span class="n">metrics</span><span class="p">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">testy</span><span class="p">,</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">))</span>

             <span class="n">precision</span> <span class="n">recall</span> <span class="n">f1</span><span class="o">-</span><span class="n">score</span> <span class="n">support</span>
          <span class="mi">0</span>       <span class="mf">0.78</span>   <span class="mf">0.44</span>     <span class="mf">0.56</span>      <span class="mi">16</span>
          <span class="mi">1</span>       <span class="mf">0.18</span>   <span class="mf">0.50</span>     <span class="mf">0.27</span>       <span class="mi">4</span>
<span class="n">avg</span> <span class="o">/</span> <span class="n">total</span>       <span class="mf">0.66</span>   <span class="mf">0.45</span>     <span class="mf">0.50</span>      <span class="mi">20</span>
</pre></div>


<p>And the confusion matrix shows we got 9 instances classified correctly
(the diagonal), and 11 incorrectly (the off-diagonal).</p>
<div class="highlight"><pre><span class="k">print</span> <span class="s">&#39; Predicted&#39;</span>
<span class="k">print</span> <span class="s">&#39; Class&#39;</span>
<span class="k">print</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">testy</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)))</span>

<span class="n">Predicted</span>
  <span class="n">Class</span>
  <span class="mi">0</span> <span class="mi">1</span>
<span class="mi">0</span> <span class="mi">7</span> <span class="mi">9</span>
<span class="mi">1</span> <span class="mi">2</span> <span class="mi">2</span>
</pre></div>


<h2>Conclusion</h2>
<p>Cross-validation often requires a lot of bookkeeping code. Writing this
over and over again for different applications is inefficient and
error-prone. So it&#8217;s great that scikit-learn has functions that
encapsulate the cross-validation process in convenient
abstractions/interfaces that do the bookkeeping for you. It also has a
wide array of useful, cutting-edge models, and the
<a href="http://scikit-learn.org/stable/">documentation</a> is not just clear and organized, but also
educational: there are lots of examples and exposition that explains how
the underlying models work, not just what the <span class="caps">API</span> is.</p>
<p>So even though we didn&#8217;t build any kick-ass, high-accuracy predictive
models here, we did get to explore some fundamental methods in building
<span class="caps">ML</span> models, and get acquainted with the powerful tools in scikit-learn.</p>
      </div>

      <nav class="article-panel social-links">
        <ul>
          <li><a href="http://twitter.com/share?text=Slender%20Means&url=http://slendermeans.org/ml4h-ch6.html" class="social twitter">twitter</a></li>
          <li><a href="http://www.facebook.com/sharer/sharer.php?s=100&p[url]=http://slendermeans.org/ml4h-ch6.html&p[images][0]=&p[title]=&p[summary]=" class="social facebook">facebook</a></li>
          <li><a href="https://plus.google.com/share?url=http://slendermeans.org/ml4h-ch6.html" class="social gplus">google+</a></li>
        </ul>
      </nav>

    <section id="comments">
      <h2>Comments</h2>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          var disqus_identifier = "ml4h-ch6.html";
          var disqus_url = "http://slendermeans.org/ml4h-ch6.html";
          (function() {
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://slendermeans.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
    </div>
  </article>


  <!-- The site footer has a CC license link and a link to the colophon -->
  <footer id="end-matter">
    <ul>
      <li><a id="cc-license" title="license" rel="license" href="http://creativecommons.org/licenses/by/3.0/deed.en_US"> Creative Commons 3.0 </a></li>

      <li><a id="rss-link" title="rss" href="http://slendermeans.org/feeds/all.atom.xml">RSS</a></li>

      <li><a id="colophon-link" title="colophon" href="http://slendermeans.org/pages/colophon.html"> Colophon </a></li>
    </ul>
  </footer>

  <!-- This puts Google Analytics and Disqus comments scripts into each page -->
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43554300-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

<script type="text/javascript">
    var disqus_shortname = 'slendermeans';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>

</html>