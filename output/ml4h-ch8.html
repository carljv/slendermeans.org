<!DOCTYPE html>
<html lang="en">

<head>
    <title> Slender Means </title>
    <meta charset="utf-8">

    <link rel="shortcut icon" type="image/x-icon" href="http://slendermeans.org/theme/images/favicon.ico">

    <link href="http://slendermeans.org/theme/css/tango_code.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/webfonts.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/layout.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/type.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/color-decor.css" rel="stylesheet">
    <link href="http://slendermeans.org/theme/css/fontawesome/font-awesome.min.css" rel="stylesheet">
</head>

<body>
  <header id="top-matter">
    <!-- The Top Matter is comprised of the site Logo (which includes an image,
    the site slogan and the site name), and the Main Navigation menu. -->

    <hgroup id="logo">
        <a href="http://slendermeans.org/index.html">
        <!-- The graphic/title logo is a link that points home. The logo contains the site slogan and title, the text of which are styled in logo.css -->
        <h1 id="site-name">
          <span id="S">s</span><span id="lender">lender</span>
          <span id="M">m</span><span id="eans">eans</span>
        </h1>
        <h2 id="site-tag"> i'd like to drop my trousers to the world i am a man of means of </h2>
      </a>
    </hgroup>
    <nav id="main-nav">
    <!-- The navigation bar with links to site pages -->
        <ul>
          <!-- Home and Archives are listed explicitly -->
          <li><a href="http://slendermeans.org/index.html">Home</a></li>
          <li><a href="http://slendermeans.org/archives.html">Archives</a></li>

          <!-- Others are in PAGES variable, though we exclude the colophon
          page, whose link will go at the site's end-matter footer -->
              <li > <a href="http://slendermeans.org/pages/about.html"> About </a></li>
              <li > <a href="http://slendermeans.org/pages/blogroll.html"> Blogroll </a></li>
              <li > <a href="http://slendermeans.org/pages/will-it-python.html"> Will it  Python? </a></li>
        </ul>
    </nav>
  </header>
      <!-- For pages that have a title describing their content -->

  <div id="content">
    <article class="h-entry">
      <h1 class="p-name"><a href = "http://slendermeans.org/ml4h-ch8.html" rel = "bookmark"> <i>Machine Learning for Hackers</i> Chapter 8: Principal Components Analysis </a></h1>

      <!-- Post info has date, author, and category -->
      <footer class="article-panel post-info">
        <ul>
          <li><time class="dt-published" datetime="2013-09-06T17:30:00" pubdate> September 06, 2013 </time></li>
            <li> <address class="p-author h-card"><a href="http://slendermeans.org/author/carl.html"> Carl </a></address></li>
              <li><a class = "p-category" href="http://slendermeans.org/category/will-it-python.html"> Will it Python </a></li>
        </ul>
      </footer>

      <div class="e-content">
        <p>The <a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch8/ch8.ipynb">code for Chapter 8</a> has been sitting around for a long time now. Let&#8217;s blow the dust off and check it out. One thing before we start: explaining <span class="caps">PCA</span> well is kinda hard. If any experts reading feel like I&#8217;ve described something imprecisely (and have a better description), I&#8217;m very open to suggestions.</p>
<h2>Introduction</h2>
<p>Chapter 8 is about <em>Principal Components Analysis</em> (<span class="caps">PCA</span>), which the authors perform on data with time series of prices for 24 stocks. In very broad terms, <span class="caps">PCA</span> is about projecting many real-life, observed variables onto a smaller number of &#8220;abstract&#8221; variables, the principal components. Principal components are selected in order to best preserve the variation and correlation of the original variables. For example, if we have 100 variables in our data, which are all highly correlated, we can project them down to just a few principal components&#8212;-i.e., the high correlation between them can be imagined as coming from an underlying factor that drives all of them, with some other less important factors driving their differences. When variables aren&#8217;t highly correlated, more principal components are needed to describe them well.</p>
<p>As you might imagine, <span class="caps">PCA</span> can be a very effective way of dealing with multi-collinearity that crops up in datasets with lots of variables. The downside is that <span class="caps">PCA</span> is just a mechanical process that is independent of the phenomenon we&#8217;re studying; the &#8220;principal components&#8221; we find don&#8217;t have to have any real-world meaning&#8212;-they&#8217;re just mathematical constructs. Sometimes we can give meaningful interpretations to the principal components by analogizing them to real underlying factors that theoretically drive our data. But this can be tricky, from both a technical and epistemological standpoint.</p>
<p>For the stocks the authors analyze, they ultimately try and reduce their description to a single principal component, which they interpret as a kind of &#8220;market-wide&#8221; factor, and compare with a broad market index (here the <span class="caps">DJIA</span>). This is a not uncommon application of <span class="caps">PCA</span> in stock analysis. But they&#8217;ve got a technical problem here.</p>
<p>To perform <span class="caps">PCA</span>, your data have to have a meaningful covariance matrix (or correlation matrix, but the conditions are equivalent). They analyze stock <em>prices</em>, which are non-stationary time series variables. This means their covariance matrices change with time, so you can&#8217;t really estimate a meaningful covariance matrix from a sample of data. Your estimator implicitly assumes the data are stationary, so your estimated covariance matrix is meaningless. If we calculate the stock <em>returns</em> in the data, though, we can do <span class="caps">PCA</span> properly, and we&#8217;ll see the relationship of the resulting principal component with the broad market index is much cleaner.</p>
<p>If you&#8217;re comfortable with <span class="caps">PCA</span> already, you don&#8217;t really have to worry about the conceptual content of this chapter. If you&#8217;re not, my advice it to take this chapter as a decent toy example of where and why one uses <span class="caps">PCA</span>, but don&#8217;t apply what&#8217;s done here elsewhere without learning more first. I&#8217;m not going to explain <span class="caps">PCA</span> in any detail; I just want to show where <span class="caps">PCA</span> functions live in the Python ecosystem and how they work. But, like most machine learning techniques, it shouldn&#8217;t be used at home without adult supervision.</p>
<p>As usual the IPython notebook lives at the Github repo <a href="https://github.com/carljv/Will_it_Python/blob/master/MLFH/ch8/ch8.ipynb">here</a>, and can be viewed via nbviewer <a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/MLFH/ch8/ch8.ipynb">here</a>.</p>
<h2>Stock data munging</h2>
<p>The raw data are in a long format, with (no. of stocks) &times; (no. of days) rows, and three columns (a date, a stock ticker and a price for that ticker on that day). This sort of dataset lends itself to a pandas DataFrame with a hierarchical index&#8212;and since there&#8217;s only one variable in the data (the price), we&#8217;ll <code>squeeze</code> the DataFrame to get a Series. The Dow Jones data, containing just one ticker, is more straightforward.</p>
<div class="highlight"><pre><span class="n">prices_long</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;data/stock_prices.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="n">parse_dates</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">squeeze</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">dji_all</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;data/dji.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">parse_dates</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                   <span class="n">squeeze</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>With the stock data indexed this way, it&#8217;s easy to create a <code>date</code> &times; <code>ticker</code> DataFrame with prices as entries&#8212;-we just use <code>unstack</code>.</p>
<div class="highlight"><pre><span class="n">prices</span> <span class="o">=</span> <span class="n">prices_long</span><span class="o">.</span><span class="n">unstack</span><span class="p">()</span>
</pre></div>


<p>Since we&#8217;ll ultimately want to perform this analysis with price returns, I&#8217;m going to create a similar dataset, just with returns instead of prices (note this will have one less day of data, since we don&#8217;t know the return for the first day in the data).</p>
<div class="highlight"><pre><span class="n">calc_returns</span> <span class="o">=</span> <span class="n">lambda</span> <span class="n">x</span><span class="o">:</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">x</span><span class="p">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">))[</span><span class="mi">1</span><span class="o">:</span><span class="p">]</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">prices</span><span class="p">.</span><span class="n">apply</span><span class="p">(</span><span class="n">calc_returns</span><span class="p">)</span>
</pre></div>


<p>Note I&#8217;m using log returns here. Pandas DataFrames have a <code>pct_change</code> method that would provide another way of computing returns.</p>
<p>The authors&#8217; <span class="caps">PCA</span> strategy here is to extract a &#8220;stock index&#8221; factor from the stock data by using the first principal component&#8212;-this is the single principal component that captures the most variation in the underlying data.</p>
<p>The function <code>make_pca_index</code> is going to extract this first principal component, using the <code>PCA</code> function in scikit-learn&#8217;s <code>sklearn.decomposition</code> module. This is not the only way to get a <span class="caps">PCA</span> in Python&#8212;-indeed <span class="caps">PCA</span> is mechanically just an eigen-decomposition of the data&#8217;s correlation or covariance, so you could do this all from scratch in Numpy. The scikit-learn implementation, though, gives us a convenient <code>PCA</code> object to work with. And as usual with scikit-learn, the <a href="http://scikit-learn.org/stable/modules/decomposition.html#pca">documentation</a> is very good.</p>
<p>The <code>PCA</code> function works with a covariance or correlation matrix. We&#8217;re going to use a correlation matrix here; and our function will just take either stock price or return data, compute its correlation, then find the first principal component of the data. Notice the sign change I do at the end there&#8212;-the component ended up being negatively related to the data (i.e. when this factor goes down, the data go up, etc.). <span class="caps">PCA</span> results are typically sign- and scale-invariant; hence the problems with interpretation. We&#8217;ll make our resulting index a &#8220;postive&#8221; one by just reversing the sign.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">make_pca_index</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">scale_data</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Compute the correlation matrix of a set of stock data, and return</span>
<span class="sd">    the first principal component.</span>

<span class="sd">    By default, the data are scaled to have mean zero and variance one</span>
<span class="sd">    before performing the <span class="caps">PCA</span>.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="n">scale_data</span><span class="p">:</span>
        <span class="n">data_std</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data_std</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">corrs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">data_std</span><span class="o">.</span><span class="n">cov</span><span class="p">())</span>
    <span class="n">pca</span>   <span class="o">=</span> <span class="n"><span class="caps">PCA</span></span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corrs</span><span class="p">)</span>
    <span class="c"># We end up getting a negative value for the index, so we&#39;ll reverse</span>
    <span class="c"># the sign to have it be more intuitive.</span>
    <span class="n">mkt_index</span> <span class="o">=</span> <span class="o">-</span><span class="n">scale</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data_std</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">mkt_index</span>
</pre></div>


<h3>A <span class="caps">PCA</span> index with price data</h3>
<p>Let&#8217;s copy the authors and make an index from raw price data. Since prices don&#8217;t have meaningully-estimated correlations, this isn&#8217;t really correct, but it&#8217;s useful to compare with what&#8217;s in the book.</p>
<div class="highlight"><pre><span class="n">price_index</span> <span class="o">=</span> <span class="n">make_pca_index</span><span class="p">(</span><span class="n">prices</span><span class="p">)</span>
</pre></div>


<p>To see what&#8217;s going on, let&#8217;s make two plots: a scatter plot of our <span class="caps">PCA</span> index with the <span class="caps">DJIA</span>, and a time series plot with the two indices. These correspond to figures 8-4 and 8-5 in the book.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">price_index</span><span class="p">,</span> <span class="n">scale</span><span class="p">(</span><span class="n">dji</span><span class="p">),</span> <span class="s">&#39;k.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;<span class="caps">PCA</span> index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Dow Jones Index&#39;</span><span class="p">)</span>
<span class="n">ols_fit</span> <span class="o">=</span> <span class="n">OLSreg</span><span class="p">(</span><span class="n">scale</span><span class="p">(</span><span class="n">dji</span><span class="p">),</span> <span class="n">price_index</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">price_index</span><span class="p">,</span> <span class="n">ols_fit</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span> <span class="s">&#39;r-&#39;</span><span class="p">,</span>
         <span class="n">label</span> <span class="o">=</span> <span class="s">&#39;R2 = </span><span class="si">%4.3f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">round</span><span class="p">(</span><span class="n">ols_fit</span><span class="o">.</span><span class="n">rsquared</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dates</span><span class="p">,</span> <span class="n">price_index</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&#39;<span class="caps">PCA</span> Price Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dates</span><span class="p">,</span> <span class="n">scale</span><span class="p">(</span><span class="n">dji</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s">&#39;<span class="caps">DJ</span> Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s">&#39;upper left&#39;</span><span class="p">)</span>
</pre></div>


<p><a class = "image" href="../images/pca_price.png">
  <img src="../images/pca_price.png" width=400 />
</a></p>
<p>This actually seems to look okay, and wouldn&#8217;t really alert us to any problems if we didn&#8217;t know any better. Let&#8217;s repeat the exercise with returns, though.</p>
<h3>A <span class="caps">PCA</span> index with returns data</h3>
<p>Since returns are stationary, we can estimate a meaningful correlation matrix, and our <span class="caps">PCA</span> will make more sense.</p>
<div class="highlight"><pre><span class="n">returns_index</span> <span class="o">=</span> <span class="n">make_pca_index</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
</pre></div>


<p>And the plots:</p>
<p><a class = "image" href="../images/pca_returns.png">
  <img src="../images/pca_returns.png" width=400 />
</a></p>
<p>Looking at these, we see a much more straightforward linear relationship between the returns to the <span class="caps">DJIA</span> and the <span class="caps">PCA</span> index derived from stock returns.</p>
<h3>Explained variance</h3>
<p>Since the principal components are just eigenvalues, there will be as many of them as their are columns in our data (here 24). As we add components we explain more and more of the original correlation matrix. Once we add all 24 the amount of variation/correlation explained is 100%&#8212;-all we&#8217;ve done is define a rotation of the matrix, so there&#8217;s no information lost. But a plot of the cumulative explained variance as we add principal components can help us to see how far and how reliably the data can be reduced.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span> <span class="o">+</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">pca_fit</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">.</span><span class="n">cumsum</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;No. of principal components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Cumulative variance explained&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="s">&#39;y&#39;</span><span class="p">,</span> <span class="n">ls</span> <span class="o">=</span> <span class="s">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#39;white&#39;</span><span class="p">)</span>
</pre></div>


<p><a class="image" href="../images/pca_variance.png">
  <img src="../images/pca_variance.png" width=400 />
</a></p>
<h3>Factor loadings</h3>
<p>We can also check out the loadings of the principal component across the stocks. What this shows us is how a change in the relates to the stocks in our data. For example a a component going up might cause half the stock returns in the data to go up and half to go down (it would positively load on some and negatively load on others.) We would expect, intuitively, a factor representing &#8220;the market,&#8221; as we think our first component does, to load on our stocks in the same direction, and roughly the same magnitude. And this is basically what we see.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">),</span> <span class="n">pca_fit</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<p><a class="image" href="../images/pca_loadings.png">
  <img src="../images/pca_loadings.png" width=400 />
</a></p>
<h2>Conclusion</h2>
<p>Since <span class="caps">PCA</span> is such a widely-used and fundamental technique, it&#8217;s important to know how to do it in Python, and the scikit-learn implementation is a good one. Check out the documentation <a href="http://scikit-learn.org/stable/modules/decomposition.html#pca">here</a>. Of course, like any statistical technique, <span class="caps">PCA</span> can definitely be misused, or at least easily misintepreted, so handle with care.</p>
      </div>

      <nav class="article-panel social-links">
        <ul>
          <li><a href="http://twitter.com/share?text=Slender%20Means&url=http://slendermeans.org/ml4h-ch8.html" class="social twitter">twitter</a></li>
          <li><a href="http://www.facebook.com/sharer/sharer.php?s=100&p[url]=http://slendermeans.org/ml4h-ch8.html&p[images][0]=&p[title]=&p[summary]=" class="social facebook">facebook</a></li>
          <li><a href="https://plus.google.com/share?url=http://slendermeans.org/ml4h-ch8.html" class="social gplus">google+</a></li>
        </ul>
      </nav>

    <section id="comments">
      <h2>Comments</h2>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          var disqus_identifier = "ml4h-ch8.html";
          var disqus_url = "http://slendermeans.org/ml4h-ch8.html";
          (function() {
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://slendermeans.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
    </div>
  </article>


  <!-- The site footer has a CC license link and a link to the colophon -->
  <footer id="end-matter">
    <ul>
      <li><a id="cc-license" title="license" rel="license" href="http://creativecommons.org/licenses/by/3.0/deed.en_US"> Creative Commons 3.0 </a></li>

      <li><a id="rss-link" title="rss" href="http://slendermeans.org/feeds/all.atom.xml">RSS</a></li>

      <li><a id="colophon-link" title="colophon" href="http://slendermeans.org/pages/colophon.html"> Colophon </a></li>
    </ul>
  </footer>

  <!-- This puts Google Analytics and Disqus comments scripts into each page -->
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-43554300-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>

<script type="text/javascript">
    var disqus_shortname = 'slendermeans';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>

</html>